<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Benchmarking Multimodal Large Language Models in Zero-shot and
Few-shot Scenarios: Preliminary Results on Studying Christian
Iconography</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Tinos:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="../../../css/site.css">

  <!-- citation information -->
  <link rel="canonical" href="https://anthology.ach.org/volumes/vol0003/benchmarking-multimodal-large-language-models-in/">
  <meta name="citation_title" content="Benchmarking Multimodal Large Language Models in Zero-shot and
Few-shot Scenarios: Preliminary Results on Studying Christian
Iconography">
  <meta name="citation_date" content="2025">
  <meta name="citation_public_url" content="https://anthology.ach.org/volumes/vol0003/benchmarking-multimodal-large-language-models-in/">
  <meta name="citation_journal_title" content="Anthology of Computers and the Humanities">
  <meta name="citation_issn" content="">
  <meta name="citation_volume" content="3">
  <meta name="citation_firstpage" content="1198">
  <meta name="citation_lastpage" content="1210">
  <meta name="citation_doi" content="10.63744/oxWtm5MhhwBH">
  <meta name="citation_author" content="Spinaci, Gianmarco">
  <meta name="citation_author" content="Klic, Lukas">
  <meta name="citation_author" content="Colavizza, Giovanni">
  <meta name="citation_editor" content="Arnold, Taylor">
  <meta name="citation_editor" content="Fantoli, Margherita">
  <meta name="citation_editor" content="Ros, and Ruben">
  <meta name="citation_abstract" content="This study evaluates the capabilities of multimodal large language
models (LLMs) in the task of single-label classification of Christian
iconography, focusing on their performance in zero-shot and few-shot
settings across curated datasets. The goal was to assess whether
general-purpose models, such as &lt;em&gt;GPT-4o&lt;/em&gt; and &lt;em&gt;Gemini 2.5&lt;/em&gt;,
can interpret the Iconography, typically addressed by supervised
classifiers, and evaluate their performance. Two research questions
guided the analysis: &lt;strong&gt;(RQ1)&lt;/strong&gt; How do multimodal LLMs
perform on image classification of Christian saints? And
&lt;strong&gt;(RQ2)&lt;/strong&gt;, how does performance vary when enriching input
with contextual information or few-shot exemplars?&lt;/p&gt;
&lt;p&gt;We conducted a benchmarking study using three datasets supporting
Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include
the top 10 most frequent classes. Models were tested under three
conditions: (1) classification using class labels, (2) classification
with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned
on the same datasets.&lt;/p&gt;
&lt;p&gt;The findings show that &lt;em&gt;Gemini-2.5 Pro&lt;/em&gt; and &lt;em&gt;GPT-4o&lt;/em&gt;
outperformed the ResNet50 baselines across the three configurations
reaching peaks of &lt;strong&gt;90.45%&lt;/strong&gt; and &lt;strong&gt;88.20%&lt;/strong&gt; in
ArtDL, respectively. Accuracy dropped significantly on the Wikidata
dataset, suggesting model sensitivity to image size and metadata
alignment. Enriching prompts with class descriptions generally improved
zero-shot performance, while few-shot learning produced lower results,
with only occasional and minimal increments in accuracy.&lt;/p&gt;
&lt;p&gt;We conclude that general-purpose multimodal LLMs are capable of
classification in visually complex cultural heritage domains, even
without specific fine-tuning. However, their performance is influenced
by dataset consistency and the design of the prompts. These results
support the application of LLMs as metadata curation tools in digital
humanities workflows, suggesting future research on prompt optimization
and the expansion of the study to other classification strategies and
models.">
  <meta name="citation_language" content="en">
  <meta name="citation_keywords" content="Multimodal Models; Large Language Models; Image Classification; Iconography">
  <meta name="citation_fulltext_html_url" content="https://anthology.ach.org/volumes/vol0003/benchmarking-multimodal-large-language-models-in/">
  <meta name="citation_pdf_url" content="https://anthology.ach.org/volumes/vol0003/benchmarking-multimodal-large-language-models-in/10.63744@oxWtm5MhhwBH.pdf">

  <!-- Lightbox CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css" />

  <!-- Pandoc syntax highlighting CSS -->
  <link rel="stylesheet" href="../../../css/syntax-highlighting.css" />
  <link rel="stylesheet" href="../../../css/article.css" />


</head>
<body>
  <!-- Navigation Bar -->
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://ach.org">
        <img src="../../../logo/logo.png" alt="ACH Logo">
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navbarMain">
      <div class="navbar-start">
      </div>
      <div class="navbar-end">
        <a class="navbar-item" href="https://anthology.ach.org/">
          <b>Home</b>
        </a>
        <a class="navbar-item" href="https://anthology.ach.org/volumes/">
          <b>Volumes</b>
        </a>
        <a class="navbar-item" href="https://anthology.ach.org/about">
          <b>About</b>
        </a>
        <span style="width: 25px"></span>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <p class="subtitle is-6 has-text-grey">Anthology of Computers and the Humanities · <a href="..">Volume 3</a></p>
        <h1 class="title paper-title">Benchmarking Multimodal Large Language Models in Zero-shot and
Few-shot Scenarios: Preliminary Results on Studying Christian
Iconography</h1>

        <div class="is-size-5 pl-7 has-text-centered">
          <a href="https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:25px!important;margin-left:3px;vertical-align:text-bottom;" src="../../../logo/cc-by.png"></a>
        </div>
      </div>

    </div>
  </section>

  <!-- Main Content -->
  <section class="section">
    <div class="container">

      <div class="paper-meta">
        <div class="authors-container">
          <div class="authors">
            <section class="authors-block">
              <p class="authors">
                  <span class="author">
                    Gianmarco Spinaci<sup>1,2</sup><a class="orcid-link" href="https://orcid.org/0000-0002-3504-3241" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span>,
                  <span class="author">
                    Lukas Klic<sup>2</sup><a class="orcid-link" href="https://orcid.org/0000-0002-9620-7107" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span> and
                  <span class="author">
                    Giovanni Colavizza<sup>1,3</sup><a class="orcid-link" href="https://orcid.org/0000-0002-9806-084X" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span>
              </p>

              <ul class="affiliations">
                  <li><sup>1</sup> Department of Classical Philology and Italian Studies, University of
Bologna, Bologna, Italy</li>
                  <li><sup>2</sup> Villa i Tatti, The Harvard University Center for Italian Renaissance
Studies, Firenze, Italy</li>
                  <li><sup>3</sup> Department of Communication, University of Copenhagen, Copenhagen,
Denmark</li>
              </ul>
            </section>

          </div>
        </div>
      </div>


      <!-- Download Buttons -->
      <div class="buttons-container">
        <a href="10.63744@oxWtm5MhhwBH.pdf" class="button is-primary" target="_blank">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Download PDF</span>
        </a>
        <a href="10.63744@oxWtm5MhhwBH.bib" class="button is-info" target="_blank">
          <span class="icon">
            <i class="fas fa-file-code"></i>
          </span>
          <span>Download Citation</span>
        </a>
        
      </div>

      <!-- DOI -->
      <div class="doi-box">
        <p class="is-size-6">
          <b>Permanent Link:</b> <a class="doi-link" href="https://doi.org/10.63744/oxWtm5MhhwBH" target="_blank">https://doi.org/10.63744/oxWtm5MhhwBH</a>
        </p>
        <p class="is-size-6">
          <b>Published:</b> 21 November 2025
        </p>
        <p class="is-size-6">
          <b>Keywords:</b> Multimodal Models, Large Language Models, Image Classification, Iconography
        </p>
      </div>

      <!-- Abstract -->
      <div class="content">
         <div class="abs"><span>Abstract</span><p>This study evaluates the capabilities of multimodal large language models (LLMs) in the task of single-label classification of Christian iconography, focusing on their performance in zero-shot and few-shot settings across curated datasets. The goal was to assess whether general-purpose models, such as <em>GPT-4o</em> and <em>Gemini 2.5</em>, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: <strong>(RQ1)</strong> How do multimodal LLMs perform on image classification of Christian saints? And <strong>(RQ2)</strong>, how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that <em>Gemini-2.5 Pro</em> and <em>GPT-4o</em> outperformed the ResNet50 baselines across the three configurations reaching peaks of <strong>90.45%</strong> and <strong>88.20%</strong> in ArtDL, respectively. Accuracy dropped significantly on the Wikidata dataset, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains, even without specific fine-tuning. However, their performance is influenced by dataset consistency and the design of the prompts. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models.</p></div>


      </div>

    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="content has-text-centered">
      <p>
        <strong>Anthology of Computers and the Humanities</strong> · Association for Computers and the Humanities
      </p>
      <p class="is-size-7">
        Paper © 2025 the authors. All other content © 2025 ACH.
      </p>
    </div>
  </footer>


  <!-- Lightbox JavaScript -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox.min.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-xml.min.js"></script>

  <!-- Custom JavaScript -->
  <script src="../../../js/navbar.js"></script>
  <script src="../../../js/lightbox-config.js"></script>
  <script src="../../../js/code-copy.js"></script>

</body>
</html>