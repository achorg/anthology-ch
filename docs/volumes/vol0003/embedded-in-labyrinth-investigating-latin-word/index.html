<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Embedded in the Labyrinth: Investigating Latin Word Senses through
Transformer-Based Contextual Embeddings and Attention</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Tinos:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="../../../css/site.css">

  <!-- citation information -->
  <link rel="canonical" href="https://anthology.ach.org/volumes/vol0003/embedded-in-labyrinth-investigating-latin-word/">
  <meta name="citation_title" content="Embedded in the Labyrinth: Investigating Latin Word Senses through
Transformer-Based Contextual Embeddings and Attention">
  <meta name="citation_date" content="2025">
  <meta name="citation_public_url" content="https://anthology.ach.org/volumes/vol0003/embedded-in-labyrinth-investigating-latin-word/">
  <meta name="citation_journal_title" content="Anthology of Computers and the Humanities">
  <meta name="citation_issn" content="">
  <meta name="citation_volume" content="3">
  <meta name="citation_firstpage" content="467">
  <meta name="citation_lastpage" content="481">
  <meta name="citation_doi" content="10.63744/FuaAvdPMdtwW">
  <meta name="citation_author" content="Kaše, Vojtěch">
  <meta name="citation_author" content="Lang, Sarah">
  <meta name="citation_author" content="Pavlas, Petr">
  <meta name="citation_editor" content="Arnold, Taylor">
  <meta name="citation_editor" content="Fantoli, Margherita">
  <meta name="citation_editor" content="Ros, and Ruben">
  <meta name="citation_abstract" content="This paper explores how transformer-based models can enhance
historical keyword-in-context studies through automatic word sense
disambiguation (WSD). Using the Latin term &lt;em&gt;labyrinthus&lt;/em&gt; as a
case study, we analyze its contextual meanings across time and genre
within the GreLa corpus. A Large language model provides preliminary
sense labels, which we use to evaluate 64 embedding variants—contextual,
attention-based, and co-occurrence-based—derived from XLM-R and Latin
BERT. Our results show that combining embedding types yields the best
performance. We also illustrate how attention-based embeddings capture
meaningful diachronic patterns, offering promising directions for future
research on semantic change and metaphor in historical texts.">
  <meta name="citation_language" content="en">
  <meta name="citation_keywords" content="labyrinth; keyword-in-context; computational Latin philology; contextual word embeddings; automatic word sense disambiguation; word sense induction; semantic change detection; metaphor detection">
  <meta name="citation_fulltext_html_url" content="https://anthology.ach.org/volumes/vol0003/embedded-in-labyrinth-investigating-latin-word/">
  <meta name="citation_pdf_url" content="https://anthology.ach.org/volumes/vol0003/embedded-in-labyrinth-investigating-latin-word/10.63744@FuaAvdPMdtwW.pdf">

  <!-- Lightbox CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css" />

  <!-- Pandoc syntax highlighting CSS -->
  <link rel="stylesheet" href="../../../css/syntax-highlighting.css" />
  <link rel="stylesheet" href="../../../css/article.css" />


</head>
<body>
  <!-- Navigation Bar -->
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://ach.org">
        <img src="../../../logo/logo.png" alt="ACH Logo">
      </a>

      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navbarMain">
      <div class="navbar-start">
      </div>
      <div class="navbar-end">
        <a class="navbar-item" href="https://anthology.ach.org/">
          <b>Home</b>
        </a>
        <a class="navbar-item" href="https://anthology.ach.org/volumes/">
          <b>Volumes</b>
        </a>
        <a class="navbar-item" href="https://anthology.ach.org/about">
          <b>About</b>
        </a>
        <span style="width: 25px"></span>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <p class="subtitle is-6 has-text-grey">Anthology of Computers and the Humanities · <a href="..">Volume 3</a></p>
        <h1 class="title paper-title">Embedded in the Labyrinth: Investigating Latin Word Senses through
Transformer-Based Contextual Embeddings and Attention</h1>

        <div class="is-size-5 pl-7 has-text-centered">
          <a href="https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"><img style="height:25px!important;margin-left:3px;vertical-align:text-bottom;" src="../../../logo/cc-by.png"></a>
        </div>
      </div>

    </div>
  </section>

  <!-- Main Content -->
  <section class="section">
    <div class="container">

      <div class="paper-meta">
        <div class="authors-container">
          <div class="authors">
            <section class="authors-block">
              <p class="authors">
                  <span class="author">
                    Vojtěch Kaše<sup>1</sup><a class="orcid-link" href="https://orcid.org/0000-0002-6601-1605" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span>,
                  <span class="author">
                    Sarah Lang<sup>2</sup><a class="orcid-link" href="https://orcid.org/0000-0002-4618-9481" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span> and
                  <span class="author">
                    Petr Pavlas<sup>1</sup><a class="orcid-link" href="https://orcid.org/0000-0001-9848-4995" target="_blank" rel="noopener">
                        <img class="orcid" src="../../../logo/orcid.png" alt="ORCID">
                      </a>
                  </span>
              </p>

              <ul class="affiliations">
                  <li><sup>1</sup> Institute of Philosophy, Czech Academy of Sciences, Prague, Czech
Republic</li>
                  <li><sup>2</sup> Max Planck Institute for the History of Science, Berlin, Germany</li>
              </ul>
            </section>

          </div>
        </div>
      </div>


      <!-- Download Buttons -->
      <div class="buttons-container">
        <a href="10.63744@FuaAvdPMdtwW.pdf" class="button is-primary" target="_blank">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Download PDF</span>
        </a>
        <a href="10.63744@FuaAvdPMdtwW.bib" class="button is-info" target="_blank">
          <span class="icon">
            <i class="fas fa-file-code"></i>
          </span>
          <span>Download Citation</span>
        </a>
        
      </div>

      <!-- DOI -->
      <div class="doi-box">
        <p class="is-size-6">
          <b>Permanent Link:</b> <a class="doi-link" href="https://doi.org/10.63744/FuaAvdPMdtwW" target="_blank">https://doi.org/10.63744/FuaAvdPMdtwW</a>
        </p>
        <p class="is-size-6">
          <b>Published:</b> 21 November 2025
        </p>
        <p class="is-size-6">
          <b>Keywords:</b> labyrinth, keyword-in-context, computational Latin philology, contextual word embeddings, automatic word sense disambiguation, word sense induction, semantic change detection, metaphor detection
        </p>
      </div>

      <!-- Abstract -->
      <div class="content">
         <div class="abs"><span>Abstract</span><p>This paper explores how transformer-based models can enhance historical keyword-in-context studies through automatic word sense disambiguation (WSD). Using the Latin term <em>labyrinthus</em> as a case study, we analyze its contextual meanings across time and genre within the GreLa corpus. A Large language model provides preliminary sense labels, which we use to evaluate 64 embedding variants—contextual, attention-based, and co-occurrence-based—derived from XLM-R and Latin BERT. Our results show that combining embedding types yields the best performance. We also illustrate how attention-based embeddings capture meaningful diachronic patterns, offering promising directions for future research on semantic change and metaphor in historical texts.</p></div>


      </div>

    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="content has-text-centered">
      <p>
        <strong>Anthology of Computers and the Humanities</strong> · Association for Computers and the Humanities
      </p>
      <p class="is-size-7">
        Paper © 2025 the authors. All other content © 2025 ACH.
      </p>
    </div>
  </footer>


  <!-- Lightbox JavaScript -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox.min.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.30.0/components/prism-xml.min.js"></script>

  <!-- Custom JavaScript -->
  <script src="../../../js/navbar.js"></script>
  <script src="../../../js/lightbox-config.js"></script>
  <script src="../../../js/code-copy.js"></script>

</body>
</html>