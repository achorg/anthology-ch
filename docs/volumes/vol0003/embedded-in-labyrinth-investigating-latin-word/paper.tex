\documentclass[final]{anthology-ch}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}

\title{Embedded in the Labyrinth: Investigating Latin Word Senses through Transformer-Based Contextual Embeddings and Attention}

\author[1]{Vojtěch Kaše}[
orcid=0000-0002-6601-1605
]

\author[2]{Sarah Lang}[
orcid=0000-0002-4618-9481
]

\author[1]{Petr Pavlas}[
orcid=0000-0001-9848-4995
]

\affiliation{1}{Institute of Philosophy, Czech Academy of Sciences, Prague, Czech Republic}
\affiliation{2}{Max Planck Institute for the History of Science, Berlin, Germany}

\keywords{labyrinth, keyword-in-context, computational Latin philology, contextual word embeddings, automatic word sense disambiguation, word sense induction, semantic change detection, metaphor detection}

\pubyear{2025}
\pubvolume{3}
\pagestart{481}
\pageend{495}
\conferencename{Computational Humanities Research 2025}
\conferenceeditors{Taylor Arnold, Margherita Fantoli, and Ruben Ros}
\doi{10.63744/FuaAvdPMdtwW}
\paperorder{29}

\addbibresource{bibliography.bib}

\begin{document}

\maketitle

\begin{abstract}
This paper explores how transformer-based models can enhance historical keyword-in-context studies through automatic word sense disambiguation (WSD). Using the Latin term \emph{labyrinthus} as a case study, we analyze its contextual meanings across time and genre within the GreLa corpus. A Large language model provides preliminary sense labels, which we use to evaluate 64 embedding variants—contextual, attention-based, and co-occurrence-based—derived from XLM-R and Latin BERT. Our results show that combining embedding types yields the best performance. We also illustrate how attention-based embeddings capture meaningful diachronic patterns, offering promising directions for future research on semantic change and metaphor in historical texts.
\end{abstract}

\section{Introduction}

Keyword-in-context search has long been a foundational technique in distant reading, particularly within the digital humanities, where it supports exploratory analysis and facilitates close reading. Tools such as Voyant have made this method widely accessible, especially for scholars interested in tracing terms across large corpora. However, the analytical reach of keyword-in-context remains limited, particularly when dealing with polysemous terms whose meanings shift across genres, periods, and discursive domains.

It is within this methodological space that our study intervenes. We focus on the term \emph{labyrinthus}, tracing its semantic evolution across the longue durée of Latin literature. Initially rooted in classical mythology, the word later developed into a metaphorical device used in theological, philosophical, and scientific contexts. While prior scholarship has examined its poetic and religious resonances \cite{Doob1992_LabyrinthIdea_chrLabyrinth}, its appropriation within early scientific discourse remains largely unexplored, although some intriguing examples are known in alchemical texts  \cite{Matton2006_FiletAriane_chrLabyrinth, Maier1618_ViatoriumDeMontibus_chrLabyrinth, Lang2021_MaierViatoriumExhibition_chrLabyrinth}.

Our goal is to go beyond surface-level co-occurrence analysis and explore whether recent advances in computational linguistics—particularly transformer-based models—can help disambiguate the contextual meanings of historically layered terms. We propose that \emph{labyrinthus} serves as a valuable case study for assessing the utility of various embedding strategies in tasks such as word sense disambiguation (WSD) and diachronic semantic analysis.

To this end, our methodology consists of five main steps. First, we extract all occurrences of the target term from the GreLa corpus \cite{CCS2025_GreLaCorpus}, a morphologically annotated and lemmatized collection of Latin and Greek texts (see Appendix~\ref{appdx:corpusBias}). Second, we use large language models (LLMs) to generate preliminary sense labels for each instance. Third, we generate multiple variants of contextual and attention-based embeddings using two transformer models. Fourth, we train four different classifiers on these embeddings to predict the LLM-generated sense labels. Finally, we evaluate classification performance and interpret results with an eye to both methodological implications and the semantic contours of the term itself.

Rather than aiming for a definitive taxonomy of senses, our goal is exploratory: to assess the comparative performance of embedding strategies, uncover semantic patterns in the data, and reflect on the strengths and limitations of these methods for historical-linguistic inquiry. In doing so, we position this study as a bridge between traditional philological concerns and recent innovations in computational language modeling.

\section{Literature Review}

Latin word sense disambiguation (WSD) is a well-established, though still relatively underexplored, task in digital humanities and historical linguistics. Early work by Bamman and Crane \cite{BammanCrane2011wordSenseVariation} introduced bilingual WSD using parallel corpora, while more recent efforts have aimed to formalize annotation frameworks for diachronic semantics \cite{McGillivray2022latinDiachronic}. Broader multilingual approaches have also been developed: \cite{logacheva2020wordsensedisambiguation158} demonstrated scalable WSD across 158 languages using word embeddings, and \cite{lendvai-wick-2022-finetuningLatinBERT} showed the effectiveness of contextualized transformer models such as LatinBERT over static embeddings. Ghinassi et al. \cite{ghinassi-etal-2024-LatinDisambiguation} extended the field further by constructing the largest annotated Latin WSD dataset to date using a multilingual pivoting strategy.

While our study is related to WSD, it introduces additional complexity. We focus not only on sense disambiguation but also on metaphorical extension and semantic change across historical contexts. These themes are increasingly central in computational semantics. The emergence of transformer-based contextual embeddings has significantly advanced both WSD and metaphor detection. Their advantages over static, type-based embeddings such as word2vec \cite{mikolov_distributed_2013, periti_systematic_2024} are well documented \cite{bevilacqua_recent_2021, periti_systematic_2024}. Some studies have identified layers within the transformer architecture—particularly deeper layers—as especially rich in semantic information \cite{tenney2019bert, teglia_how_2025}, while attention distributions have been shown to carry useful disambiguation signals as well \cite{ion_unsupervised_2025}. At the same time, some researchers argue for the continued relevance of count-based contextual representations, especially in historical-linguistic applications \cite{geeraerts_lexical_2024}.

These methodological discussions form the backdrop to our analysis. We position \emph{labyrinthus} as a historically rich and semantically complex term -- one whose interpretive range spans myth, poetry, theology, and early scientific discourse. By testing multiple embedding types and classification strategies against automatically generated sense labels, we aim to contribute to an ongoing conversation about the utility of modern NLP tools for historical semantic inquiry.

\section{Materials}

Our analysis begins with the extraction of all occurrences of the term `labyrinthus' from the GreLa corpus, along with essential metadata, including the name of the author, the title of the work, and the date of composition or publication. Given that GreLa provides morphologically annotated and lemmatized data, it enables the precise retrieval of all instances of the term without recourse to wildcard characters or fuzzy search techniques.

For each identified occurrence, we extracted four types of contextual information. First, we retrieved the raw text of the sentence in which the term appears. Second, we collected the full set of morphological annotations -- comprising the lemma and part-of-speech tag -- for each token within the sentence. Third, we extracted a broader context window consisting of the sentence containing the target term, preceded and followed by one adjacent sentence each. Finally, we constructed a concordance of morphologically annotated data comprising the ten tokens preceding the target term, the term itself, and the ten tokens following it. In our experimental setup, we assess the utility of these different types of contextual data as inputs for computational models.

\section{Methods}

\subsection{Automatic sense labeling}
To obtain classified sense labels for the usage of the term `labyrinthus' across the target sentences, the domain experts in our team inspected a random sample of sentences and designed a prompt for LLM to classify the sense into 6 different categories:

\begin{enumerate}\small
\item[1 --] \textbf{Mythological:} References to the Cretan myth or its main figures and settings
(Daedalus, Minotaur, Ariadne, Theseus, Crete).
Restricted to explicit mythic allusions.

\item[2 --] \textbf{Technical literal:} Descriptions of actual or imagined physical labyrinths
such as buildings, caves, mines, or mechanical structures, used non-figuratively.

\item[3 --] \textbf{Confusion metaphorics:} Figurative uses expressing moral, spiritual, or intellectual
confusion, entrapment, or the search for guidance and clarity.

\item[4 --] \textbf{Scientific complexity metaphorics:}
Uses in scientific or natural-philosophical discourse where “labyrinthus” conveys
systemic or structural intricacy (as in nature, geometry, alchemy, or method).

\item[5 --] \textbf{Medical anatomical:}
Anatomical or physiological contexts, especially references to the inner ear
and related bodily structures.
Applies even within scientific treatises.

\item[6 --] \textbf{Ambiguous or indeterminate:}
Bibliographic entries, cross-references, or fragmentary cases where the intended
sense cannot be determined.
\end{enumerate}

The prompt was then executed on our local server machine using Lamma 4 Scout (109B) LLM \cite{meta2024llama} as available through the ollama library \cite{ollama2023} (The full prompt is available as Appendix \ref{appdx:prompt}). Subsequently, we repeatedly inspected a random sample of 100 sentences to evaluate the performance of the LLM and the embedding-based models (see Appendix \ref{appdx:labellingResults}).

\subsection{Embeddings}

Using these annotated instances, we proceeded to employ the XLM-RoBERTa-BASE (hence XLM-R) \cite{conneau2019unsupervised} and Latin BERT (hence LaBERT) \cite{bamman2020latin} models to obtain contextualized word embeddings of the target token. To investigate the semantic signal distributed across the encoder stack, we extracted target-token embeddings from the last 5 hidden layers of the model (i.e., layers 8--12 in 1-based indexing) for both sentence and concordance contexts.

In addition to the out-of-the-box models, we further fine-tuned Latin BERT specifically for Word Sense Disambiguation (WSD) using a combination of manually and automatically sense-annotated resources. Training pairs were constructed from three complementary datasets: (1) the Latin subset of the SemEval lexical semantic change task \cite{schlechtweg_semeval-2020_2020}, (2) the dictionary-based sense usage dataset accompanying the publication of Latin BERT \cite{bamman2020latin}, and (3) the silver sense-labeled corpus produced by \cite{ghinassi-etal-2024-LatinDisambiguation}. Each pair consisted of two contextualized occurrences of the same lemma labeled as either expressing the same or different senses. The model was trained in a contrastive WiC-style setup, starting from a frozen encoder and subsequently unfreezing the top three transformer layers for controlled fine-tuning. This procedure yielded a WSD-optimized Latin BERT variant fine-tuned for word-sense discrimination in historical Latin.

To extract attention-based features, we computed layer-specific attention distributions centered on a target lemma. For each sentence or concordance window, we identified the subword tokens corresponding to the target lemma and retrieved the attention scores from the rest of the input to the target. We averaged the attention scores across the subword span and selected the top-$k$ attention heads with the highest total attention mass. The final attention vector was obtained by averaging the attention distributions of these top heads. This vector was then aggregated by lemma: for each non-target lemma in the input, we summed the attention weights assigned to its subword tokens. This yielded a dictionary mapping each lemma to its total attention score and providing the individual subword-level contributions.

These token-weight dictionaries were then used to construct attention embeddings. For each context, we created a high-dimensional sparse vector representing attention weights over a shared vocabulary of lemmata, restricted to tokens morphologically tagged as nouns, verbs, adjectives, or proper names, and occurring in at least two contexts. These serve as our raw attention embeddings. To further reduce dimensionality and mitigate sparsity, we applied Truncated Singular Value Decomposition (SVD), projecting the raw attention vectors into a 400-dimensional dense space that retained over 95\% of the variance in the original matrix.

This procedure yielded 30 plus 30 plus 18 embedding variants per instance of \emph{labyrinthus}: 5 target-lemma contextual embeddings, 5 attention-based raw embeddings, and 5 attention-based SVD-reduced embeddings, for each of the two context types (sentence and concordance) and base model (XLM-R and LaBERT), together with 3 target-lemma contextual embeddings, 3 attention-based raw embeddings, and 3 attention-based SVD-reduced embeddings, for each of the two context types (sentence and concordance) and the WSD-optimzed LaBERT model (as only the last 3 layers of the encoder were finetuned).

As a non-transformer baseline, we also constructed word-document co-occurrence matrices from both sentence and concordance contexts, using the same lemmatized vocabulary as in the attention embeddings. Again, we retained both their sparse and SVD-reduced forms. Altogether, this yields a total of 82 individual embedding variants.

\subsection{Word Sense Disambiguation Task}

To evaluate the effectiveness of the embedding variants described above, we framed the task as a multiclass word sense disambiguation (WSD) problem. Each instance of the target lemma \emph{labyrinthus} was assigned a sense label using our large language model (LLM)-based classification pipeline (described in Appendix ~\ref{appdx:prompt}). We treated these labels as ground-truth classes and tested how well different embedding representations could predict them.

We experimented with four supervised classification algorithms, using the embedding vectors as input features:

\begin{itemize}
\item \textbf{Multinomial Logistic Regression (LR)} \cite{theil1969multinomial}: A linear classifier that models class probabilities via a softmax function over linear combinations of input features.

\item \textbf{Random Forest (RF)} \cite{breiman2001random}: An ensemble of decision trees trained via bootstrap aggregation (bagging), with random feature selection at each split to reduce overfitting and increase robustness.

\item \textbf{Histogram-based Gradient Boosting (HGB)} \cite{ke2017lightgbm}: A boosting method that discretizes continuous input features into histograms, enabling fast training and efficient memory usage while capturing non-linear patterns.

\item \textbf{Multilayer Perceptron (MLP)} \cite{rosenblatt1958perceptron}: A feed-forward neural network with multiple hidden layers trained via backpropagation, capable of learning complex non-linear representations.
\end{itemize}

These classifiers were selected to capture a range of inductive biases and strengths across different types of input representations. Logistic Regression offers a fast, interpretable baseline and performs well on dense, low-dimensional data such as contextual embeddings. However, it struggles with sparse or non-linearly separable inputs. Random Forests are better suited for sparse and categorical features and can model non-linear interactions, though they may be less effective on high-dimensional dense inputs. Histogram-based Gradient Boosting provides a balance between flexibility and efficiency, handling both sparse and dense features while modeling complex interactions. Finally, the Multilayer Perceptron is well-suited for heterogeneous high-dimensional data, especially when combining sparse attention vectors with dense embeddings. Its strong non-linear modeling capacity makes it effective in capturing distributed semantic signal across diverse embedding variants.

All the below introduced analyses and visualizations were implemented using the Python 3 programming language \cite{rossum_python}. We make the code available for reuse via Github: https://github.com/CCS-ZCU/labyrinth.

\section{Results}

\subsection{Automatic Sense Labels and Embedding Projections}

The automatic sense labeling task yielded a distribution of \emph{labyrinthus} instances across the six categories described in the previous section as visualized in Figure~\ref{fig:labels-distribution}. This distribution provides a first overview of the dominant and rare senses identified by the LLM classifier.

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures/labyrinthus_classification_single_label_barplot.png}
\caption{Distribution of \emph{labyrinthus} sense labels across the dataset}
\label{fig:labels-distribution}
\end{figure}

As an initial sanity check and exploratory step, we examined 2-dimensional projections of the embedding variants using three dimensionality reduction techniques—UMAP, t-SNE, and PCA—each with three different parameterizations. These visualizations helped assess whether different senses exhibit distinguishable structure in embedding space.

Figure~\ref{fig:projections-baseline} shows the projections of the sentence-based co-occurrence baseline embeddings (reduced to 400 dimensions using SVD). Despite their simplicity, these embeddings reveal some meaningful clustering—most notably for the \emph{medical\_anatomical} sense (highlighted in red), which appears to be clearly separated. For comparison, Figure~\ref{fig:projections-concatenated} displays projections based on concatenated contextual and attention embeddings from layer 10 (1-based index) of WSD-optimized Latin BERT. These patterns appear more diffuse, suggesting a richer and less easily interpretable feature space.

Full projections for all 82 embedding variants and their combinations are available in the project repository under the path \texttt{./figures/labyrinthus\_projections\_*.png}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures/labyrinthus_projections_svd400.png}
\caption{2D projections of the co-occurrence baseline embeddings (400D SVD of sentence-level co-occurrence matrix) using UMAP, t-SNE, and PCA.}
\label{fig:projections-baseline}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{figures/labyrinthus_projections_labert_wsd_embed_l9_labert_wsd_AND_att_l9_labert_wsd_svd400_embed.png}
\caption{2D projections of concatenated contextual and attention embeddings from layer 10 of WSD-optimized Latin BERT using UMAP, t-SNE, and PCA.}
\label{fig:projections-concatenated}
\end{figure}

\subsection{Sense Classification Evaluation}

To quantitatively evaluate the semantic signal captured by different embedding variants, we trained supervised classification models with embeddings as predictors and LLM-assigned labels as classes. In addition to models based on individual embedding types, we evaluated combinations of contextual and attention embeddings, both raw and SVD-projected, as well as combinations with the baseline co-occurrence vectors. In total, this resulted in 186 input configurations. For each of the input configurations, we trained classification models using four different classification algorithms (LR, RF, HGB, MLP). To account for random variation and to allow a more rigid performance evaluation, each input configuration and classification model pair were trained ten times to account for random variation, and macro-averaged F1 scores were computed across runs. This enabled us to identify the best performing classification algorithm for the different embedding types and their combinations (For full overview of the results, with mean F1 scores and standard deviation across the 10 training iterations for each classification model and each input configuration, see the CSV file \texttt{./data/labyrinthus\_classification\_results.csv} in the project repository).

Regarding individual classification algorithms, we observe that

\begin{itemize}
\item \textbf{Logistic Regression (LR)} performed reasonably well in dense contextual embeddings, but failed to detect signals in inputs based on sparse raw attention embeddings.
\item \textbf{Random Forest (RF)} showed strong results with sparse attention embeddings, even after SVD projection, but underperformed on dense inputs.
\item \textbf{Histogram-based Gradient Boosting (HGB)} offered balanced performance across input types but was consistently outperformed by MLP.
\item \textbf{Multilayer Perceptron (MLP)} achieved the best performance across the majority of embedding variants, particularly with combined dense and sparse features.
\end{itemize}

The best-performing configuration was an MLP model trained on a concatenation of:
\begin{itemize}
\item contextual embeddings (XLM-R, concordance context, layer 8),
\item raw attention embeddings (XLM-R, concordance context, layer 8), and
\item the concordance-level co-occurrence baseline (raw).
\end{itemize}
This model achieved an average F1 score of 0.678365, with some runs reaching up to 0.689177.

Interestingly, the co-occurrence baseline alone yielded relatively strong performance, surpassing most attention-only and contextual-only models. This suggests that the distributional information captured by count-based methods provides complementary signal that is not fully recoverable from transformer-based embeddings.

We also find that across all model types, XLM-R embeddings generally outperformed those from Latin BERT, though the margin was often small. No consistent pattern emerged regarding layer-specific performance; while deeper layers (e.g., layer 12) sometimes provided the clearest signal for classification, lower layers (especially 8 and 9) occasionally yielded more visually coherent clusters in projection plots.

We observed a slight advantage of concordance-level context across the board, indicating that the fluctuating length of input sentences can negatively affect the performance of the models.

While using an LLM for both annotation and evaluation ensured internal consistency, it also risked a degree of circular validation, since the same model family defined and then tested the category boundaries. To address this limitation and independently assess reliability, we manually annotated a sample of 100 instances of labyrinthus and re-evaluated the best-performing classifiers against this human ground truth. The resulting confusion matrices (Figure \ref{fig:confusion1}) illustrate that the medical\_anatomical sense is consistently the most distinct and reliably identified category. In contrast, the mythological and technical\_literal senses appear more easily conflated with the two metaphorical classes, suggesting that their boundaries are semantically or contextually less sharp in the data.

For the more abstract or metaphorical usages, both attention-based embeddings and the co-occurrence baseline contributed essential discriminative cues, highlighting that distributional and contextual information capture complementary aspects of meaning. Notably, the WSD-optimized Latin BERT exhibited a relatively strong ability to distinguish the mythological sense, performing comparably to the best XLM-R variants despite the limited sample size.

Overall, this diagnostic test should be viewed as an initial validation rather than a definitive benchmark, given the small and manually curated evaluation set. Nevertheless, the results indicate that models trained on LLM-labeled data retain meaningful signal when applied to human-verified sense distinctions.

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures/labyrinthus_confusion_matrix_best_models_GT.png}
\caption{Confusion matrices of the six best-performing models evaluated on a manually annotated sample of 100 labyrinthus instances, showing F1 results per class and misclassification patterns.}
\label{fig:confusion1}
\end{figure}

Finally, to explore temporal trends, we extended our 2D projection setup by adding a third dimension: the date of composition of the containing text. Figure~\ref{fig:3d-projection} shows an example 3D projection using raw attention embeddings from layer 12 of WSD-optimized Latin BERT. Here we clearly observe the rise of the \emph{medical\_anatomical} sense and \emph{scientific\_complexity\_metaphorics} in Early Modern texts, in contrast to the \emph{mythological} and \emph{technical\_literal} sense, which dominates earlier periods. This illustrates the potential of transformer-based attention embeddings for studying semantic change over time.

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures/labyrinthus_3d_projection.png}
\caption{3D projection of attention embeddings from layer 12 of WSD-optimized Latin BERT, with color-coded sense labels and date on the z-axis.}
\label{fig:3d-projection}
\end{figure}

\section{Discussion}

In this study, we explored how historically motivated research on keywords-in-context can be enriched by transformer-based approaches to word sense disambiguation (WSD). Using the Latin term labyrinthus as a case study, we designed a series of experiments to assess the effectiveness of various embedding strategies and their combinations in predicting the correct sense of the target word in context.

Several limitations of our approach warrant discussion.

First, the sense labels used as ground truth were generated through iterative prompting of a large language model (LLM). While the resulting categories were refined through close reading and expert review, we acknowledge that the boundaries between senses are fluid and subject to interpretative disagreement. In this light, the LLM-based labels function more as an operational taxonomy than as a fixed ground truth. Nevertheless, the observed stability of the classifications—both across runs and in comparison with human annotation—suggests that such LLM-guided annotation can serve as a viable proxy for large-scale exploratory work, especially when combined with targeted expert validation.

Second, the conclusions we draw are necessarily limited by the scope of this single-term case study. Although labyrinthus provides a particularly rich and polysemous example—ranging from mythological and anatomical to philosophical and metaphorical uses—further work is needed to generalize these findings. In future research, we plan to extend our methodology to other semantically complex Latin terms with strong metaphorical potential, such as mercurius. The bilingual structure of the GreLa corpus also enables comparative analysis across Latin and Greek, offering new ways to model conceptual transfer and semantic interaction between the two languages.

Finally, our experiments only lightly addressed the diachronic dimension. Building on the initial 3D projections presented here, future work will explicitly model semantic change over time, tracing the emergence, dissemination, and transformation of senses across historical periods and genres.

Overall, this study demonstrates that the integration of LLM-based annotation with contextual, attention-based, and classical distributional embeddings can advance the precision and interpretive reach of computational philology. By linking quantitative modeling with philological judgment, such hybrid methods can help bridge the gap between statistical regularity and historical meaning—a step toward more systematic, yet still interpretively sensitive, digital scholarship in the humanities.

\newpage
\section*{Acknowledgements}

The work of VK and PP was supported by the TOME project (Ministry of Education, Youth and Sports of the Czech Republic, ERC CZ, project no. LL 2320). Special thanks go to Farzad Mahootian (NYU New York), whose invitation of VK, SL, and PP to the workshop “Alchemy of Global Partnerships” (NYU Abu Dhabi, May 14–17, 2025) brought the three of us together and inspired the inception of this project.

\printbibliography

\appendix

\appendix
\section{LLM classification prompt}\label{appdx:prompt}

To support the annotation of word sense labels, we implemented a lightweight prompt-based classification system using a local LLM API (based on \texttt{llama4:17b-scout-16e-instruct-q4\_K\_M}). The model was prompted with a few-shot instruction template and returned a numerical label representing one of ten predefined semantic categories. Each occurrence of the word \textit{labyrinthus} was classified into exactly one of the six categories.

\paragraph{Prompt}
The prompt included the classification instruction and a few-shot setup with three labeled examples. Each prediction was obtained by inserting the test passage as the final example:

\footnotesize
\begin{verbatim}
You are a Latin philologist and semantic analyst.
Classify how “labyrinthus” is used in a Latin passage.

Return exactly ONE digit (0–5). No words, no punctuation.

Categories:
0 – mythological
Explicit reference to the Cretan myth (Daedalus, Minotaur, Ariadne, Theseus, Crete, Ovid). Not general architectural or technical uses.

1 – technical_literal
Literal descriptions of built or natural labyrinths (buildings, caves, gardens, fishing traps, mines). No metaphor or abstraction.

2 – confusion_metaphorics
Moral, spiritual, or epistemic confusion, entrapment, or rhetorical intricacy.
Common cues: error, confusio, filum (gratiae), via/exitus, anima/animus, peccatum, sophistae.

3 – scientific_complexity_metaphorics
Scientific or natural-philosophical contexts where “labyrinthus” symbolizes complexity, method, or structure (astronomy, geometry, alchemy, brain/nature as system).
Common cues: methodus, hypotheseis, experimentum, calculi, astronomia, geometria, elementa, natura, systema.

4 – medical_anatomical
Anatomical/physiological context (esp. inner ear): auris, cochlea, vestibulum, tympanum, nervus, membrana, meatus, aquaeductus, os petrosum. Use 4 even in scientific treatises.

5 – ambiguous_indeterminate
Bibliographic/index mentions, titles, or genuinely unclear/fragmentary contexts.

Decision rules:
• Body parts/functions → 4
• Concrete real-world structures (literal) → 1
• Scientific/systemic metaphor → 3
• Moral/spiritual/epistemic confusion → 2
• Mythic narrative → 0
• Indeterminate or purely bibliographic → 5

Clarifications:
– Use 4 only for real anatomy (esp. inner ear). If the body is used metaphorically/structurally (e.g., cerebrum as a labyrinth), choose 3.
– Prefer 3 for scientific or methodical reasoning about complex systems, even when Ariadne/filum imagery appears.
– Use 2 for human moral or intellectual struggle, not for technical/analytic complexity.

\end{verbatim}
\normalsize

\paragraph{Prediction Protocol}
Each passage was submitted as a JSON payload via a REST API call. The model's prediction was parsed as an integer and matched against the predefined label map. This setup allowed us to rapidly assign semantic class labels to all instances of \textit{labyrinthus} in the corpus for supervised evaluation of embedding-based classifiers.

\section{Discussion of results of the LLM labelling task}\label{appdx:labellingResults}

In the classification task, human annotations were compared with outputs from the LLM classification and the predictions of the best performing models. Overall performance was strong across methods, with high agreement between human and machine annotations. Discrepancies typically arose from differences in granularity or interpretive framing rather than from clear misclassifications.

The category system employed had a substantial impact on outcomes. Categories were selected to remain manageable, based initially on LLM-generated suggestions and refined to reduce ambiguity. However, the chosen set sometimes failed to reflect the nuances of metaphorical usage.
Overlapping or closely related domains frequently produced borderline cases where both human and machine classifications varied.

However, misclassifications did occur. These were most serious when the LLM or the embedding based models failed to recognise clear disciplinary contexts, such as alchemical passages referencing Paracelsus, or astronomical texts dense with technical vocabulary. Such errors suggest limitations in the model, particularly in distinguishing overlapping terminology common in Early Modern texts. In these cases, domain-specific indicators that are readily identified by human reviewers were not consistently picked up by the automatic classifications.

The classification of metaphor relied on contextual cues rather than on function or type, echoing simple KWIC-based distributional semantics techniques previously suggested for the disambiguation of alchemical nomenclature through surrounding keywords \cite{Lang2022decknamen}. While effective in many cases, this approach would benefit from refinement. Incorporating bibliographic metadata, field-specific specialist vocabulary and clearly defined category boundaries could improve accuracy, especially in ambiguous cases. For instance, texts from the \textit{EMLAP} or \textit{NOSCEMUS} corpora often signalled alchemical or scholarly contexts that the LLM failed to fully exploit.

Ultimately, model outputs largely aligned with human reasoning, especially where domain-specific vocabulary or concrete cues were present. However, future iterations would benefit from a finer-grained category system and explicit prompts to capture domain knowledge.

\section{Corpus Selection and Limitations}\label{appdx:corpusBias}

\paragraph{Motivation}
Given the size and heterogeneity of available datasets, manual inspection of all occurrences of the term \textit{labyrinthus} proved impractical. Most instances are likely non-metaphorical and thus fall outside the scope of our investigation. Preliminary classification using LLMs and embedding models provided an effective means of narrowing the dataset to relevant cases.

\paragraph{The aggregate corpus and its components}
GreLa \cite{CCS2025_GreLaCorpus} constitutes an extensive corpus encompassing Greek and Latin literature from the eighth century BCE to the seventeenth century CE.

It comprises over 11,000 individual works, amounting to approximately 26 million sentences and 380 million tokens. The corpus is the result of an integration of several pre-existing collections.

Besides a Greek component derived from the LAGT (Lemmatized Ancient Greek Texts) Corpus \cite{Kase2024_LAGT_chrLabyrinth}, which consolidates ancient Greek texts from multiple sources, including the Perseus Digital Library, \textit{The First Thousand Years of Greek}, Glaux, and the OGA corpus but is not directly relevant to our current case study, there is an extensive Latin component. It incorporates material from \textit{Corpus Corporum} \cite{Roelli2014_CorpusCorporum_chrLabyrinth}, a broad-based Latin literature repository, as well as two databases focused on early modern scientific and alchemical texts: NOSCEMUS \cite{Korenjak2023_NoscemusProject_chrLabyrinth, Froestl2023_TranskribusAlchemica_chrLabyrinth, Zathammer2025_NoscemusSourcebook_chrLabyrinth}, which covers early modern scientific literature, and \textit{Early Modern Latin Alchemical Prints} (EMLAP, \cite{Hedesan2025_EMLAP_chrLabyrinth}).

Our corpora vary significantly in composition and purpose. The \textit{NOSCEMUS} corpus, designed to trace the development of early modern science from circa 1450 to 1850, offers relatively balanced coverage across centuries. The \textit{EMLAP} corpus focuses on sixteenth-century Latin alchemical texts, though some texts may predate this period as the inclusion in the corpus is based only on appearance date of the specific edition included. These two corpora represent the quite systematically assembled components for their specific aims.
In contrast, the \textit{Corpus Corporum} sources are more eclectic in their coverage. The corpus is shaped by availability of digitized sources, rather than a design aimed at thematic or chronological balance.

\paragraph{Representativeness}
This unevenness in the GreLa corpus introduces bias. Scientific and alchemical texts are likely overrepresented, while other genres remain under-sampled. The dataset is therefore not suitable for answering general questions about metaphor usage in Latin literature. However, it is well suited to tracing metaphorical patterns in early modern scientific and alchemical writing, which we are particularly interested in. We expect these to be particularly present in paratextual or introductory sections of alchemical works.

We also had to contend with the diachronic range of the corpus, which complicates metaphor identification due to shifting language use. Nonetheless, plotting metaphorical occurrences over time and by context remains potentially  informative, provided these limitations are kept in view.

Although the corpus is imbalanced and incomplete, these features do not invalidate our method. These computational tools are used to guide close reading rather than replace it. In this sense, the visualisations and outputs are exploratory, pointing toward promising passages for detailed analysis in the form of close readings.

Indeed, the very metaphor that sparked our interest in this study -- an intriguing alchemical use of \textit{labyrinthus} as a metaphor -- lies outside the scope of this corpus, underscoring that the methodological value of our study is not necessarily tied to the exhaustiveness of our corpus.

Critiques of digital humanities often target the representativeness of corpora. While such concerns are valid, they apply equally to traditional scholarship, which typically relies on a narrow set of texts due to practical constraints. Our approach seeks to expand the range of material available for interpretation while remaining conscious of its limitations. The digital method serves as a tool for identifying sources, not a substitute for historical analysis. As such, corpus imbalance, though significant, is not a decisive flaw for the kind of exploratory work undertaken here.

\end{document}