@book{homer_odyssey_1999,
	title = {The {Odyssey}},
	url = {https://www.gutenberg.org/ebooks/1728},
	urldate = {2024-01-13},
	publisher = {Project Gutenberg},
	author = {{Homer}},
	translator = {Butcher, Samuel Henry and Lang, Andrew},
	year = {1999},
}

@book{woolf_mrs_2023,
	title = {Mrs. {Dalloway}},
	url = {https://www.gutenberg.org/ebooks/71865},
	urldate = {2024-01-13},
	publisher = {Project Gutenberg},
	author = {Woolf, Virginia},
	year = {2023},
}

@book{king_library_2003,
	address = {Pullman, Washington},
	title = {The {Library} of {Leonard} and {Virginia} {Woolf}: {A} {Short}-{Title} {Catalog}},
	isbn = {0-87422-270-2},
	publisher = {Washington State University Press},
	author = {King, Julia and Miletic-Vejzovic, Laila},
	year = {2003},
	keywords = {1880-1969—Library—Catalogs, 1882-1941—Library—Catalogs, Leonard, Private libraries—England—Catalogs, Rare books, Virginia, Woolf},
}

@book{pichler_llms_2024,
	title = {»{LLMs} for everything?« {Potentiale} und {Probleme} der {Anwendung} von {In}-{Context}-{Learning} für die {Computational} {Literary} {Studies}},
	url = {https://doi.org/10.5281/zenodo.10698510},
	publisher = {Zenodo},
	author = {Pichler, Axel and Reiter, Nils},
	month = feb,
	year = {2024},
	doi = {10.5281/zenodo.10698510},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{abdin_phi-4_2024,
	title = {Phi-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2412.08905},
	doi = {10.48550/arXiv.2412.08905},
	abstract = {We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, Sébastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J. and Javaheripi, Mojan and Kauffmann, Piero and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liu, Weishung and Mendes, Caio C. T. and Nguyen, Anh and Price, Eric and Rosa, Gustavo de and Saarikivi, Olli and Salim, Adil and Shah, Shital and Wang, Xin and Ward, Rachel and Wu, Yue and Yu, Dingli and Zhang, Cyril and Zhang, Yi},
	month = dec,
	year = {2024},
	note = {arXiv:2412.08905 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{openai_gpt-4o_2024,
	title = {{GPT}-4o {System} {Card}},
	url = {http://arxiv.org/abs/2410.21276},
	doi = {10.48550/arXiv.2410.21276},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {{OpenAI}},
	month = oct,
	year = {2024},
}

@phdthesis{mccotter_septimus_2020,
	title = {Septimus {Smith} {Had} to {Die}: {An} {Examination} of {Virginia} {Woolf}’s {Frustration} with the {Mental} {Health} {System} {After} {WWI}},
	url = {https://scholar.umw.edu/student_research/317},
	school = {University of Mary Washington},
	author = {McCotter, Mackenzie K.},
	year = {2020},
}

@misc{parnami_learning_2022,
	title = {Learning from {Few} {Examples}: {A} {Summary} of {Approaches} to {Few}-{Shot} {Learning}},
	shorttitle = {Learning from {Few} {Examples}},
	url = {http://arxiv.org/abs/2203.04291},
	doi = {10.48550/arXiv.2203.04291},
	abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently1 proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
	language = {en},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Parnami, Archit and Lee, Minwoo},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04291 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	language = {en},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{gu_survey_2024,
	title = {A {Survey} on {LLM}-as-a-{Judge}},
	url = {http://arxiv.org/abs/2411.15594},
	doi = {10.48550/arXiv.2411.15594},
	abstract = {Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and realworld deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. The associated resources can be accessed at https://github.com/IDEA-FinAI/LLM-as-a-Judge.},
	language = {en},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and Wang, Yuanzhuo and Guo, Jian},
	month = dec,
	year = {2024},
	note = {arXiv:2411.15594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{umphrey_investigating_2024,
	title = {Investigating {Expert}-in-the-{Loop} {LLM} {Discourse} {Patterns} for {Ancient} {Intertextual} {Analysis}},
	url = {http://arxiv.org/abs/2409.01882},
	doi = {10.48550/arXiv.2409.01882},
	abstract = {This study explores the potential of large language models (LLMs) for identifying and examining intertextual relationships within biblical, Koine Greek texts. By evaluating the performance of LLMs on various intertextuality scenarios the study demonstrates that these models can detect direct quotations, allusions, and echoes between texts. The LLM’s ability to generate novel intertextual observations and connections highlights its potential to uncover new insights. However, the model also struggles with long query passages and the inclusion of false intertextual dependences, emphasizing the importance of expert evaluation. The expert-in-the-loop methodology presented offers a scalable approach for intertextual research into the complex web of intertextuality within and beyond the biblical corpus.},
	language = {en},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Umphrey, Ray and Roberts, Jesse and Roberts, Lindsey},
	month = sep,
	year = {2024},
	note = {arXiv:2409.01882 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{kristeva_desire_2024,
	address = {New York},
	series = {European perspectives},
	title = {Desire in language: a semiotic approach to literature and art},
	shorttitle = {Desire in language},
	publisher = {Columbia University Press},
	author = {Kristeva, Julia and Roudiez, Leon S. and Gora, Thomas and Jardine, Alice and Kristeva, Julia and Kristeva, Julia},
	year = {2024},
	keywords = {Criticism, Essays, Semiotics and art, Semiotics and literature},
}

@book{barthes_image_1977,
	address = {London},
	edition = {13. [Dr.]},
	title = {Image, music, text: essays},
	isbn = {978-0-00-686135-5},
	shorttitle = {Image, music, text},
	publisher = {Fontana},
	author = {Barthes, Roland and Heath, Stephen},
	year = {1977},
}

@book{genette_palimpsestes_2014,
	address = {Paris},
	series = {Collection {Poétique}},
	title = {Palimpsestes: la littérature au second degré},
	isbn = {978-2-02-118401-3},
	shorttitle = {Palimpsestes},
	language = {fre},
	number = {33},
	publisher = {Éditions du Seuil},
	author = {Genette, Gérard},
	year = {2014},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	language = {en},
	urldate = {2024-12-20},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@book{genette_palimpsests_1997,
	series = {Stages ({Series})},
	title = {Palimpsests: {Literature} in the {Second} {Degree}},
	isbn = {978-0-8032-7029-9},
	url = {https://books.google.de/books?id=KbYzNp94C9oC},
	publisher = {University of Nebraska Press},
	author = {Genette, G.},
	year = {1997},
	lccn = {96051576},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	language = {en},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{pichler_concepts_2022,
	title = {From {Concepts} to {Texts} and {Back}: {Operationalization} as a {Core} {Activity} of {Digital} {Humanities}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {2371-4549},
	shorttitle = {From {Concepts} to {Texts} and {Back}},
	url = {https://culturalanalytics.org/article/57195-from-concepts-to-texts-and-back-operationalization-as-a-core-activity-of-digital-humanities},
	doi = {10.22148/001c.57195},
	abstract = {This article puts operationalization as a research practice and its theoretical consequences into focus. As all sciences as well as humanities areas use concepts to describe their realm of investigation, digital humanities projects are usually faced with the challenge of ‘bridging the gap’ from theoretical concepts (whose meaning(s) depend on a certain theory and which are used to describe expectations, hypothesis and results) to results derived from data. The process of developing methods to bridge this gap is called ‘operationalization’, and it is a common task for any kind of quantitative, formal, or digital analysis. Furthermore, operationalization choices have long-lasting consequences, as they (obviously) influence the results that can be achieved, and, in turn, the possibilities to interpret these results in terms of the original research question. However, even though this process is so important and so common, its theoretical consequences are rarely reflected. Because the concepts that are operationalized cannot be operationalized in isolation, operationalizing is not only an engineering or implementation challenge, but touches on the theoretical core of the research questions we work on, and the fields we work in.
            In this article, we first clarify the need to operationalize on selected, representative examples, situate the process within typical DH workflows, and highlight the consequences that operationalization decisions have. We will then argue that operationalization plays such a crucial role for the digital humanities that any kind of theory needs to take off from operationalization practices. Based on these assumptions, we will develop a first scheme of the constraints and necessities of such a theory and reflect their epistemic consequences.},
	language = {en},
	number = {4},
	urldate = {2024-12-14},
	journal = {Journal of Cultural Analytics},
	author = {Pichler, Axel and Reiter, Nils},
	month = dec,
	year = {2022},
}

@article{alfaro_intertextuality_1996,
	title = {Intertextuality: {Origins} and {Development} of the {Concept}},
	volume = {18},
	issn = {0210-6124},
	shorttitle = {Intertextuality},
	url = {https://www.jstor.org/stable/41054827},
	abstract = {The analysis of the concept of intertextuality carried out in this essay begins with a survey of the various ways in which the subject appears before Kristeva's introduction of the term as such. Then it concentrates on Bakhtin's and Kristeva's role as the first contributors to the development of a theory of intertextuality. The detailed sudy of the wide variety of perspectives from which the phenomenon and its increasing relevance have been approached by later critics constitutes the last part of the essay.},
	number = {1/2},
	urldate = {2024-06-12},
	journal = {Atlantis},
	author = {Alfaro, María Jesús Martínez},
	year = {1996},
	note = {Publisher: AEDEAN: Asociación española de estudios anglo-americanos},
	pages = {268--285},
}

@article{hoff_pseudo-homeric_1999,
	title = {The {Pseudo}-{Homeric} {World} of {Mrs}. {Dalloway}},
	volume = {45},
	issn = {0041462X},
	url = {https://www.jstor.org/stable/441830?origin=crossref},
	doi = {10.2307/441830},
	language = {en},
	number = {2},
	urldate = {2024-05-11},
	journal = {Twentieth Century Literature},
	author = {Hoff, Molly},
	year = {1999},
	pages = {186},
}

@incollection{randall_woolf_2012,
	edition = {1},
	title = {Woolf and {Intertextuality}},
	url = {https://www.cambridge.org/core/product/identifier/9780511777103%23c00361-5-1/type/book_part},
	language = {en},
	urldate = {2024-01-03},
	booktitle = {Virginia {Woolf} in {Context}},
	publisher = {Cambridge University Press},
	author = {Fernald, Anne E.},
	editor = {Randall, Bryony and Goldman, Jane},
	month = dec,
	year = {2012},
	doi = {10.1017/CBO9780511777103.007},
	pages = {52--64},
}

@article{schubert_intertextuality_2020,
	title = {Intertextuality and {Digital} {Humanities}},
	volume = {62},
	issn = {2196-7032, 1611-2776},
	url = {https://www.degruyter.com/document/doi/10.1515/itit-2019-0036/html},
	doi = {10.1515/itit-2019-0036},
	abstract = {Proceeding from the debate on intertextuality, some considerations are presented here for Literary and Historical Studies that suggest a theory-driven approach applying algorithm-based procedures. It will be shown that methodical tensions between qualitative and quantitative approaches can be solved simultaneously in this way. On this basis, the approach combines intertextuality theory with an algorithm-based procedure (here a search based on Word Mover’s Distance).},
	language = {en},
	number = {2},
	urldate = {2024-01-02},
	journal = {it - Information Technology},
	author = {Schubert, Charlotte},
	month = apr,
	year = {2020},
	pages = {53--59},
}

@inproceedings{liebl-burghardt-2020-shakespeare,
    title = "\enquote{Shakespeare in the Vectorian Age} {--} An evaluation of different word embeddings and {NLP} parameters for the detection of Shakespeare quotes",
    author = "Liebl, Bernhard  and
      Burghardt, Manuel",
    editor = "DeGaetano, Stefania  and
      Kazantseva, Anna  and
      Reiter, Nils  and
      Szpakowicz, Stan",
    booktitle = "Proceedings of the 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",
    month = dec,
    year = "2020",
    address = "Online",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.latechclfl-1.7/",
    pages = "58--68",
}

@article{10.1093/llc/fqr009,
    author = {Kane, Andrew and Tompa, Frank Wm.},
    title = {Janus: the intertextuality search engine for the electronic Manipulus florum project},
    journal = {Literary and Linguistic Computing},
    volume = {26},
    number = {4},
    pages = {407-415},
    year = {2011},
    month = {05},
    issn = {0268-1145},
    doi = {10.1093/llc/fqr009},
    url = {https://doi.org/10.1093/llc/fqr009},
    eprint = {https://academic.oup.com/dsh/article-pdf/26/4/407/6191804/fqr009.pdf},
}

@incollection{kups70449,
            year = {2023},
          volume = {16},
           pages = {69--87},
       booktitle = {Digitale Edition in {\"O}sterreich Digital Scholarly Edition in Austria},
          editor = {Roman Bleier and Helmut Klug W.},
          author = {Bernhard Oberreither},
           title = {A Linked Data Vocabulary for Intertextuality in Literary Studies, with some Considerations Regarding Digital Editions},
       publisher = {BoD},
         address = {Norderstedt},
             url = {https://kups.ub.uni-koeln.de/70449/},
        keywords = {Digital Humanities ; Digitale Editionen}
}

@inproceedings{schlupkothen-formit-2019,
    author = {Frederik Schlupkothen and Julia Nantke},
    title = {FormIt: Eine multimodale Arbeitsumgebung zur systematischen Erfassung literarischer Intertextualität},
    booktitle = {Book of Abstracts of DHd},
    year = 2019,
    doi = {10.5281/zenodo.4622105}
}

@inproceedings{kraneiss_2025_14943022,
  author       = {Kraneiß, Natalie and
                  Frank, Ingo and
                  Horstmann, Jan and
                  Normann, Immanuel},
  title        = {Die Modellierung von intertextuellen Bezügen in
                   genealogischen Texten
                  },
  month        = feb,
  year         = 2025,
    booktitle = {Book of Abstracts of DHd},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.14943022},
  url          = {https://doi.org/10.5281/zenodo.14943022},
}

@book{kristeva_mot_1969,
  author = {Kristeva, Julia},
  title = {Séméiôtiké: recherches pour une sémanalyse},
  publisher = {Éditions du Seuil},
  address = {Paris},
  year = {1969},
  note = {Originally published in Tel Quel, 1967}
}
