@article{bamman2024classification,
  title={On Classification with Large Language Models in Cultural Analytics},
  author={Bamman, David and Chang, Kent K and Lucy, Li and Zhou, Naitian},
  journal={arXiv preprint arXiv:2410.12029},
  year={2024}
}

@inproceedings{gonzalez2023yes,
  title={Yes but.. can chatgpt identify entities in historical documents?},
  author={Gonz{\'a}lez-Gallardo, Carlos-Emiliano and Boros, Emanuela and Girdhar, Nancy and Hamdi, Ahmed and Moreno, Jose G and Doucet, Antoine},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
  pages={184--189},
  year={2023},
  organization={IEEE}
}

@book{guldi2023dangerous,
  title={The dangerous art of text mining: A methodology for digital history},
  author={Guldi, Jo},
  year={2023},
  publisher={Cambridge University Press}
}

@article{qin2023chatgpt,
  title={Is ChatGPT a general-purpose natural language processing task solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}

@book{liu1995translingual,
  title={Translingual practice: Literature, national culture, and translated modernity—China, 1900-1937},
  author={Liu, Lydia He},
  year={1995},
  publisher={Stanford University Press}
}

@article{stewart2025methodology,
  title={A Methodology for Studying Linguistic and Cultural Change in China, 1900-1950},
  author={Stewart, Spencer Dean},
  journal={arXiv preprint arXiv:2502.04286},
  year={2025}
}

@article{wang2023gpt,
  title={Gpt-ner: Named entity recognition via large language models},
  author={Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2304.10428},
  year={2023}
}

@book{piotrowski2012natural,
  title={Natural language processing for historical texts},
  author={Piotrowski, Michael},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@article{fang2025comparative,
  title={A Comparative Analysis of Word Segmentation, Part-of-Speech Tagging, and Named Entity Recognition for Historical Chinese Sources, 1900-1950},
  author={Fang, Zhao and Wu, Liang-Chun and Kong, Xuening and Stewart, Spencer Dean},
  journal={arXiv preprint arXiv:2503.19844},
  year={2025}
}

@article{zhang2023llmaaa,
  title={Llmaaa: Making large language models as active annotators},
  author={Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  journal={arXiv preprint arXiv:2310.19596},
  year={2023}
}

@article{hiltmann2025ner4all,
  title={NER4all or Context is All You Need: Using LLMs for low-effort, high-performance NER on historical texts. A humanities informed approach},
  author={Hiltmann, Torsten and Dr{\"o}ge, Martin and Dresselhaus, Nicole and Grallert, Till and Althage, Melanie and Bayer, Paul and Eckenstaler, Sophie and Mendi, Koray and Schmitz, Jascha Marijn and Schneider, Philipp and others},
  journal={arXiv preprint arXiv:2502.04351},
  year={2025}
}

@article{xia2000segmentation,
  title={The segmentation guidelines for the Penn Chinese Treebank (3.0)},
  author={Xia, Fei},
  journal={University of Pennsylvania Technical Report, IRCS00-06},
  year={2000}
}

@misc{underwood2025languagemodelsrepresentpast,
      title={Can Language Models Represent the Past without Anachronism?}, 
      author={Ted Underwood and Laura K. Nelson and Matthew Wilkens},
      year={2025},
      eprint={2505.00030},
      archivePrefix={arXiv},
      primaryClass={cs.CL}, 
}

@article{xue2005penn,
  title={The penn chinese treebank: Phrase structure annotation of a large corpus},
  author={Xue, Naiwen and Xia, Fei and Chiou, Fu-Dong and Palmer, Marta},
  journal={Natural language engineering},
  volume={11},
  number={2},
  pages={207--238},
  year={2005},
  publisher={Cambridge University Press}
}

@misc{antoun2024camembert,
  title        = {CamemBERT 2.0: {A} Smarter French Language Model Aged to Perfection},
  author       = {Antoun, Wissam and Kulumba, Francis and Touchent, Rian and de la Clergerie, Éric and Sagot, Benoît and Seddah, Djamé},
  year         = {2024},
  eprint       = {2411.08868},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{10.1145/3458754,
author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
doi = {10.1145/3458754},
abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at .},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
articleno = {2},
numpages = {23},
keywords = {domain-specific pretraining, NLP, Biomedical}
}

@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}

@article{ziems2024can,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={237--291},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{klein2025provocationshumanitiesgenerativeai,
      title={Provocations from the Humanities for Generative AI Research}, 
      author={Lauren Klein and Meredith Martin and André Brock and Maria Antoniak and Melanie Walsh and Jessica Marie Johnson and Lauren Tilton and David Mimno},
      year={2025},
      eprint={2502.19190},
      archivePrefix={arXiv},
      primaryClass={cs.CY}, 
}

@article{underwood2025impact,
  title={The impact of language models on the humanities and vice versa},
  author={Underwood, Ted},
  journal={Nature Computational Science},
  pages={1--3},
  year={2025},
  publisher={Nature Publishing Group US New York}
}

@book{d2023data,
  title={Data feminism},
  author={D'ignazio, Catherine and Klein, Lauren F},
  year={2023},
  publisher={MIT press}
}

@inproceedings{wu2009domain,
  title={Domain adaptive bootstrapping for named entity recognition},
  author={Wu, Dan and Lee, Wee Sun and Ye, Nan and Chieu, Hai Leong},
  booktitle={EMNLP'09 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Volume 3},
  pages={1523--1532},
  year={2009},
  organization={Association for Computing Machinery}
}

@inproceedings{novotny2023people,
  title={People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts},
  author={Novotn{\`y}, V{\'\i}t and Luger, Kristina and {\v{S}}tef{\'a}nik, Michal and Vrabcova, Tereza and Hor{\'a}k, Ale{\v{s}}},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={14104--14113},
  year={2023}
}

@book{rey2013mille,
  title={Mille ans de langue fran{\c{c}}aise, histoire d’une passion},
  author={Rey, Alain and Duval, Fr{\'e}d{\'e}ric and Siouffi, Gilles},
  year={2013},
  publisher={Perrin}
}

@article{alizadeh2025open,
  title={Open-source LLMs for text annotation: a practical guide for model setting and fine-tuning},
  author={Alizadeh, Meysam and Kubli, Ma{\"e}l and Samei, Zeynab and Dehghani, Shirin and Zahedivafa, Mohammadmasiha and Bermeo, Juan D and Korobeynikova, Maria and Gilardi, Fabrizio},
  journal={Journal of Computational Social Science},
  volume={8},
  number={1},
  pages={17},
  year={2025},
  publisher={Springer}
}

@inproceedings{batjargal2014approach,
  title={An approach to named entity extraction from historical documents in traditional Mongolian script},
  author={Batjargal, Biligsaikhan and Khaltarkhuu, Garmaabazar and Kimura, Fuminori and Maeda, Akira},
  booktitle={IEEE/ACM Joint Conference on Digital Libraries},
  pages={489--490},
  year={2014},
  organization={IEEE}
}

@inproceedings{kapan2022fine,
  title={Fine-tuning NER with spaCy for transliterated entities found in digital collections from the multilingual Persian Gulf},
  author={Kapan, Almazhan and Kirmizialtin, Suphan and Kukreja, Rhythm and Wrisley, David Joseph},
  year={2022},
  organization={CEUR Workshop Proceedings}
}

@article{humbel2021named,
  title={Named-entity recognition for early modern textual documents: a review of capabilities and challenges with strategies for the future},
  author={Humbel, Marco and Nyhan, Julianne and Vlachidis, Andreas and Sloan, Kim and Ortolja-Baird, Alexandra},
  journal={Journal of Documentation},
  volume={77},
  number={6},
  pages={1223--1247},
  year={2021},
  publisher={Emerald Publishing Limited}
}
