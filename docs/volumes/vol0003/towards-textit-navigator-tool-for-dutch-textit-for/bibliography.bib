@inbook{koolen_detecting_2022,
title = "Detecting Formulaic Language Use in Historical Administrative Corpora",
author = "Marijn Koolen and F.G. Hoekstra",
year = "2022",
language = "English",
series = "CEUR Workshop Proceedings ",
number = "3290",
pages = "127--151",
editor = "Folgert Karsdorp and Alie Lassche and Kristoffer Nielbo",
booktitle = "Proceedings of the Computational Humanities Research Conference 2022",
}

@article{guralnick_humans_2024,
	title = {Humans in the loop: {Community} science and machine learning synergies for overcoming herbarium digitization bottlenecks},
	volume = {12},
	copyright = {© 2024 The Authors. Applications in Plant Sciences published by Wiley Periodicals LLC on behalf of Botanical Society of America.},
	issn = {2168-0450},
	shorttitle = {Humans in the loop},
	doi = {10.1002/aps3.11560},
	abstract = {Premise Among the slowest steps in the digitization of natural history collections is converting imaged labels into digital text. We present here a working solution to overcome this long-recognized efficiency bottleneck that leverages synergies between community science efforts and machine learning approaches. Methods We present two new semi-automated services. The first detects and classifies typewritten, handwritten, or mixed labels from herbarium sheets. The second uses a workflow tuned for specimen labels to label text using optical character recognition (OCR). The label finder and classifier was built via humans-in-the-loop processes that utilize the community science Notes from Nature platform to develop training and validation data sets to feed into a machine learning pipeline. Results Our results showcase a {\textgreater}93\% success rate for finding and classifying main labels. The OCR pipeline optimizes pre-processing, multiple OCR engines, and post-processing steps, including an alignment approach borrowed from molecular systematics. This pipeline yields {\textgreater}4-fold reductions in errors compared to off-the-shelf open-source solutions. The OCR workflow also allows human validation using a custom Notes from Nature tool. Discussion Our work showcases a usable set of tools for herbarium digitization including a custom-built web application that is freely accessible. Further work to better integrate these services into existing toolkits can support broad community use.},
	language = {en},
	number = {1},
	urldate = {2025-04-15},
	journal = {Applications in Plant Sciences},
	author = {Guralnick, Robert and LaFrance, Raphael and Denslow, Michael and Blickhan, Samantha and Bouslog, Mark and Miller, Sean and Yost, Jenn and Best, Jason and Paul, Deborah L. and Ellwood, Elizabeth and Gilbert, Edward and Allen, Julie},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aps3.11560},
	keywords = {OCR, citizen science, digitization, humans in the loop, machine learning, natural history collections, Notes from Nature, object classification, object detection},
	pages = {e11560},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\25P9R3AH\\Guralnick et al. - 2024 - Humans in the loop Community science and machine learning synergies for overcoming herbarium digiti.pdf:application/pdf},
}

@inproceedings{ngo_semantic_2021,
	address = {Cham},
	title = {A {Semantic} {Search} {Engine} for {Historical} {Handwritten} {Document} {Images}},
	isbn = {978-3-030-86324-1},
	doi = {10.1007/978-3-030-86324-1_7},
	abstract = {A very large number of historical manuscript collections are available in image formats and require extensive manual processing in order to search through them. So, we propose and build a search engine for automatically storing, indexing and efficiently searching the manuscript images. Firstly, a handwritten text recognition technique is used to convert the images into textual representations. In the next steps, we apply the named entity recognition and historical knowledge graph to build a semantic search model, which can understand the user’s intent in the query and the contextual meaning of concepts in documents, to return correctly the transcriptions and their corresponding images for users.},
	language = {en},
	booktitle = {Linking {Theory} and {Practice} of {Digital} {Libraries}},
	publisher = {Springer International Publishing},
	author = {Ngo, Vuong M. and Munnelly, Gary and Orlandi, Fabrizio and Crooks, Peter and O’Sullivan, Declan and Conlan, Owen},
	editor = {Berget, Gerd and Hall, Mark Michael and Brenn, Daniel and Kumpulainen, Sanna},
	year = {2021},
	keywords = {Handwriting transcription, Knowledge graph, Named entity},
	pages = {60--65},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\Y7S47XNE\\Ngo et al. - 2021 - A Semantic Search Engine for Historical Handwritten Document Images.pdf:application/pdf},
}

@article{koolen_value_2023,
	title = {The {Value} of {Preexisting} {Structures} for {Digital} {Access}: {Modelling} the {Resolutions} of the {Dutch} {States} {General}},
	volume = {16},
	issn = {1556-4673},
	shorttitle = {The {Value} of {Preexisting} {Structures} for {Digital} {Access}},
	doi = {10.1145/3575864},
	abstract = {The Resolutions of the Dutch States General (1576–1796) is an archive covering over two centuries of decision making and consists of a heterogeneous series of handwritten and printed documents. The archive, which has recently been digitised, is a rich source for historical research. However, owing to the archive’s heterogeneity and dispersion of information, historians and other researchers find it hard to use the archive for their research. In this article, we describe how we deal with the challenges of structuring and connecting the information in this archive. We focus on identifying the existing structural elements, to turn the archive from a set of pages into a set of meeting dates and individual resolutions, with rich metadata for each resolution. To deal with the challenges of historical language change, spelling variation, and text recognition mistakes, we exploit the repetitive nature of the language of the resolutions and use fuzzy string searching to identify structural elements by the formulaic expressions that signal their boundaries. We also discuss and provide an analysis of the value of extracting different types of entities from the text and argue that the choice of which types of entities to focus on should be made based on how they support relevant research questions and methods. In the resolutions, we choose to prioritise person qualifications such as profession, legal status, or title, over person names. Qualifications allow users to select certain groups of people and to meaningfully combine with other layers of metadata, whereas person names lack contextual information to disambiguate them, making it unclear which and how many persons are referred to by selecting a specific person name. We show how our methodology results in a computational platform that allows users to explore and analyse the archive through many connected layers of metadata.},
	number = {1},
	urldate = {2025-05-01},
	journal = {J. Comput. Cult. Herit.},
	author = {Koolen, Marijn and Hoekstra, Rik and Oddens, Joris and Sluijter, Ronald and Van Koert, Rutger and Brouwer, Gijsjan and Brugman, Hennie},
	month = jun,
	year = {2023},
	pages = {1:1--1:24},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\59EQ7EM9\\Koolen et al. - 2023 - The Value of Preexisting Structures for Digital Access Modelling the Resolutions of the Dutch State.pdf:application/pdf},
}

@article{nockels_implications_2024,
	title = {The implications of handwritten text recognition for accessing the past at scale},
	volume = {80},
	issn = {0022-0418},
	doi = {10.1108/JD-09-2023-0183},
	abstract = {This paper focuses on image-to-text manuscript processing through Handwritten Text Recognition (HTR), a Machine Learning (ML) approach enabled by Artificial Intelligence (AI). With HTR now achieving high levels of accuracy, we consider its potential impact on our near-future information environment and knowledge of the past.,In undertaking a more constructivist analysis, we identified gaps in the current literature through a Grounded Theory Method (GTM). This guided an iterative process of concept mapping through writing sprints in workshop settings. We identified, explored and confirmed themes through group discussion and a further interrogation of relevant literature, until reaching saturation.,Catalogued as part of our GTM, 120 published texts underpin this paper. We found that HTR facilitates accurate transcription and dataset cleaning, while facilitating access to a variety of historical material. HTR contributes to a virtuous cycle of dataset production and can inform the development of online cataloguing. However, current limitations include dependency on digitisation pipelines, potential archival history omission and entrenchment of bias. We also cite near-future HTR considerations. These include encouraging open access, integrating advanced AI processes and metadata extraction; legal and moral issues surrounding copyright and data ethics; crediting individuals’ transcription contributions and HTR’s environmental costs.,Our research produces a set of best practice recommendations for researchers, data providers and memory institutions, surrounding HTR use. This forms an initial, though not comprehensive, blueprint for directing future HTR research. In pursuing this, the narrative that HTR’s speed and efficiency will simply transform scholarship in archives is deconstructed.},
	language = {en},
	number = {7},
	urldate = {2025-05-01},
	journal = {Journal of Documentation},
	author = {Nockels, Joseph and Gooding, Paul and Terras, Melissa},
	month = apr,
	year = {2024},
	note = {Publisher: Emerald Publishing Limited},
	pages = {148--167},
	file = {Full Text:C\:\\Users\\PeetersSA\\Zotero\\storage\\IRA4DXVP\\Nockels et al. - 2024 - The implications of handwritten text recognition for accessing the past at scale.pdf:application/pdf;Snapshot:C\:\\Users\\PeetersSA\\Zotero\\storage\\H9BLAXVD\\html.html:text/html},
}

@misc{otten_gids_2004,
	title = {Gids voor de archieven van de ministeries en de {Hoge} {Colleges} van {Staat}, 1813-1940},
	author = {Otten, F.J.M.},
    year = {2004},
	url = {https://resources.huygens.knaw.nl/retroboeken/archiefgids_overheid/#page=0&accessor=toc_1&view=homePane},
	urldate = {2025-05-02},
	file = {Gids voor de archieven van de ministeries en de Hoge Colleges van Staat, 1813-1940:C\:\\Users\\PeetersSA\\Zotero\\storage\\HEQRWTK6\\archiefgids_overheid.html:text/html},
}

@article{romein_assessing_2025,
	title = {Assessing advanced handwritten text recognition engines for digitizing historical documents},
	issn = {2524-7840},
	doi = {10.1007/s42803-025-00100-0},
	abstract = {This study provides critical insights and evaluates the performance of state-of-the-art Handwritten Text Recognition (HTR) engines—PyLaia, HTR + , IDA, TrOCR-f, and Transkribus’ proprietary Transformer-based “supermodel” Titan—to digitize historical documents. Using a diverse range of datasets that include different scripts, this research assesses each engine's accuracy and efficiency in handling multilingual content, complex styles, abbreviations, and historical orthography. Results indicate that, while all engines can be trained or fine-tuned to improve performance, Titan and TrOCR-f exhibit superior out-of-the-box capabilities for Latin-script documents. PyLaia, IDA, and HTR + excel in specific non-Latin scripts when specifically trained or fine-tuned. This study underscores the importance of training, fine-tuning, and integrating language models, providing critical insights for future advancements in HTR technology and its application in the digital humanities.},
	language = {en},
	urldate = {2025-05-13},
	journal = {International Journal of Digital Humanities},
	author = {Romein, C. A. and Rabus, A. and Leifert, G. and Ströbel, P. B.},
	month = may,
	year = {2025},
	keywords = {Digital humanities, Handwritten text recognition (HTR), Historical document digitization, IDA, Language models, Multilingual content, Planet AI, PyLaia, Transkribus, TrOCR},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\UCJSH553\\Romein et al. - 2025 - Assessing advanced handwritten text recognition engines for digitizing historical documents.pdf:application/pdf},
}

@article{hawkins_archives_2022,
	title = {Archives, linked data and the digital humanities: increasing access to digitised and born-digital archives via the semantic web},
	volume = {22},
	issn = {1573-7500},
	shorttitle = {Archives, linked data and the digital humanities},
	doi = {10.1007/s10502-021-09381-0},
	abstract = {Mass digitisation and the exponential growth of born-digital archives over the past two decades have resulted in an enormous volume of archives and archival data being available digitally. This has produced a valuable but under-utilised source of large-scale digital data ripe for interrogation by scholars and practitioners in the Digital Humanities. However, current digitisation approaches fall short of the requirements of digital humanists for structured, integrated, interoperable, and interrogable data. Linked Data provides a viable means of producing such data, creating machine-readable archival data suited to analysis using digital humanities research methods. While a growing body of archival scholarship and praxis has explored Linked Data, its potential to open up digitised and born-digital archives to the Digital Humanities is under-examined. This article approaches Archival Linked Data from the perspective of the Digital Humanities, extrapolating from both archival and digital humanities Linked Data scholarship to identify the benefits to digital humanists of the production and provision of access to Archival Linked Data. It will consider some of the current barriers preventing digital humanists from being able to experience the benefits of Archival Linked Data evidenced, and to fully utilise archives which have been made available digitally. The article argues for increased collaboration between the two disciplines, challenges individuals and institutions to engage with Linked Data, and suggests the incorporation of AI and low-barrier tools such as Wikidata into the Linked Data production workflow in order to scale up the production of Archival Linked Data as a means of increasing access to and utilisation of digitised and born-digital archives.},
	language = {en},
	number = {3},
	urldate = {2025-05-21},
	journal = {Archival Science},
	author = {Hawkins, Ashleigh},
	month = sep,
	year = {2022},
	keywords = {Artificial Intelligence, Cultural heritage, Digital humanities, Archaeology and Heritage, Data publication and archiving, Digital and New Media, Digital archives, Digital Humanities, Digital Journalism, Genealogy, Linked data},
	pages = {319--344},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\QQ3PDMME\\Hawkins - 2022 - Archives, linked data and the digital humanities increasing access to digitised and born-digital ar.pdf:application/pdf},
}

@article{candela_ontological_2023,
	title = {An {Ontological} {Approach} for {Unlocking} the {Colonial} {Archive}},
	volume = {16},
	issn = {1556-4673},
	doi = {10.1145/3594727},
	abstract = {Cultural Heritage institutions have been exploring new ways of making available their catalogues in digital format. Recently, new approaches have emerged as methods to reuse and make available the contents for computational purposes. This work introduces a methodology to transform digital collections into Linked Open Data following best practices. The framework has been applied to Indigenous and Spanish colonial archives based on the collection Relaciones Geográficas of Mexico and Guatemala provided by the LLILAS Benson Latin American Studies and Collections. The results of this work are publicly available. This work aims at encouraging Cultural Heritage institutions to publish and reuse their digital collections using advanced methods and techniques.},
	number = {4},
	urldate = {2025-05-22},
	journal = {J. Comput. Cult. Herit.},
	author = {Candela, Gustavo and Pereda, Javier and Sáez, Dolores and Escobar, Pilar and Sánchez, Alexander and Torres, Andrés Villa and Palacios, Albert A. and McDonough, Kelly and Murrieta-Flores, Patricia},
	month = nov,
	year = {2023},
	pages = {74:1--74:18},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\M4S7M2BG\\Candela et al. - 2023 - An Ontological Approach for Unlocking the Colonial Archive.pdf:application/pdf},
}

@article{ketelaar_veranderingen_2020,
	title = {Veranderingen in archiefvorming en archiefgebruik in een veranderende samenleving 1747-18471},
	volume = {133},
	issn = {0040-7518, 2352-1163},
	doi = {10.5117/TVGESCH2020.3.002.KETE},
	abstract = {Abstract Changing Society, Changing Archives 1747-1847 Developments in Dutch society and government in the first half of the nineteenth century led to changes in archival practices. The newly-established unitary state needed more and different information about society and people, and that information had to be managed and archived. However, after the Restoration in 1813 many traditional archiving practices were reintroduced. Aside from the function of government, collecting and publishing archival documents was not only (as in the eighteenth century) deemed necessary to bolster legal legitimacy, but was seen as a prerequisite for writing local, provincial, and national histories. A small number of cities and provincial governments appointed an archivist to serve the community by collecting and publishing archival documents.},
	language = {en},
	number = {3},
	urldate = {2025-05-28},
	journal = {Tijdschrift voor Geschiedenis},
	author = {Ketelaar, Eric},
	month = nov,
	year = {2020},
	note = {Publisher: Amsterdam University Press},
	pages = {435--456},
	file = {Snapshot:C\:\\Users\\PeetersSA\\Zotero\\storage\\CKLG6ZQY\\TVGESCH2020.3.002.html:text/html},
}

@article{linhares_pontes_melhissa_2022,
	title = {{MELHISSA}: a multilingual entity linking architecture for historical press articles},
	volume = {23},
	issn = {1432-1300},
	shorttitle = {{MELHISSA}},
	doi = {10.1007/s00799-021-00319-6},
	abstract = {Digital libraries have a key role in cultural heritage as they provide access to our culture and history by indexing books and historical documents (newspapers and letters). Digital libraries use natural language processing (NLP) tools to process these documents and enrich them with meta-information, such as named entities. Despite recent advances in these NLP models, most of them are built for specific languages and contemporary documents that are not optimized for handling historical material that may for instance contain language variations and optical character recognition (OCR) errors. In this work, we focused on the entity linking (EL) task that is fundamental to the indexation of documents in digital libraries. We developed a Multilingual Entity Linking architecture for HIstorical preSS Articles that is composed of multilingual analysis, OCR correction, and filter analysis to alleviate the impact of historical documents in the EL task. The source code is publicly available. Experimentation has been done over two historical document corpora covering five European languages (English, Finnish, French, German, and Swedish). Results have shown that our system improved the global performance for all languages and datasets by achieving an F-score@1 of up to 0.681 and an F-score@5 of up to 0.787.},
	language = {en},
	number = {2},
	urldate = {2025-06-10},
	journal = {International Journal on Digital Libraries},
	author = {Linhares Pontes, Elvys and Cabrera-Diego, Luis Adrián and Moreno, Jose G. and Boros, Emanuela and Hamdi, Ahmed and Doucet, Antoine and Sidere, Nicolas and Coustaty, Mickaël},
	month = jun,
	year = {2022},
	keywords = {Digital Humanities, Deep learning, Cultural Heritage, Digital libraries, Entity linking, ESCRT, Heritage Management, Heuristics, Historical data, Machine Translation, Multilingualism},
	pages = {133--160},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\XL4VRJU5\\Linhares Pontes et al. - 2022 - MELHISSA a multilingual entity linking architecture for historical press articles.pdf:application/pdf},
}

@phdthesis{nockels2025making,
  author       = {Nockels, Joseph},
  title        = {Making the past readable: a study of the impact of handwritten text recognition (HTR) on libraries and their users},
  school       = {The University of Edinburgh},
  year         = {2025},
  type         = {PhD Doctor of Philosophy},
  address      = {Edinburgh, UK},
  advisor      = {Terras, Melissa and Gooding, Paul and Ames, Sarah},
  doi          = {10.7488/era/5988},
  abstract     = {This thesis considers the socio-technical infrastructure surrounding AI-enabled Handwritten Text Recognition (HTR), the process of converting images of historical textual materials into computer-readable text. Despite sectoral awareness of the technology, institutional approaches still display gaps between conceptualising and operationalising HTR. The following main research question is answered: What is the impact of HTR on libraries, users, and the wider information environment? Foundational knowledge is provided by exploring HTR's relation to other computational methods and traditional areas of palaeography and papyrology. Through a mixed methods approach, including interviews with National Library of Scotland (NLS) staff, survey methods and thematic analysis, this thesis approaches HTR from multiple vantage points. It critically reflects on HTR’s affordances regarding library audience development by adapting digital engagement strategies, as well as the practicalities in embedding the technology within content-holding institutions. In aligning insights from HTR developers, providers and users, a technical fluency of how to operationalise HTR institutionally is presented. This informs an understanding of HTR’s potential near future implications on the information environment and research. Such analysis results in a set of recommendations for HTR’s future provision, directed thematically at libraries, HTR users and developers. These recommendations include ways to enhance institutional processes, such as digital preservation and audience engagement, in relation to HTR outputs; as well as assessing general HTR capability locally. Other recommendations involve enabling greater collaboration in HTR projects: through dataset sharing principles, standardised metadata and flexible tool usage.},
  keywords     = {Handwritten Text Recognition, Optical Character Recognition, Digital Libraries, Collections as Data},
  language     = {en},
  sponsor      = {Arts and Humanities Research Council (AHRC)}
}

@inproceedings{van_koert_loghi_2024,
	address = {Cham},
	title = {Loghi: {An} {End}-to-{End} {Framework} for {Making} {Historical} {Documents} {Machine}-{Readable}},
	isbn = {978-3-031-70645-5},
	shorttitle = {Loghi},
	doi = {10.1007/978-3-031-70645-5_6},
	abstract = {Loghi is a novel framework and suite of tools for the layout analysis and text recognition of historical documents. Scans are processed in a modular pipeline, with the option to use alternative tools in most stages. Layout analysis and text recognition can be trained on example images with PageXML ground truth. The framework is intended to convert scanned documents to machine-readable PageXML. Additional tooling is provided for the creation of synthetic ground truth. A visualiser for troubleshooting the text recognition training is also made available. The result is a framework for end-to-end text recognition, which works from initial layout analysis on the scanned documents, and includes text line detection, text recognition, reading order detection and language detection.},
	language = {en},
	booktitle = {Document {Analysis} and {Recognition} – {ICDAR} 2024 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {van Koert, Rutger and Klut, Stefan and Koornstra, Tim and Maas, Martijn and Peters, Luke},
	editor = {Mouchère, Harold and Zhu, Anna},
	year = {2024},
	keywords = {Handwritten Text Recognition, Layout Analysis, PageXML},
	pages = {73--88},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\ATJKYZ4Y\\van Koert et al. - 2024 - Loghi An End-to-End Framework for Making Historical Documents Machine-Readable.pdf:application/pdf},
}

@misc{jeurgens_schetsboek_2016,
	title = {Schetsboek {\textbar} 1 januari 2016 {\textbar} pagina 55},
	url = {https://kvan.courant.nu/issue/SB/2016-01-01/edition/null/page/55},
	author = {Jeurgens, Charles},
	language = {nl},
	urldate = {2025-07-01},
	journal = {Periodiekviewer Koninklijke Vereniging van Archivarissen},
	month = jan,
	year = {2016},
	file = {Snapshot:C\:\\Users\\PeetersSA\\Zotero\\storage\\YLVZFABI\\55.html:text/html},
}

@article{vriend_archive_2025,
	title = {An archive in numbers: the pulse of the {Dutch} {Ministry} of {Colonies}, 1813–1900},
	volume = {25},
	issn = {1573-7500},
	shorttitle = {An archive in numbers},
	doi = {10.1007/s10502-025-09483-z},
	abstract = {In this article, a bird’s-eye view on archives is advocated as a third perspective, alongside the approaches of reading against and along the archival grain. Here, the archival grain—metaphorically speaking—is analyzed in numbers. A bird’s-eye perspective helps us to reconstruct an archive’s pieces and production, trace deviating numbers in document flows, and leads us to a further understanding of archives. In a case study, the archive of the Dutch Ministry of Colonies is at the center of investigation. The study provides insights into the relationships between different parts of the archive and the overall document flow of the organization. It examines the origins of the numerous written pages within this archive and the pulse of their day-to-day production. Peaks and troughs in document production appear on a yearly, monthly or even daily basis. The analysis reveals a significant paperwork explosion occurring roughly between 1851 and 1883, alongside deviating numbers in document flows, such as a dramatic drop in the total number of pages in 1869. Why did these events occur? On a more detailed level, peaks in the daily production of specific files are traced. While some of these thicker files might be labeled as ‘archival events,’ most result from the administrative deposits of large case files.},
	language = {en},
	number = {2},
	urldate = {2025-07-01},
	journal = {Archival Science},
	author = {Vriend, Nico},
	month = jun,
	year = {2025},
	keywords = {Archival theory, Data publication and archiving, Digital Humanities, Genealogy, Archival events, Archive destruction, Archives in bird’s-eye view, Chronology, Colonial archives, Oral History, Palaeography, Paperwork explosions},
	pages = {17},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\6JW8UKIK\\Vriend - 2025 - An archive in numbers the pulse of the Dutch Ministry of Colonies, 1813–1900.pdf:application/pdf},
}

@misc{romein_entangled_2020,
	title = {Entangled {Histories} {Ordinances} {Low} {Countries}},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	doi = {10.5281/ZENODO.3567844},
	abstract = {https://lab.kb.nl/dataset/entangled-histories-ordinances-low-countries},
	language = {en},
	urldate = {2025-07-01},
	publisher = {Zenodo},
	author = {Romein, Christel Annemieke and Veldhoen, Sara and de Gruijter, Michel},
	month = jan,
	year = {2020},
	keywords = {Transkribus, Digital Humanities, Computational linguistics, digital scholarship, Entangled Histories, KB Lab, Police Ordinances},
	annote = {Other
\{"references": ["https://lab.kb.nl/dataset/entangled-histories-ordinances-low-countries"]\}},
	file = {PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\P684DCQS\\Romein et al. - 2020 - Entangled Histories Ordinances Low Countries.pdf:application/pdf},
}

@inproceedings{manjavacas_arevalo_non-parametric_2022,
	address = {Taipei, Taiwan},
	title = {Non-{Parametric} {Word} {Sense} {Disambiguation} for {Historical} {Languages}},
	doi = {10.18653/v1/2022.nlp4dh-1.16},
	abstract = {Recent approaches to Word Sense Disambiguation (WSD) have profited from the enhanced contextualized word representations coming from contemporary Large Language Models (LLMs). This advancement is accompanied by a renewed interest in WSD applications in Humanities research, where the lack of suitable, specific WSD-annotated resources is a hurdle in developing ad-hoc WSD systems. Because they can exploit sentential context, LLMs are particularly suited for disambiguation tasks. Still, the application of LLMs is often limited to linear classifiers trained on top of the LLM architecture. In this paper, we follow recent developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to achieve strong few-shot performance on WSD for historical languages (English and Dutch, date range: 1450-1950). We test our hypothesis using (i) a large, general evaluation set taken from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch.},
	urldate = {2025-07-09},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Natural} {Language} {Processing} for {Digital} {Humanities}},
	publisher = {Association for Computational Linguistics},
	author = {Manjavacas Arevalo, Enrique and Fonteyn, Lauren},
	editor = {Hämäläinen, Mika and Alnajjar, Khalid and Partanen, Niko and Rueter, Jack},
	month = nov,
	year = {2022},
	pages = {123--134},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\RAIEBLST\\Manjavacas Arevalo and Fonteyn - 2022 - Non-Parametric Word Sense Disambiguation for Historical Languages.pdf:application/pdf},
}

@incollection{van_oort_citizen_2025,
	title = {Citizen {Science} and {Participatory} {Engagement} with a {Contentious} {Past}},
	isbn = {978-3-030-61493-5},
	abstract = {Citizen science is an increasingly widely used methodology for performing scientific research, in which scholars collaborate with members of the public that do not act out of a professional capacity. In the humanities, citizen science is often employed for labor...},
	language = {en},
	urldate = {2025-07-15},
	booktitle = {The {Palgrave} {Encyclopedia} of {Cultural} {Heritage} and {Conflict}},
	publisher = {Palgrave Macmillan, Cham},
	author = {van Oort, Thunnis and Prats López, Montserrat and Ganzevoort, Wessel and van Galen, Coen},
	year = {2025},
	doi = {10.1007/978-3-030-61493-5_319-1},
	pages = {1--7},
}

@article{colavizza_archives_2021,
	title = {Archives and {AI}: {An} {Overview} of {Current} {Debates} and {Future} {Perspectives}},
	volume = {15},
	issn = {1556-4673},
	shorttitle = {Archives and {AI}},
	doi = {10.1145/3479010},
	abstract = {The digital transformation is turning archives, both old and new, into data. As a consequence, automation in the form of artificial intelligence techniques is increasingly applied both to scale traditional recordkeeping activities, and to experiment with novel ways to capture, organise, and access records. We survey recent developments at the intersection of Artificial Intelligence and archival thinking and practice. Our overview of this growing body of literature is organised through the lenses of the Records Continuum model. We find four broad themes in the literature on archives and artificial intelligence: theoretical and professional considerations, the automation of recordkeeping processes, organising and accessing archives, and novel forms of digital archives. We conclude by underlining emerging trends and directions for future work, which include the application of recordkeeping principles to the very data and processes that power modern artificial intelligence and a more structural—yet critically aware—integration of artificial intelligence into archival systems and practice.},
	number = {1},
	urldate = {2025-07-15},
	journal = {J. Comput. Cult. Herit.},
	author = {Colavizza, Giovanni and Blanke, Tobias and Jeurgens, Charles and Noordegraaf, Julia},
	month = dec,
	year = {2021},
	pages = {4:1--4:15},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\IHEABEC5\\Colavizza et al. - 2021 - Archives and AI An Overview of Current Debates and Future Perspectives.pdf:application/pdf},
}

@book{stoler_along_2009,
	address = {Princeton, NJ},
	title = {Along the archival grain: epistemic anxieties and colonial common sense},
	isbn = {978-0-691-14636-2},
	shorttitle = {Along the archival grain},
	language = {eng},
	publisher = {Princeton Univ. Press},
	author = {Stoler, Ann Laura},
	year = {2009},
	annote = {Includes bibliographical references and index},
}

@inproceedings{klut_laypa_2023,
	address = {New York, NY, USA},
	series = {{HIP} '23},
	title = {Laypa: {A} {Novel} {Framework} for {Applying} {Segmentation} {Networks} to {Historical} {Documents}},
	isbn = {979-8-4007-0841-1},
	shorttitle = {Laypa},
	url = {https://dl.acm.org/doi/10.1145/3604951.3605520},
	doi = {10.1145/3604951.3605520},
	abstract = {We present novel software to process scans of historical documents to extract their layout information. We do this using a ResNet backbone with a feature pyramid head. We extract region information directly into PageXML. For baseline extraction, we use a two stage processing approach. The software has been applied successfully to several projects. The results show the feasibility to automatically label text lines and regions in historical documents.},
	urldate = {2025-07-15},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Historical} {Document} {Imaging} and {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Klut, Stefan and van Koert, Rutger and Sluijter, Ronald},
	month = aug,
	year = {2023},
	pages = {67--72},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\4LLT2S47\\Klut et al. - 2023 - Laypa A Novel Framework for Applying Segmentation Networks to Historical Documents.pdf:application/pdf},
}

@misc{luthra_unsilencing_2022,
	title = {Unsilencing {Colonial} {Archives} via {Automated} {Entity} {Recognition}},
	doi = {10.48550/arXiv.2210.02194},
	abstract = {Colonial archives are at the center of increased interest from a variety of perspectives, as they contain traces of historically marginalized people. Unfortunately, like most archives, they remain difficult to access due to significant persisting barriers. We focus here on one of them: the biases to be found in historical findings aids, such as indexes of person names, which remain in use to this day. In colonial archives, indexes can perpetuate silences by omitting to include mentions of historically marginalized persons. In order to overcome such limitations and pluralize the scope of existing finding aids, we propose using automated entity recognition. To this end, we contribute a fit-for-purpose annotation typology and apply it on the colonial archive of the Dutch East India Company (VOC). We release a corpus of nearly 70,000 annotations as a shared task, for which we provide baselines using state-of-the-art neural network models. Our work intends to stimulate further contributions in the direction of broadening access to (colonial) archives, integrating automation as a possible means to this end.},
	urldate = {2025-07-15},
	publisher = {arXiv},
	author = {Luthra, Mrinalini and Todorov, Konstantin and Jeurgens, Charles and Colavizza, Giovanni},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02194 [cs]},
	keywords = {Computer Science - Digital Libraries},
	file = {Preprint PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\H7JW4RMZ\\Luthra et al. - 2022 - Unsilencing Colonial Archives via Automated Entity Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\PeetersSA\\Zotero\\storage\\7S9CT2R9\\2210.html:text/html},
}

@article{colavizza_index-driven_2019,
	title = {Index-{Driven} {Digitization} and {Indexation} of {Historical} {Archives}},
	volume = {6},
	issn = {2297-2668},
	doi = {10.3389/fdigh.2019.00004},
	abstract = {The promise of digitization of historical archives lies in their indexation at the level of contents. Unfortunately, this kind of indexation does not scale, if done manually. In this article we present a method to bootstrap the deployment of a content-based information system for digitized historical archives, relying on historical indexing tools. Commonly prepared to search within homogeneous records when the archive was still current, such indexes were as widespread as they were disconnected, that is to say situated in the very records they were meant to index. We first present a conceptual model to describe and manipulate historical indexing tools. We then introduce a methodological framework for their use in order to guide digitization campaigns and index digitized historical records. Finally, we exemplify the approach with a case study on the indexation system of the X Savi alle Decime in Rialto, a Venetian magistracy in charge for the exaction—and related record keeping—of a tax on real estate in early modern Venice.},
	language = {English},
	urldate = {2025-08-26},
	journal = {Frontiers in Digital Humanities},
	author = {Colavizza, Giovanni and Ehrmann, Maud and Bortoluzzi, Fabio},
	month = mar,
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {digitization, Archives, digital archives, Indexation, information retrieval, Republic of Venice, Venice},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\DWTDVW45\\Colavizza et al. - 2019 - Index-Driven Digitization and Indexation of Historical Archives.pdf:application/pdf},
}

@misc{van_schuijlenburg_haicu_2025,
	title = {The {HAICu} {Project} ({WP2})},
	url = {https://zenodo.org/records/15829129},
	urldate = {2025-10-28},
	author = {van Schuijlenburg, Koen and Romein, C. A. (Annemieke) and Wolf, Ben and Weggeman, Sjors and Peeters, Sebastiaan and Dhali, Maruf A. and Dijkstra, Klaas and Weber, Andreas and Schomaker, Lambert},
	month = jul,
	year = {2025},
	doi = {10.5281/zenodo.15829129},
	file = {Full Text PDF:C\:\\Users\\PeetersSA\\Zotero\\storage\\T59ZI5HG\\van Schuijlenburg et al. - 2025 - The HAICu Project (WP2).pdf:application/pdf},
}
