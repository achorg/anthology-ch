@book{boone2000stories,
publisher = {University of Texas Press},
isbn = {0292708769},
year = {2000},
title = {Stories in red and black : pictorial histories of the Aztecs and Mixtecs / Elizabeth Hill Boone.},
edition = {1st ed.},
language = {eng},
address = {Austin, Texas, USA},
author = {Boone, Elizabeth Hill},
keywords = {Manuscripts Nahuatl; Aztec painting; Nahuatl language -- Writing; Manuscripts Mixtec; Mixtec art; Mixtec language -- Writing},
lccn = {99006214},
}

@book{williams2013complete,
series = {Linda Schele series in Maya and pre-Columbian studies},
abstract = {The pre-Hispanic Mixtec people of Mexico recorded political and religious history, including the biographies and genealogies of their rulers, in pictograms on hand-painted, screen-fold manuscripts known as codices. Functioning rather like movie production storyboards, the codices served as outlines of oral traditions to stimulate the memories of bards who knew the complete narratives, which were sung, danced, and performed at elite functions. Centuries later we have limited access to those original performances, and all that remains for our codex interpretation is what is painted on the pages - perhaps 5 to 10 percent of their memory-encoded information. Continuing the pioneering interpretation he began in Lord Eight Wind of Suchixtlan and the Heroes of Ancient Oaxaca, Robert Lloyd Williams offers an authoritative guide to the entire contents of the codex in The Complete Codex Zouche-Nuttall. Although the reverse document (pages 42-84) has been described in previous literature, the obverse document (pages 1-41) has not been, and it has remained elusive as to narrative. The Complete Codex Zouche-Nuttall elucidates the three sections of the codex, defines them as to function and content, and provides interpretive and descriptive essays about the Native American history the codex recorded prior to the arrival of Europeans in Mexico and the New World generally. With a full-colour reproduction of the entire Codex Zouche-Nuttall and Williams's expert guidance in unlocking its narrative strategies and structures, The Complete Codex Zouche-Nuttall opens an essential window into the Mixtec social and political cosmos.},
publisher = {University of Texas Press},
isbn = {9780292744387},
year = {2013},
title = {The complete codex Zouche-Nuttall : Mixtec lineage histories and political biographies / by Robert Lloyd Williams ; foreword by Rex Koontz.},
edition = {1st ed.},
language = {eng},
address = {Austin, Texas, USA},
author = {Williams, Robert Lloyd},
keywords = {Manuscripts Mixtec; Mixtec Indians -- Kings and rulers; Mixtec Indians -- Kinship; Mixtec Indians -- Politics and government; Codex Nuttall},
lccn = {2012042789},
}

@book{jansen1988mesoamerican,
  author = {Maarten Jansen},
  title = {The Mesoamerican Codices},
  year = {1988},
  publisher = {British Museum Publications},
  address = {British Museum, Great Russell Street, London WC1B 3DG, United Kingdom},
}

@book{smith1973picturewriting,
  author = {Mary Elizabeth Smith},
  title = {Picture Writing from Ancient Southern Mexico: Mixtec Place Signs and Maps},
  publisher = {University of Oklahoma Press},
  address = {2800 Venture Drive Norman, Oklahoma, USA 73069},
  year = {1973},
  series = {Civilization of the American Indian Series}
}

@inproceedings{webber-etal-2024-analyzing,
    title = "Analyzing Finetuned Vision Models for {M}ixtec Codex Interpretation",
    author = "Webber, Alexander and
      Sayers, Zachary and
      Wu, Amy and
      Thorner, Elizabeth and
      Witter, Justin and
      Ayoubi, Gabriel and
      Grant, Christan",
    editor = "Mager, Manuel and
      Ebrahimi, Abteen and
      Rijhwani, Shruti and
      Oncevay, Arturo and
      Chiruzzo, Luis and
      Pugh, Robert and
      von der Wense, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.americasnlp-1.6",
    pages = "42--49",
    abstract = "Throughout history, pictorial record-keeping has been used to document events, stories, and concepts. A popular example of this is the Tzolk`in Maya Calendar. The pre-Columbian Mixtec society also recorded many works through graphical media called codices that depict both stories and real events. Mixtec codices are unique because the depicted scenes are highly structured within and across documents. As a first effort toward translation, we created two binary classification tasks over Mixtec codices, namely, gender and pose. The composition of figures within a codex is essential for understanding the codex`s narrative. We labeled a dataset with around 1300 figures drawn from three codices of varying qualities. We finetuned the Visual Geometry Group 16 (VGG-16) and Vision Transformer 16 (ViT-16) models, measured their performance, and compared learned features with expert opinions found in literature. The results show that when finetuned, both VGG and ViT perform well, with the transformer-based architecture (ViT) outperforming the CNN-based architecture (VGG) at higher learning rates. We are releasing this work to allow collaboration with the Mixtec community and domain scientists."
}

@ARTICLE{egptHeiroDL,
  author={Barucci, Andrea and Cucci, Costanza and Franci, Massimiliano and Loschiavo, Marco and Argenti, Fabrizio},
  journal={IEEE Access},
  title={A Deep Learning Approach to Ancient Egyptian Hieroglyphs Classification},
  year={2021},
  volume={9},
  pages={123438-123447},
  keywords={Deep learning;Task analysis;Convolutional neural networks;Artificial intelligence;Training;Phonetics;Image recognition;Deep learning;convolutional neural networks;image recognition and classification;ancient Egyptian hieroglyphs;cultural heritage},
  doi={10.1109/ACCESS.2021.3110082}}

@article{Can2018HowTT,
author = {Can, G\"{u}lcan and Odobez, Jean-Marc and Gatica-Perez, Daniel},
title = {How to Tell Ancient Signs Apart? Recognizing and Visualizing Maya Glyphs with CNNs},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4673},
doi = {10.1145/3230670},
abstract = {Thanks to the digital preservation of cultural heritage materials, multimedia tools (e.g., based on automatic visual processing) considerably ease the work of scholars in the humanities and help them to perform quantitative analysis of their data. In this context, this article assesses three different Convolutional Neural Network (CNN) architectures along with three learning approaches to train them for hieroglyph classification, which is a very challenging task due to the limited availability of segmented ancient Maya glyphs. More precisely, the first approach, the baseline, relies on pretrained networks as feature extractor. The second one investigates a transfer learning method by fine-tuning a pretrained network for our glyph classification task. The third approach considers directly training networks from scratch with our glyph data. The merits of three different network architectures are compared: a generic sequential model (i.e., LeNet), a sketch-specific sequential network (i.e., Sketch-a-Net), and the recent Residual Networks. The sketch-specific model trained from scratch outperforms other models and training strategies. Even for a challenging 150-class classification task, this model achieves 70.3\% average accuracy and proves itself promising in case of a small amount of cultural heritage shape data. Furthermore, we visualize the discriminative parts of glyphs with the recent Grad-CAM method, and demonstrate that the discriminative parts learned by the model agree, in general, with the expert annotation of the glyph specificity (diagnostic features). Finally, as a step toward systematic evaluation of these visualizations, we conduct a perceptual crowdsourcing study. Specifically, we analyze the interpretability of the representations from Sketch-a-Net and ResNet-50. Overall, our article takes two important steps toward providing tools to scholars in the digital humanities: increased performance for automation and improved interpretability of algorithms.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {20},
pages = {1--25},
keywords = {Maya glyphs, convolutional neural networks, crowdsourcing, shape recognition, transfer learning}
}

@inproceedings{dosovitskiy2021an,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  booktitle={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{naseer2021intriguing,
  title={Intriguing properties of vision transformers},
  author={Naseer, Muzammal et al.},
  journal={arXiv preprint arXiv:2105.10497},
  year={2021}
}

@article{d2021convit,
   title={ConViT: improving vision transformers with soft convolutional inductive biases*},
   volume={2022},
   ISSN={1742-5468},
   DOI={10.1088/1742-5468/ac9830},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={d’Ascoli, Stéphane and Touvron, Hugo and Leavitt, Matthew L and Morcos, Ari S and Biroli, Giulio and Sagun, Levent},
   year={2022},
   month=nov, 
   pages={114005} 
}

@inproceedings{ren2015faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing et al.},
  address= {Montréal, Canada},
  booktitle={Advances in Neural Information Processing Systems},
  year={2015}
}

@inproceedings{redmon2016you,
  title={You Only Look Once: Unified, Real-Time Object Detection},
  author={Redmon, Joseph et al.},
  booktitle={CVPR},
  address = {Las Vegas, Nevada, USA},
  year={2016}
}

@article{ocrhindi2013,
author = {Yadav, Divakar and Sanchez-Cuadrado, Sonia and Morato, Jorge},
year = {2013},
month = {01},
pages = {117-140},
title = {Optical Character Recognition for Hindi Language Using a Neural-network Approach},
volume = {9},
number = {1},
journal = {Journal of Information Processing Systems},
doi = {10.3745/JIPS.2013.9.1.117}
}

@article{iglesia2021maya,
  author = {Martin de la Iglesia and Franziska Diehr and Uwe Sikora and Sven Gronemeyer and Maximilian Behnert-Brodhun and Christian Prager and Nikolai Grube},
  title = {The Code of Maya Kings and Queens: Encoding and Markup of Maya Hieroglyphic Writing},
  journal = {Journal of the Text Encoding Initiative},
  volume = {14},
  year = {2021}
}

@inproceedings{zora265462,
       publisher = {CEUR-WS},
           pages = {113--126},
            year = {2024},
           month = {12},
           title = {Bringing Rome to life: evaluating historical image generation},
          series = {CEUR Workshop Proceedings},
       booktitle = {Proceedings of the Computational Humanities Research Conference 2024},
          address = {Aarhus, Denmark},
          author = {Phillip Benjamin Str{\"o}bel and Zejie Guo and {\"U}lk{\"u} Karag{\"o}z and Eva Maria Willi and Felix K Maier},
        abstract = {This study evaluates the potential of AI image generation for visualising historical events, focusing on two ancient Roman scenarios: the Roman triumph and the Lupercalia festival. Using DALL-E 3, we generated 600 images based on 100 prompts derived from scientific texts. We then conducted a twopart evaluation: (1) A human evaluation by 21 history students, who compared image pairs and ratedindividual images on accuracy and prompt alignment, and (2) two automated analyses, one modelled after the human evaluation protocol and one using visual question-answering (VQA) techniques. Our results reveal both the promise and limitations of AI in historical visualisation. While DALL-E 3 produced many convincing images, there were notable discrepancies between human and automated assessments. We found that Large Language Models tend to rate images more favourably than humanevaluators.We contribute a novel dataset for historical image generation, initial human and automated evaluation protocols, and insights into the challenges of using AI for historical visualisation, which is incredibly important for historians to reconstruct past events. Our findings highlight the need for refined evaluation methods and underscore the complexity of assessing historical accuracy in AI-generated imagery. This study lays the groundwork for future research on improving AI models for historical visualisation and developing more robust evaluation frameworks.},
            issn = {1613-0073},
        language = {english},
        keywords = {Digital Humanities, image generation, human evaluation, automatic evaluation, history, image datase},
        doi = {10.5167/uzh-265462},
}

@inproceedings{muther2023querying,
    address = "Paris, France",
    author = "Ryan Muther and Mathew Barber and David Smith" ,
    title = "Querying the Past:
Automatic Source Attribution with Language Models" ,
    booktitle = "Proceedings of the Computational Humanities Research Conference 2023",
    month = 12,
    year = "2023",
    pages = "344--355"
}

@article{FIORUCCI2020102,
title = {Machine Learning for Cultural Heritage: A Survey},
journal = {Pattern Recognition Letters},
volume = {133},
pages = {102-108},
year = {2020},
issn = {0167-8655},
doi = {10.1016/j.patrec.2020.02.017},
author = {Marco Fiorucci and Marina Khoroshiltseva and Massimiliano Pontil and Arianna Traviglia and Alessio {Del Bue} and Stuart James},
keywords = {Artificial Intelligence, Machine Learning, Cultural Heritage, Digital Humanities},
abstract = {The application of Machine Learning (ML) to Cultural Heritage (CH) has evolved since basic statistical approaches such as Linear Regression to complex Deep Learning models. The question remains how much of this actively improves on the underlying algorithm versus using it within a ‘black box’ setting. We survey across ML and CH literature to identify the theoretical changes which contribute to the algorithm and in turn them suitable for CH applications. Alternatively, and most commonly, when there are no changes, we review the CH applications, features and pre/post-processing which make the algorithm suitable for its use. We analyse the dominant divides within ML, Supervised, Semi-supervised and Unsupervised, and reflect on a variety of algorithms that have been extensively used. From such an analysis, we give a critical look at the use of ML in CH and consider why CH has only limited adoption of ML.}
}
