\documentclass[final]{anthology-ch}

\usepackage{booktabs}
\usepackage{graphicx}

\usepackage{adjustbox}
\usepackage{tablefootnote}
\usepackage{amsmath}

\title{The Latin Language Evolved Over Time, Masked Models Disregard That}

\author[1]{Miriam Cuscito}[
orcid=0009-0003-9585-2803
]
\author[2]{Alfio Ferrara}[
orcid=0000-0002-4991-4984
]
\author[3]{Martin Ruskov}[
orcid=0000-0001-5337-0636
]

\affiliation{1}{Department of Languages, Literatures, Cultures and Mediations, University of Milan, Milan, Italy}
\affiliation{2}{Department of Informatics, University of Milan, Milan, Italy}
\affiliation{3}{Department of Languages and Modern Cultures, University of Genoa, Genoa, Italy}

\keywords{historical adequacy, algorithmic bias, masked language models, Latin language, model evaluation}

\pubyear{2025}
\pubvolume{3}
\pagestart{1321}
\pageend{1332}
\conferencename{Computational Humanities Research 2025}
\conferenceeditors{Taylor Arnold, Margherita Fantoli, and Ruben Ros}
\doi{10.63744/sLAHYnQdA8fu}
\paperorder{81}

\addbibresource{bibliography.bib}

\begin{document}

\maketitle

\begin{abstract}

Training of Latin language models is rarely done with consideration of important historical watersheds.

Here we demonstrate how this leads to a poor performance when specific socio-temporal contextualisation is sought, something common to humanities research. We perform an evaluation that compares the historical adequacy of Latin language models, i.e. their ability to generate tokens, representative for a historical period.

We adopt a previously established method and refine it to overcome limitations due to Latin being an under-resourced language and one with intense tradition of intertextuality. To do this we extract word lists and concordances from the LatinISE corpus and use them to compare seven masked language models trained for Latin. We further perform statistical analysis of the results in order to identify the best and worst performing models in each of the historical contexts of interest.

We show that BERT medieval multilingual best captures the Classical linguistic context. Four models are indistinguishably good in our evaluation of the the Neo-Latin linguistic context.

These findings have broad implications for wider historical language research and beyond. Among these, we emphasise the need to train historical language models with due attention on consistent historical periods and we discuss the possible usefulness of noisy predictions.
Historical research of language models provides a neat demonstration of how model biases could impact their performance in specific domains.
\end{abstract}

\section{Introduction}

The relevance of computational linguistics and language models to the digital humanities is continuously growing. BERT-like masked language models (MLMs) are a particularly important class of interest.
On one hand, these are the large language models where most relevant training on historical texts has been done~\cite{manjavacas_macberth_2021,beck_ghisbert_2023,palmero_aprosio_bertoldo_2022,gabay_freem_2022}, on the other these models are widely used as a starting point for a number of other important tasks in computational linguistics, such as text reuse detection~\cite{reimers_sentence-bert_2019} or semantic shift detection~\cite{cassotti_xl-lexeme_2023}.
Historical analysis using computational linguistics is challenged by the fact that these MLMs are trained with disregard to historical changes in language and culture. Such disregard is commonly due to the need to collect a critical mass of text in a corpus for training data~\cite{gabay_freem_2022,cuscito_shakespeare_2025}.
This is particularly evident with the Latin language, which has inevitably evolved over its long timespan, but research in language models hugely ignores this.
The work of Riemenschneider and Frank~\cite{riemenschneider_exploring_2023} is a notable exception, also attentive to the historical importance and intertextuality that also underlines the  relevance of Latin to research in other languages from the classical and medieval periods.

In this contribution we study the \emph{historical adequacy} of different MLMs for Latin.
We do this by applying a technique that was previously used to measure such \emph{adequacy} of models to Early Modern English, by capturing the \emph{historical bias} of models and interpreting it as a representation of the socio-temporal context of the model training corpus~\cite{cuscito_shakespeare_2025}.
Key here is an important assumption in computational humanities research, that often remains implicit: this socio-temporal context should be close to the one being studied using these models. Due to the nature of MLMs and the abundance of lexical resources in computational linguistics, this approach is inherently lexical, but evidence shows that it is also able to capture particularities in orthography, grammar, semantics and culture~\cite{cuscito_how_2024,cuscito_shakespeare_2025}.

\section{Background}

To contextualise our work, this section provides an overview of available and relevant corpora and models within our knowledge.

\subsection{Corpora}

We present a number of corpora that are relevant to the experiments conducted here due to their impact in model training and broader computational linguistics. On a general note, a huge challenge in the context of historical research is the scarcity of corpora, due to a number of factors, including loss of manuscripts and limited digitisation. This, together with the inherent high intertextuality, leads to a noteworthy probability that the same text emerges across multiple corpora.

\begin{table}[ht]
\centering
\begin{tabular}{llrrr}
\toprule
Name & Timespan & Words & Share\\
\midrule
Nova                & XV-XXI cent. 	  & 1.7M & 15.1\%  \\
Mediaevalis         & V II-XIV cent.  & 2.4M & 21.4\%  \\
Romana Postclassica & I-VI cent. 	  & 4.6M & 41.6\%  \\
Romana Classica     & I cent. BC  	  & 1.8M & 16.1\%  \\
Romana Antiqua      & VII-II cent. BC & 0.4M &	3.2\%  \\
\bottomrule
\end{tabular}
\caption{Subcorpora in the evaluation corpus \emph{LatinISE}.}
\label{tab:corpora}
\end{table}

As the name suggests, \textbf{LatinISE} is a Latin text corpus collected from three historical sources: LacusCurtius, Intratext and Musisque Deoque~\cite{mcgillivray_tools_2013}. This corpus was developed to be used with SketchEngine\footnote{\url{https://sketchengine.eu}} and thus was collected with semantic annotations in mind.
As further explained in Section~\ref{sec:evaluation}, this is our corpus of choice for the current evaluation and a breakdown of its historical subcorpora is featured in Table~\ref{tab:corpora}.

\textbf{Corpus corporum} is currently the largest curated corpus of Latin~\cite{roelli_new_2024}. It is very likely to contain any other historical curated and publicly available corpora, including the treebanks discussed at the end of this section. This corpus is being actively expanded, and this is our explanation why models trained on it earlier, report smaller training set sizes than ones trained later.

\textbf{The Internet Archive} is a digitisation effort that also includes OCR and with this it has been used as a training corpus. It also includes an important chunk of Latin texts, but this digitsation is an unsupervised process and as a consequence the produced texts are often of poor quality~\cite{bamman_extracting_2012}.

\textbf{CommonCrawl} is the largest web scraping effort online and thus an important corpus for languages, including Latin which counts as one of its top 100 languages. A cleaned corpus with this top  languages subset has been derived and named \textbf{CC100}. However, remotely similarly to the case of the Internet Archive, these corpora have been only automatically curated which has been a source of concerns~\cite{baack_critical_2024}.
By construction it is a contemporary corpus, containing sources such as Wikipedia in Latin.

There is a number of other established smaller corpora, such as \textbf{The Latin Library}, \textbf{Wikipedia in Latin} and the syntactically annotated treebanks \textbf{Perseus Digital Library} and \textbf{PROIEL} project. These are typically included in other aggregation efforts and is part of the reason why the previously mentioned corpora have consequential overlaps in content.

\subsection{Models}

\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\toprule
Model & Corpus & Tokens & Timespan\\
\midrule
LaBerta~\cite{riemenschneider_exploring_2023} & Corpus Corporum & 168M & 700BC-2004\tablefootnote{taken from current version of corpus, which is probably broader than the version at time of retrieval}\\
RoBERTa Latin cased3  & Corpus Corporum & 232M & 700BC-2004\footnotemark[2]\\
Cicero Similis~\cite{cook_what_2021} & 4 small corpora & 1.2M & N/A\tablefootnote{from the model card: ``model training data excludes modern and 19th century texts''}\\
BERT adapter pfeiffer~\cite{pfeiffer_mad-x_2020} & Wikipedia in Latin & N/A & 2002-2025\tablefootnote{based on information from \url{https://en.wikipedia.org/wiki/List_of_Wikipedias\#Wikipedia_editions}}\\
RoBERTa medieval & N/A & N/A & N/A\\
BERT medieval multilingual & CC100,...\tablefootnote{includes CC100, Corpus Corporum and other corpora that cumulatively represent less than 10\%} & 650M & 500BC - 1600\tablefootnote{The model card contains controversial information about the end date of CC100, its biggest subcorpus - XVIII cent. is indicated in one instance.}\\
LatinBERT~\cite{bamman_latin_2020} & Internet Archive,...\tablefootnote{also Patrologia Latina, the Latin Library, Wikipedia in Latin, Corpus Thomisticum, Perseus} & 643M & 200BC-1922\tablefootnote{Since the Internet Archive is the source of >85\% of the data, we consider it to determine the overall period. This is obviously an approximation, e.g. due to the presence of Wikipedia in Latin}\\
\bottomrule
\end{tabular}
\caption{Models and their corresponding training corpora.}
\label{tab:models}
\end{table}

We present the models we included in our evaluation. Most of these were selected due to the documented research that accompanied to their training, but some less documented were also included due to their performance.

\textbf{LaBerta} is one of the models produced in a systematic effort to train language models on classical languages~\cite{riemenschneider_exploring_2023}. It was pre-trained from the RoBERTa~\cite{liu_roberta_2019} architecture on the version of Corpus Corporum that was available at its time, and we believe this explains the different reported number of tokens for this corpus in the context of different models in Table~\ref{tab:models}. Here we select the monolingual model \emph{LaBerta}, even if the multilingual \emph{PhilBerta} could also be very relevant to this evaluation.

\textbf{Cicero Similis} is another model that is presumedly BERT-based and trained on 4 relatively small corpora: Phi5, Tesserae, Thomas Aquinas, and Patrologes Latina~\cite{cook_what_2021}.

\textbf{BERT adapter pfeiffer} is the Latin implementation of a generic adapter approach that uses Wikipedia in different languages as a training corpus~\cite{pfeiffer_mad-x_2020}. Latin is not in the focus of the analysis included in the original publication, nor is mask-filling as a task.

\textbf{LatinBERT} was trained with the BERT architecture~\cite{devlin-etal-2019-bert} on a bespoke corpus that combines the digitisation of materials from the Internet Archive~\cite{bamman_extracting_2012,bamman_latin_2020} and several curated corpora: Perseus Digital Library, the Latin Library, the Patrologia Latina, Corpus Thomisticum and Wikipedia in Latin. The Internet Archive represents more than 85\% of the used corpus size, so the collection of this particular data is of central interest. It relies on the OCR digitisation performed by the Internet Archive, which is of varying quality. To mitigate this problem, the authors retain only the materials that have at least 40\% valid tokens in Latin (which we note is a very low threshold) and upsample the other included corpora to reduce the share of the corpus from the Internet Archive to about 50\%~\cite{bamman_extracting_2012,bamman_latin_2020}.

As shown in Table~\ref{tab:models}, we found several other models, fine-tuned for Latin. From these we included the ones that indicated interesting results during preliminary evaluations. The models we additionally selected are \textbf{RoBERTa Latin cased v3}, \textbf{RoBERTa medieval} and \textbf{BERT medieval multilingual}.
Several other models that we have identified and not included in the evaluation are listed in Appendix~\ref{apx:models}.

We would also like to explicate the fact that when foundation models such as BERT or RoBERTa are fine-tuned, even if this fine-tuning is attentive to the socio-temporal context, contamination from the pre-training is probable.

\section{Method}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/032-fig-process.png}
\caption{The process introduced by Cuscito et al.~\cite{cuscito_shakespeare_2025} and employed here. Note that ``historical'' and ``contemporary'' refer to \emph{Classica} and \emph{Nova} respectively in the context here. The dotted box indicates the perimeter of steps performed with SketchEngine.}
\label{fig:process}
\end{figure*}

Following the evaluation process of Cuscito et al.~\cite{cuscito_shakespeare_2025} illustrated in Fig.~\ref{fig:process}, we perform a masked-word prediction task to assess the historical adequacy of models trained on Latin to the variant of the language used in the Classical period it on one hand, and on the other -- Neo-Latin (see respectively \emph{Romana Classica} and \emph{Nova} in Table~\ref{tab:corpora}).
We deviate from the procedure originally defined in Cuscito et al.~\cite{cuscito_shakespeare_2025} only for the calculation of bias $\beta$, as shown in equation~(\ref{eq:beta}). The entire process is explained in detail below.

For the weighted word lists in the start of the process in Fig.~\ref{fig:process}, we define word frequency functions $f_{nova}$ and $f_{classica}$, which given a word return its normalised relative frequency in the corresponding corpus. Thus, we can employ the equations~(\ref{eq:dist}) and~(\ref{eq:sigma}) defined below, which are just a reformulation of equation (1) in Cuscito et al.~\cite{cuscito_shakespeare_2025}. This reformulation splits the calculation of \emph{historical specificity} ($\sigma$) in two steps to make it more explicit. More specifically, we define an intermediate \emph{distance function} ($d$) in equation~(\ref{eq:dist}), which is later used in equation~(\ref{eq:sigma}).
The constant four in the fourth root functions in equation~(\ref{eq:dist}) is adopted from Cuscito et al.~\cite{cuscito_shakespeare_2025}. It controls how words present in only one corpus mix with ones that are present in both. Values smaller than 4 leave a gap between $\sigma$ scores for words that are present in both corpora, and those that are present in one only (i.e. one of the $f$ functions is null). On the other hand, values greater than 4 lead to words that are exclusive to one of the corpora not getting a greater weight.

\begin{equation}
\begin{array}{r l}
d(f_a,f_b,w) = \sqrt[4]{\sqrt[4]{f_a(w)} -\sqrt[4]{f_b(w)}} & \text{given that}~f_a(w) \geq f_b(w)\\
\end{array}
\label{eq:dist}
\end{equation}

\begin{equation}
\sigma(w) = \left\{
\begin{array}{r l}
\frac{d(f_{nova},f_{classica},w)}{\max_w{ d(f_{nova},f_{classica},w)}} & \text{if}~f_{nova}(w) \geq f_{classica}(w) \\
\frac{-d(f_{classica},f_{nova},w)}{max_w{d(f_{classica},f_{nova},w)}} & \text{otherwise.}
\end{array}
\right.
\label{eq:sigma}
\end{equation}

With the \emph{historical specificity} function, we can measure \emph{sentence valence} ($\rho$) as the trimmed sum of the \emph{specificity} of words contained in it.
Notice that for this \emph{valence} to arrive at an extreme for a sentence, at least two words need to have non-zero \emph{specificity}.
In this way, even when one of these words is masked, the others would still carry some non-zeros valence, rendering the masked sentence indicative of its socio-temporal context.
Only such sentences are selected for evaluation, as reported in the next section.

\begin{equation}
\rho(s) = min(1, max(-1, \sum_{w_i \in s \cap\mathcal{C}}\sigma(w_i)))
\label{eq:rho}
\end{equation}

Weighting the \emph{historic specificity} of word tokens predicted by models ($w_m$), with the corresponding prediction probabilities ($\mathbf{p}_m$), allows us to subsequently define model \emph{historical bias} ($\beta$). As a way to reduce the bias introduced by the very high probability of texts being present both in model training corpora and in our evaluation corpus (i.e. data leakage), during the computation of \emph{historical bias} we exclude the originally encountered word from the assessment by considering \emph{specificity} $\sigma(w)=0$ for it. As a consequence, this also reduces the overall \emph{historical bias} scores of models.

\begin{equation}
\beta(m, s) = \sum_{w_m~\text{for}~s}\left\{
\begin{array}{r l}
0 & \text{if}~w_m~\text{is the original token in the sentence} \\
\sigma(w_m)\mathbf{p}_m & \text{otherwise.}
\end{array}
\right.
\label{eq:beta}
\end{equation}

From the above, also \emph{historical adequacy} ($\delta$) is derived. It provides a normalised measure of \emph{historical bias} regardless of direction:

\begin{equation}
\delta(m, s)=1-\frac{\mid\rho(s)-\beta(m, s)\mid}{2}
\label{eq:delta}
\end{equation}

To elucidate the results of this method, at the end we perform statistical significance testing on \emph{historical adequacy} to see if any models could be claimed better than others in any of the two historical contexts represented by our word lists. First, we compare all models using repeated measures ANOVA. In the case of this indicating possible significance, we proceed to perform a combination of one-tailed paired t-tests to compare models against each other and calculate Cohen's d to measure the corresponding effect sizes.
We do this separately for results on \emph{Classica} and \emph{Nova} sentences, as we expect that different models behave differently in the two contexts.

\section{Evaluation}
\label{sec:evaluation}

\begin{table*}[htbp]

\adjustbox{width=\textwidth}{

\begin{tabular}{llllllllllll}
\toprule
& senatus & uterque & fortuna & lex & populus & et & terrae & rex & vitae & christo & dei \\
\midrule
\textbf{$f_{classica}$} & 0.027 & 0.005 & 0.016 & 0.005  & 0.009 & 1.000 & 0.007 & 0.012 & 0.009 & 0.000 & 0.000 \\
\textbf{$f_{nova}$} & 0.000 & 0.000 & 0.007 & 0.004 &  0.008 & 1.000 & 0.008 & 0.018 & 0.022 & 0.011 & 0.041 \\
\textbf{$\sigma$} & 0.981 & 0.877 & 0.623 & 0.439 & 0.386 & 0.000 & -0.361 & -0.505 & -0.603 & -0.873 & -0.947 \\
\bottomrule
\end{tabular}
}
\caption{An illustrative sample with some values of relative word frequencies and \emph{specificity} ($\sigma$) between the two corpora.}
\label{tab:wordlist}
\end{table*}

We use the LatinISE corpus as a diachronic reference, because we consider it both representative for the evolution of Latin, and -- in combination with SketchEngine -- it affords easily accessible word frequency lists and their corresponding concordances.
To ensure representativeness, we consider the 1000 most frequent words.
We discard the \emph{Romana Antiqua} subcorpus from our considerations due to its small size, as indicated in Table~\ref{tab:corpora}. As a way to emphasise the linguistic evolution over time from the remaining subcorpora we choose the two which are most distant in time, i.e. \emph{Romana Classica} and \emph{Nova}.
Taking the 1000 most frequent words from the two resulted in a combined list of 1329 words scored with \emph{historical specificity} ($\sigma$).
A sample of this scoring is shown in Table~\ref{tab:wordlist}.

T1/Tinos(0)/m/n' undefined
For our analysis, we collect 5000 sentences (concordances from SketchEngine) from each of the \emph{Romana Classica} and the \emph{Nova} subcorpora of LatinISE with \emph{sentence valence} of $\rho=1$ and $\rho=-1$ respectively (which we call \emph{threshold valence}), as samples from these corpora have highest probability to bear distinct socio-temporal connotations. Then we perform the comparison detailed in the previous section. A visual overview of the results could be seen in Fig~\ref{fig:beta-delta}.

\begin{figure*}[!h]
\centering

\includegraphics[width=\linewidth,trim={12 500 30 45},clip]{figures/076-fig-beta-stats.png}
\includegraphics[width=\linewidth,trim={-8 5 0 5},clip]{figures/077-fig-beta-boxplot.png}

\includegraphics[width=\linewidth,trim={0 4 0 23},clip]{figures/077-fig-delta-boxplot.png}
\includegraphics[width=\linewidth,trim={12 500 30 45},clip]{figures/076-fig-delta-stats.png}

\caption{Comparison overview of results for bias $\beta$ (above) and historical adequacy $\delta$ (below). It could be seen that for \emph{Classica} sentences $\delta$ is just rescaling of $\delta$, whereas for \emph{Nova} the values are also mirrored vertically. Thus $\delta$ brings together the two scales of $\beta$.}

\label{fig:beta-delta}
\end{figure*}

For illustrative purposes, in Table~\ref{tab:examples} we explore in detail the results emerging from two sentences in our evaluation. Our selected example from the \emph{Classica} subcorpus is the phrase ``Nonne in terris multa, ut oppidum in Graecia Hippion Argos?'' from \emph{De re rustica} by Varro which translates to ``Are there not many in the world, such as the Greek town of Argos Hippion?''.
Our example from the \emph{Nova} subcorpus, ``Sed statim Solymanus ipso patre acrior Pannoniam invasit.'' from \emph{Icon Animorum} by John Barclay, could be translated as ``But immediately Suleyman, more aggressive than his father himself, invaded Pannonia.''
Notably for the second, models propose the originally present word ``in'', but it doesn't contribute to the corresponding scores. This occurs also in other test sentences and, as already mentioned, draws the absolute value of overall scores of the models down.

The tokenisers of some models (as seen in Table~\ref{tab:examples} for \emph{BERT medieval multilingual} on the \emph{Nova} example) produce tokens that are not words and which do not get attributed \emph{valence} even if they might be meaningful. Similarly, even though this did not make it into the illustrative example shown here, the \emph{Latin BERT} model commonly predicts word tokens with orthographic defects, probably due to OCR, which also do not get \emph{valence} assigned. As a consequence \emph{Latin BERT} obtains a very low score and was excluded from the quantitative comparison.

For statistical significance testing, we fist perform repeated measures ANOVA on the results of the different models, with \emph{historical adequacy} ($\delta$) as a dependent variable and the evaluation corpus groupings as an independent variable (also referred to as within-subject factor) and this gave us a reference probability of $p=0.0001$ and F-statistic $F=121.2929$. This probability is below the 1\textperthousand~confidence threshold and thus very strong indication of statistical evidence that the models do not perform equally.

\begin{figure*}[htbp]
\centering

\includegraphics[width=.9\textwidth]{figures/079-fig-significance-delta-heatmap-postprocessed.png}

\caption{The pairwise statistical  analysis results, probabilities of t-tests and Cohen's \emph{d}.}
\label{fig:significance}
\end{figure*}

An overview of the pairwise t-tests on \emph{historical adequacy} ($\delta$) scores could be seen in Fig~\ref{fig:significance}. In it only pairwise significant differences are shown, with reported direction of significance (color), t-test probability and effect size (Cohen's d).

The heatmap for \emph{Classica} sentences shows that \emph{BERT medieval multilingual} consistently outperforms other models, followed by \emph{RoBERTa medieval} and \emph{BERT adapter pfeiffer}. Then with less consistent significance, yet in this order, follow \emph{Cicero Similis} and \emph{LaBerta}, even if the former does not perform significantly better than the model that ranks last, i.e. \emph{Roberta Latin cased3}.
The result for \emph{Nova} sentences shows that \emph{BERT adapter pfeiffer} is worse than the other models, followed by \emph{BERT medieval multilingual}. This pair is the only one that has statistical significance that is above the 1\textperthousand~confidence threshold, yet it is still below the 5\% threshold which is commonly accepted in the social sciences and humanities.
As for the other models, they are statistically indistinguishable in the \emph{Nova} context.
However, as seen in Fig~\ref{fig:significance}, the measured effect sizes turn out to be rather small.
This is a particularly important sign, because of the large number of sampled sentences (n=5000 for each of the periods).

\section{Discussion}

In the adopted approach we consider the most frequent word tokens from the evaluation corpora, regardless of their content or grammatical function. As illustrated by the example sentences provided in the previous section, this has the effect of capturing not barely lexical differences, but more complex phenomena like geographic relationships or historical circumstances. These two are just examples of the context contributing to the evaluation of the choice of words.
While admittedly just considering the most frequently used vocabulary does not capture all of this context, it does capture orthographic and morphological variation, grammatical evolution, and socio-cultural phenomena.
However, due to Latin being a highly inflected and syntactically complex language, the integrity of this generic approach might be questioned and alternative ways of calculating specificity could be considered.

To justify the exclusion of the \emph{Romana Antiqua} subcorpus due to the sentence identification process, consider that sentences need to both contain the words to be masked, and have a \emph{threshold valence} ($\rho=\pm1$) as a sign that the sentences are contextually charged for the corresponding period.
As an illustration of the importance of the latter, consider a contrast to a sentence we would not like to have included, i.e. one with generic references that does not provide clues about its socio-historical context. A naive illustrative example could be ``Caelum stellatum aspicio''. It translates to ``I look at the starry sky'' and the context it carries is generic and does not relate to a particular culture.

Overall, the evaluation tends to produce results for \emph{bias} which are very close to neutral (i.e. $\beta=0)$. This is noticeably more so when compared to the results for historical models in the study of English reported by Cuscito et al.~\cite{cuscito_shakespeare_2025}.
While undoubtedly our modification of excluding the original word token from the evaluation had an impact on this, we speculate about other contributing factors.
Our main hypthesised reason is plainly that models were trained on corpora indiscriminately and thus have a mixed representation across periods. If the \emph{LatinISE} corpus could be an indication, the bulk of available texts comes from the the period of the \emph{Postclassica} corpus, which means that they are both socio-temporally close to the context of \emph{Classica}, and have had cultural influence on \emph{Nova}. On the other hand, a more complex view of dating could need to be considered, because the historical tradition of hand-copying manuscripts has lead to the phenomenon that in certain cases only more recent copies of classical manuscripts are preserved and exhibit medieval features introduced by the scribes~\cite{omayio_historical_2022}.

Considering the models, \emph{BERT medieval multilingual} appears to perform best for the historical context of interest (the Classical period, represented by the \emph{Classica} corpus), which is also observable by the 75\% threshold line for \emph{bias} in Fig.~\ref{fig:beta-delta}.
It is a surprise that \emph{BERT adapter pfeiffer} performs relatively well on the \emph{Classica} corpus, because it is supposedly the only model that is trained only on contemporarily written Latin texts (i.e. Wikipedia in Latin, as indicated in Table~\ref{tab:models}).
When it comes to the evaluations in the \emph{Nova} context, a reason why four models -- namely \emph{Cicero Similis}, \emph{LaBerta}, \emph{RoBERTa mesdieval} and \emph{Roberta Latin cased3} -- appear indistinguishable could
be due to the high number of more recent Latin corpora being used for training.

Arguably, the results of \emph{Latin BERT} -- and \emph{the Internet Archive} respectively -- show that unlike the case of printed documents where optical character recognition (OCR) might have caused a breakthrough in terms of mass availability of training corpora, this is yet to happen with historical documents and, respectively, handwritten text recognition (HTR).
Yet, despite a failure to generate exact meaningful words that could be captured by the method employed here, such models are still valuable for the approximations they are able to produce in under-resourced contexts where there is nothing better available.

\section{Conclusion}

The proposed evaluation approach builds on the central role of words in both MLMs and Corpus Linguistics, particularly expressed in the affordances of tools like SketchEngine.
Whereas the method is computationally robust, the effect size measures in the statistical analysis limit conclusions to little beyond what is visible from Fig.~\ref{fig:beta-delta}.

An open question remains whether narrowing down the choice of word tokens considered in the evaluation could result in a more clear distinction between model performance.
An example of doing this could be making a distinction between word types, even if this means simply content vs functional words.
More targeted word selection could be attempted by masking and quantifying the specificity of -- for example -- content words only. Alternatively, a lemmatized vocabulary could be considered as a way to capture also less common word forms in the calculation of adequacy.
The particular case of \emph{Latin BERT} raises the question whether in future research the current evaluation method should be adapted towards mitigating transcription noise, or keep it as it is as a way to account for orthographic variation.

Further experiments need to compare other periods, particularly involving the content-rich preiods that correspond to the \emph{Romana Postclassica} and \emph{Medievalis} periods in the \emph{LatinISE} corpus. Particularly interesting are comparisons involving the linguistic variation in Medieval Latin.

As previously suggested, the widely adopted ``don't stop pre-training'' principle~\cite{gururangan_dont_2020} carries risks of historical bias (e.g. through anachronisms)~\cite{underwood_can_2025,riemenschneider_exploring_2023,cuscito_shakespeare_2025}.
One way to train models that are historically-aware is through the pre-training of dedicated models to historical periods.
This is due to another principle -- ``what is done is done'' -- reminding that historical memory can develop only in the direction of time~\cite{periti_what_2022}. That is, models trained on medieval texts are expected to have some historical memory of previous periods, but not of subsequent periods.
This is why models with a medieval cut-off date for their corpus are also of interest.
Yet, \emph{BERT medieval multilingual}, assumingly from its name, gets closer to such idea and this might be a possible explanation for its better performance on \emph{Classica} sentences than on \emph{Nova} ones.
In our analysis, despite naming, we found no conclusive evidence that such historically-aware models exist, as summarised in Table~\ref{tab:models}.

Finally, even though in recent years attention has shifted away from MLM autoencoders towards next token autoregression models, the reemergence of the fill-in-the-middle paradigm~\cite{bavarian_efficient_2022,nguyen_meet_2023} reaffirms that some form of mask-filling approaches will continue to have their relevance also in future.

\printbibliography

\appendix

\section{Further Models}
\label{apx:models}

The following models for Latin have been identified on HuggingFace:

\begin{itemize}
\item \url{http://hf.co/AdapterHub/bert-base-multilingual-cased\_la\_wiki\_pfeiffer}
\item \url{http://hf.co/bowphs/LaBerta}~\cite{riemenschneider_exploring_2023}
\item \url{http://hf.co/bowphs/PhilBerta}~\cite{riemenschneider_exploring_2023}
\item \url{http://hf.co/Cicciokr/XLM-Roberta-Base-Latin-Uncased}
\item \url{http://hf.co/ClassCat/roberta-base-latin-v2}
\item \url{http://hf.co/cook/cicero-similis}
\item \url{http://hf.co/HPLT/hplt\_bert\_base\_la}~\cite{de_gibert_new_2024}
\item \url{http://hf.co/LuisAVasquez/simple-latin-bert-uncased}
\item \url{http://hf.co/magistermilitum/BERT\_medieval\_multilingual}
\item \url{http://hf.co/magistermilitum/RoBERTa\_medieval}
\item \url{http://hf.co/pnadel/LatinBERT}~\cite{bamman_latin_2020}
\item \url{http://hf.co/pstroe/roberta-base-latin-cased}
\item \url{http://hf.co/pstroe/roberta-base-latin-cased2}
\item \url{http://hf.co/pstroe/roberta-base-latin-cased3}
\end{itemize}

\end{document}