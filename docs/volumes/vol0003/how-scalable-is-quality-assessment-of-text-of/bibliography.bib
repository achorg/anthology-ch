@inproceedings{alex2014estimating,
  title={Estimating and rating the quality of optically character recognised text},
  author={Alex, Beatrice and Burns, John},
  booktitle={Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage},
  pages={97--102},
  year={2014},
  doi={10.1145/2595188.2595214}
}

@article{Correia_Luck_2023, title={Digitizing historical balance sheet data: A practitioner’s guide}, volume={87}, DOI={10.1016/j.eeh.2022.101475}, journal={Explorations in Economic History}, author={Correia, Sergio and Luck, Stephan}, year={2023}, month=jan, pages={101475}}

@inproceedings{cuper2023unraveling,
  title={Unraveling confidence: examining confidence scores as proxy for OCR quality},
  author={Cuper, Mirjam and van Dongen, Corine and Koster, Tineke},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={104--120},
  year={2023},
  organization={Springer},
  doi={10.1007/978-3-031-41734-4_7}
}

@article{cuper_examining_2022,
	title = {Examining a {Multi} {Layered} {Approach} for {Classification} of {OCR} {Quality} without {Ground} {Truth}},
	volume = {4},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.17613/03DS-9973},
	abstract = {While the digital availability of heritage text collections is increasing, there is a lack of reliable methods to assess the OCR quality of these texts. There are several possible measures, but each has its disadvantages. We examined if, instead of a single measure, a combination of measures would give a more accurate indication of OCR quality. We therefore built a first version of a multi-layered approach for the classification of OCR quality, named QuPipe. We tested QuPipe on a set of sentences from 17th century Dutch newspapers and found that using QuPipe led to an increase in correctly classifying the quality. However, although these results are positive, QuPipe needs to be developed and tested further to become feasible as an instrument to classify the OCR quality.},
	journal = {DH Benelux Journal. The Humanities in a Digital World},
	author = {Cuper, Mirjam},
	month = jul,
	year = {2022},
	pages = {42--59},
}

@misc{ehrmann_extended_2020,
	title = {Extended {Overview} of {CLEF} {HIPE} 2020: {Named} {Entity} {Processing} on {Historical} {Newspapers}},
	shorttitle = {Extended {Overview} of {CLEF} {HIPE} 2020},
	doi = {10.5167/UZH-200192},
	author = {Ehrmann, Maud and Romanello, Matteo and Flückiger, Alex and Clematide, Simon},
	month = sep,
	year = {2020},
	note = {Publisher: CEUR-WS},
}

@article{fleischhacker_enhancing_2025,
	title = {Enhancing {OCR} in historical documents with complex layouts through machine learning},
	volume = {26},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {1432-5012, 1432-1300},
	doi = {10.1007/s00799-025-00413-z},
	abstract = {Abstract          This paper explores the challenge of processing and extracting information from large quantities of printed serial sources from the 19th century, which have been largely untapped due to the inadequacies of existing extraction techniques. We focus on the Habsburg Central Europe’s Hof- und Staatsschematismus, a comprehensive record published between 1702 and 1918 that documents the Habsburg civil service’s hierarchy and the evolution of its central administration over two centuries. Our approach sees the significant investment into machine learning-driven layout detection prior to the OCR-process. We generated synthetic data mimicking the Hof- und Staatsschematismus style for initial training of a Faster R-CNN model, followed by fine-tuning the model with a smaller dataset of manually annotated historical documents. Subsequently, we optimised Tesseract-OCR for our document style to enhance the combined structure extraction and OCR process. Our evaluation demonstrates significant improvements in OCR performance metrics (WER and CER), with the combined structure detection and fine-tuned OCR process showing a decrease in error rates of 15.68 percentage points for CER and 19.95 percentage points for WER. These findings underscore the potential of ML techniques in facilitating the extraction and analysis of historical documents.},
	number = {1},
	journal = {International Journal on Digital Libraries},
	author = {Fleischhacker, David and Kern, Roman and Göderle, Wolfgang},
	month = mar,
	year = {2025},
	note = {Publisher: Springer Science and Business Media LLC},
}

@incollection{franke_maier_methoden_2021,
	title = {Methoden und {Metriken} zur {Messung} von {OCR}-{Qualität} für die {Kuratierung} von {Daten} und {Metadaten}},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	booktitle = {Qualität in der {Inhaltserschließung}},
	publisher = {De Gruyter},
	author = {Neudecker, Clemens and Zaczynska, Karolina and Baierer, Konstantin and Rehm, Georg and Gerber, Mike and Schneider, Julián Moreno},
	editor = {Franke-Maier, Michael and Kasprzik, Anna and Ledl, Andreas and Schürmann, Hans},
	month = sep,
	year = {2021},
	doi = {10.1515/9783110691597-009},
	pages = {137--166},
}

@inproceedings{gupta2015automatic,
  title={Automatic assessment of OCR quality in historical documents},
  author={Gupta, Anshul and Gutierrez-Osuna, Ricardo and Christy, Matthew and Capitanu, Boris and Auvil, Loretta and Grumbach, Liz and Furuta, Richard and Mandell, Laura},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  number={1},
  year={2015},
  doi={10.1609/aaai.v29i1.9487}
}

@inproceedings{hamdi_analysis_2019,
	address = {Champaign, IL, USA},
	title = {An {Analysis} of the {Performance} of {Named} {Entity} {Recognition} over {OCRed} {Documents}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	doi = {10.1109/JCDL.2019.00057},
	booktitle = {2019 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	publisher = {IEEE},
	author = {Hamdi, Ahmed and Jean-Caurant, Axel and Sidere, Nicolas and Coustaty, Mickael and Doucet, Antoine},
	month = jun,
	year = {2019},
	pages = {333--334},
}

@article{hill2019quantifying,
  title={Quantifying the impact of dirty OCR on historical text analysis: Eighteenth Century Collections Online as a case study},
  author={Hill, Mark J and Hengchen, Simon},
  journal={Digital Scholarship in the Humanities},
  volume={34},
  number={4},
  pages={825--843},
  year={2019},
  publisher={Oxford University Press},
  doi={10.1093/llc/fqz024}
}

@article{holley_how_2009,
	title = {How {Good} {Can} {It} {Get}?: {Analysing} and {Improving} {OCR} {Accuracy} in {Large} {Scale} {Historic} {Newspaper} {Digitisation} {Programs}},
	volume = {15},
	issn = {1082-9873},
	doi = {10.1045/march2009-holley},
	number = {3/4},
	journal = {D-Lib Magazine},
	author = {Holley, Rose},
	month = mar,
	year = {2009},
	note = {Publisher: CNRI Acct},
}

@article{ines_jerele_optical_2012,
	title = {Optical {Character} {Recognition} of {Historical} {Texts}: {End}-{User} {Focused} {Research} for {Slovenian} {Books} and {Newspapers} from the 18th and 19th {Century}},
	url = {http://eudml.org/doc/254555},
	number = {21},
	journal = {Review of the National Center for Digitization},
	author = {Jerele, Ines and
              Erjavec, Tomaž and
              Pokorn, Daša and
              Kavčič-Čolić, Alenka},
	year = {2012},
	note = {Publisher: Faculty of Mathematics},
	pages = {117--126}
}

@inproceedings{kiessling_2025_version,
author = {Kiessling, Benjamin},
title = {Version 5 of the Kraken ATR Engine for the Humani\-ties},
year = {2025},
publisher = {Springer - Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-04624-6_26},
doi = {10.1007/978-3-032-04624-6_26},
abstract = {Automatic text recognition for contemporary and historical printed&nbsp;or handwritten works has become a crucial tool in the inventory of&nbsp;many humanities scholars employing digital methods, but few retrodigitization software packages are adapted to this environment with its unparalleled diversity, domain-specific conventions, and low-resource settings.We examine how the design principles behind kraken, a freely-licensed&nbsp;ATR engine optimized for use on historical and non-Latin script documents, pertain to specific use cases encountered in the humanities and allow it&nbsp;to be easily adapted to process even highly non-conventional material.&nbsp;In addition, we shine a light on the significant functional additions, performance enhancements, and quality of life improvements added in its recent version 5 stable releases. Among these are advanced reading order support including trainable reading order, unsupervised recognition pretraining,&nbsp;and a new public model repository with enhanced metadata.The latest release of the software can be found at},
booktitle = {Docu\-ment Ana\-ly\-sis and Recog\-ni\-tion – ICDAR 2025: 19th Inter\-natio\-nal Conference, Wuhan, China, September 16–21, 2025, Proceedings, Part III},
pages = {443–-458},
numpages = {16},
keywords = {Handwritten Text Recognition, Layout Analysis, Reading Order, Open Source Software},
location = {Wuhan, China}
}

@article{kirchner_ocr_2016,
	title = {{OCR} bei {Inkunabeln} -- {Offizinspezifischer} {Ansatz} der {Universitätsbibliothek} {Würzburg}},
	volume = {36},
	issn = {2191-4664, 0720-6763},
	doi = {10.1515/abitech-2016-0036},
	abstract = {Zusammenfassung: Im Rahmen des BMBF-geförderten Projekts KALLIMACHOS an der Universität Würzburg soll unter anderem die Textgrundlage für digitale Editionen per OCR gewonnen werden. Das Bearbeitungskorpus besteht aus deutschen, französischen und lateinischen Inkunabeln. Dieser Artikel zeigt, wie man mit bereits heute existierenden Methoden und Programmen den Problemen bei der OCR von Inkunabeln entgegentreten kann. Hierzu wurde an der Universitätsbibliothek Würzburg ein Verfahren erprobt, mit dem auf ausgewählten Werken einer Druckerwerkstatt bereits Zeichengenauigkeiten von bis zu 95 Prozent und Wortgenauigkeiten von bis zu 73 Prozent erzielt werden.
	Abstract: In the context of the Kallimachos project at the University of Würzburg, the textual base for digital editions of incunabula is obtained via OCR. The corpus of texts to be worked on consists of German, French, and Latin opera. This article shows how problems with OCR of incunabula can be tackled with already existing methods and programs. The developed method focuses on setting up a type-specific OCR training to be reused with different medieval printings from one printshop. Following this method we achieved letter accuracies of up to 95 percent and word accuracies of up to 73 percent.},
	number = {3},
	journal = {ABI Technik},
	author = {Kirchner, Felix and Dittrich, Marco and Beckenbauer, Phillip and Nöth, Maximilian},
	month = sep,
	year = {2016},
	pages = {178--188},
}

@inproceedings{koistinen-etal-2017-improving,
    title = "Improving Optical Character Recognition of {F}innish Historical Newspapers with a Combination of Fraktur {\&} Antiqua Models and Image Preprocessing",
    author = {Koistinen, Mika  and
      Kettunen, Kimmo  and
      P{\"a}{\"a}kk{\"o}nen, Tuula},
    editor = {Tiedemann, J{\"o}rg  and
      Tahmasebi, Nina},
    booktitle = "Proceedings of the 21st Nordic Conference on Computational Linguistics",
    month = may,
    year = "2017",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-0238/",
    pages = "277--283"
}

@inproceedings{neudecker2021survey,
  title={A survey of OCR evaluation tools and metrics},
  author={Neudecker, Clemens and Baierer, Konstantin and Gerber, Mike and Clausner, Christian and Antonacopoulos, Apostolos and Pletschacher, Stefan},
  booktitle={Proceedings of the 6th International Workshop on Historical Document Imaging and Processing},
  pages={13--18},
  year={2021},
  doi={10.1145/3476887.3476888}
}

@article{Novotny_2020,
	title = {When Tesseract Does It Alone: Optical Character Recognition of Medieval Texts},
	issn = {2336-4289},
	url = {https://nlp.fi.muni.cz/raslan/raslan20.pdf},
    pages = "3--22",
	journal = {RASLAN -- Recent Advances in Slavonic Natural Processing},
	author = {Novotný, Vít},
	month = dec,
	year = {2020},
	note = {Publisher: Tribun EU},
}

@inproceedings{popat2009panlingual,
  title={A panlingual anomalous text detector},
  author={Popat, Ashok C},
  booktitle={Proceedings of the 9th ACM symposium on Document engineering},
  pages={201--204},
  year={2009},
  doi={10.1145/1600193.1600237}
}

@inproceedings{rezanezhad2023document,
author = {Rezanezhad, Vahid and Baierer, Konstantin and Gerber, Mike and Labusch, Kai and Neudecker, Clemens},
title = {Document Layout Analysis with Deep Learning and Heuristics},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3604951.3605513},
abstract = {The automated yet highly accurate layout analysis (segmentation) of historical document images remains a key challenge for the improvement of Optical Character Recognition (OCR) results. But historical documents exhibit a wide array of features that disturb layout analysis, such as multiple columns, drop capitals and illustrations, skewed or curved text lines, noise, annotations, etc. We present a document layout analysis (DLA) system for historical documents implemented by pixel-wise segmentation using convolutional neural networks. In addition, heuristic methods are applied to detect marginals and to determine the reading order of text regions. Our system can detect more layout classes (e.g. initials, marginals) and achieves higher accuracy than competitive approaches. We describe the algorithm, the different models and how they were trained and discuss our results in comparison to the state-of-the-art on the basis of three historical document datasets.},
booktitle = {Proceedings of the 7th International Workshop on Historical Document Imaging and Processing},
pages = {73--78},
numpages = {6},
keywords = {Segmentation, Reading order detection, Document layout analysis},
location = {San Jose, CA, USA},
series = {HIP '23}
}

@inproceedings{sankaran2013error,
  title={Error detection in highly inflectional languages},
  author={Sankaran, Naveen and Jawahar, CV},
  booktitle={2013 12th International Conference on Document Analysis and Recognition},
  pages={1135--1139},
  year={2013},
  organization={IEEE},
  doi={10.1109/ICDAR.2013.230}
}

@incollection{sfikas_confidence-aware_2024,
	address = {Cham},
	title = {Confidence-{Aware} {Document} {OCR} {Error} {Detection}},
	volume = {14994},
	booktitle = {Document {Analysis} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Hemmer, Arthur and Coustaty, Mickaël and Bartolo, Nicola and Ogier, Jean-Marc},
	editor = {Sfikas, Giorgos and Retsinas, George},
	year = {2024},
	doi = {10.1007/978-3-031-70442-0_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--228},
}

@inproceedings{smith2007overview,
  title={An overview of the Tesseract OCR engine},
  author={Smith, Ray},
  booktitle={Ninth international conference on document analysis and recognition (ICDAR 2007)},
  volume={2},
  pages={629--633},
  year={2007},
  organization={IEEE},
  doi={10.1109/ICDAR.2007.4376991}
}

@inproceedings{strobel_evaluation_2022,
	address = {Marseille, France},
	title = {Evaluation of {HTR} models without {Ground} {Truth} {Material}},
	url = {https://aclanthology.org/2022.lrec-1.467/},
	abstract = {The evaluation of Handwritten Text Recognition (HTR) models during their development is straightforward: because HTR is a supervised problem, the usual data split into training, validation, and test data sets allows the evaluation of models in terms of accuracy or error rates. However, the evaluation process becomes tricky as soon as we switch from development to application. A compilation of a new (and forcibly smaller) ground truth (GT) from a sample of the data that we want to apply the model on and the subsequent evaluation of models thereon only provides hints about the quality of the recognised text, as do confidence scores (if available) the models return. Moreover, if we have several models at hand, we face a model selection problem since we want to obtain the best possible result during the application phase. This calls for GT-free metrics to select the best model, which is why we (re-)introduce and compare different metrics, from simple, lexicon-based to more elaborate ones using standard language models and masked language models (MLM). We show that MLM-based evaluation can compete with lexicon-based methods, with the advantage that large and multilingual transformers are readily available, thus making compiling lexical resources for other metrics superfluous.},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Ströbel, Phillip Benjamin and Volk, Martin and Clematide, Simon and Schwitter, Raphael and Hodel, Tobias and Schoch, David},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {4395--4404},
}

@inproceedings{van_strien_assessing_2020,
	address = {Valletta, Malta},
	title = {Assessing the {Impact} of {OCR} {Quality} on {Downstream} {NLP} {Tasks}},
	doi = {10.5220/0009169004840496},
	abstract = {A growing volume of heritage data is being digitized and made available as text via optical character recognition (OCR). Scholars and libraries are increasingly using OCR-generated text for retrieval and analysis. However, the process of creating text through OCR introduces varying degrees of error to the text. The impact of these errors on natural language processing (NLP) tasks has only been partially studied. We perform a series of extrinsic assessment tasks — sentence segmentation, named entity recognition, dependency parsing, information retrieval, topic modelling and neural language model fine-tuning — using popular, out-of-the-box tools in order to quantify the impact of OCR quality on these tasks. We find a consistent impact resulting from OCR errors on our downstream tasks with some tasks more irredeemably harmed by OCR errors. Based on these results, we offer some preliminary guidelines for working with text produced through OCR.},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {van Strien, Daniel and Beelen, Kaspar and Ardanuy, Mariona and Hosseini, Kasra and McGillivray, Barbara and Colavizza, Giovanni},
	year = {2020},
	pages = {484--496},
}

@article{tanner_measuring_2009,
	title = {Measuring {Mass} {Text} {Digitization} {Quality} and {Usefulness}: {Lessons} {Learned} from {Assessing} the {OCR} {Accuracy} of the {British} {Library}'s 19th {Century} {Online} {Newspaper} {Archive}},
	volume = {15},
	issn = {1082-9873},
	doi = {10.1045/july2009-munoz},
	number = {7/8},
	journal = {D-Lib Magazine},
	author = {Tanner, Simon and Muñoz, Trevor and Ros, Pich Hemy},
	month = jul,
	year = {2009},
	note = {Publisher: CNRI Acct},
}

@incollection{traub_impact_2015,
	address = {Cham},
	title = {Impact {Analysis} of {OCR} {Quality} on {Research} {Tasks} in {Digital} {Archives}},
	copyright = {http://www.springer.com/tdm},
	booktitle = {Lecture {Notes} in {Computer} {Science}},
	publisher = {Springer International Publishing},
	author = {Traub, Myriam C.},
	editor = {Van Ossenbruggen, Jacco and Hardman, Lynda},
	year = {2015},
	doi = {10.1007/978-3-319-24592-8_19},
	note = {ISSN: 0302-9743, 1611-3349},
	pages = {252--263},
}

@online{wick2020calamari,
  author  = {Christoph Wick and Christian Reul and Frank Puppe},
  title   = {Calamari -- A High-Per\-for\-mance Ten\-sor\-flow-based Deep Lear\-ning Package for Optical Character Recognition},
  year    = {2020},
  journal = {Digital Humanities Quarterly},
  volume  = {14},
  number  = {2},
  url     = {https://dhq.digitalhumanities.org/vol/14/2/000451/000451.html}
}

@inproceedings{wudtke2011recognizing,
  title={Recog\-nizing gar\-bage in OCR output on his\-tori\-cal docu\-ments},
  author={Wudtke, Richard and Ringlstetter, Christoph and Schulz, Klaus U},
  booktitle={Proceedings of the 2011 Joint Workshop on Mul\-ti\-lingual OCR and Ana\-ly\-tics for Noisy Un\-structured Text Data},
  pages={1--6},
  year={2011},
  doi={10.1145/2034617.2034626}
}
