@misc{greif2025multimodalllmsocrocr,
      title={Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents}, 
      author={Gavin Greif and Niclas Griesshaber and Robin Greif},
      year={2025},
      eprint={2504.00414},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00414}, 
}

@misc{Kanerva_Ledins_Kapyaho_Ginter_2025, title={OCR Error Post-Correction with LLMs in Historical Documents: No Free Lunches}, DOI={10.48550/arXiv.2502.01205}, abstractNote={Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora.}, note={arXiv:2502.01205 [cs]}, number={arXiv:2502.01205}, publisher={arXiv}, author={Kanerva, Jenna and Ledins, Cassandra and Käpyaho, Siiri and Ginter, Filip}, year={2025}, month={feb}, }

@incollection{bauderjones2025,
    author = {Julia Bauder and Chris Jones} ,
    title = {{Using Generative AI to Turn 19th-
Century Library Catalogues into Data:
Applications and Limitations}},
    booktitle = {Library Catalogues as Data: Research, Practice and Usage},
    publisher = {Facet Publishing},
    year = {2025},
    pages = {167-184},
    editor = {Paul Gooding, Melissa Terras
and Sarah Ames},
}

@misc{Kim_Baudru_Ryckbosch_Bersini_Ginis_2025, title={Early evidence of how LLMs outperform traditional systems on OCR/HTR tasks for historical records},  DOI={10.48550/arXiv.2501.11623}, abstractNote={We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to transcribe historical handwritten documents in a tabular format and compare their performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract, and TrOCR. Considering the tabular form of the data, two types of experiments are executed: one where the images are split line by line and the other where the entire scan is used as input. Based on CER and BLEU, we demonstrate that LLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the evaluated CER and BLEU scores to human evaluations to better judge the outputs of whole-scan experiments and understand influential factors for CER and BLEU. Combining judgments from all the evaluation metrics, we conclude that two-shot GPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan images yield the transcriptions of the historical records most similar to the ground truth.}, note={arXiv:2501.11623 [cs]}, number={arXiv:2501.11623}, publisher={arXiv}, author={Kim, Seorin and Baudru, Julien and Ryckbosch, Wouter and Bersini, Hugues and Ginis, Vincent}, year={2025}, month={jan}, }

@article{Backes_Iurshina_Shahid_Mayr_2024, title={Comparing free reference extraction pipelines}, volume={25}, DOI={https://doi.org/10.1007/s00799-024-00404-6}, abstractNote={In this paper, we compare the performance of several popular pre-trained reference extraction and segmentation toolkits combined in different pipeline configurations on three different datasets. The extraction is end-to-end, i.e. the input is PDF documents, and the output is parsed reference objects. The evaluation is for reference strings and individual fields in the reference objects using alignment by identical fields and close-to-identical values. Our results show that Grobid and AnyStyle perform best of all compared tools, although one may want to use them in combination. Our work is meant to serve as a reference for researchers interested in applying out-of-the-box reference extraction and -parsing tools, for example, as a preprocessing step to a more complex research question. Our detailed results on different datasets with results for individual parsed fields will allow them to focus on aspects that are particularly important to them.}, number={4}, journal={International Journal on Digital Libraries}, author={Backes, Tobias and Iurshina, Anastasiia and Shahid, Muhammad Ahsan and Mayr, Philipp}, year={2024}, month={dec}, pages={841–853}, language={en}, }

@inproceedings{Ghiriti_Goderle_Kern_2024, address={Berlin, Heidelberg}, title={Exploring the Capabilities of GPT4-Vision as OCR Engine}, DOI={https://doi.org/10.1007/978-3-031-72440-4_1}, abstractNote={Many museums and libraries conducted efforts to digitize their assets, and many historic documents are now available as digital images. However, these documents are not directly accessible to retrieval systems that rely on written text and not images. In this study, the novel GPT4-Vision is being studied for its ability of optical character recognition (OCR), in cases where established methods, such as Tesseract may have difficulties. We find that GPT4-Vision provides excellent results even in cases where even humans struggle. We also identified a number of key limitations, including the long runtime implying high energy requirements, the lack of handling of rotated images, the necessity for layout hints, and limitations regarding image size. Even with these limitations, it is expected that large language models and vision transformers will play an important role to make historical documents more accessible for further processing, or directly to users.}, booktitle={Linking Theory and Practice of Digital Libraries: 28th International Conference on Theory and Practice of Digital Libraries, TPDL 2024, Ljubljana, Slovenia, September 24–27, 2024, Proceedings, Part II}, publisher={Springer-Verlag}, author={Ghiriti, Alex and Göderle, Wolfgang and Kern, Roman}, year={2024}, month={sept}, pages={3–12}, }

@misc{Shi_Peng_Liao_Lin_Chen_Liu_Zhang_Jin_2023, title={Exploring OCR Capabilities of GPT-4V(ision): A Quantitative and In-depth Evaluation}, DOI={https://doi.org/10.48550/arXiv.2310.16809}, abstractNote={This paper presents a comprehensive evaluation of the Optical Character Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large Multimodal Model (LMM). We assess the model’s performance across a range of OCR tasks, including scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich document. The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image. Based on these observations, we affirm the necessity and continued research value of specialized OCR models. In general, despite its versatility in handling diverse OCR tasks, GPT-4V does not outperform existing state-of-the-art OCR models. How to fully utilize pre-trained general-purpose LMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study offers a critical reference for future research in OCR with LMMs. Evaluation pipeline and results are available at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.}, note={arXiv:2310.16809 [cs]}, number={arXiv:2310.16809}, publisher={arXiv}, author={Shi, Yongxin and Peng, Dezhi and Liao, Wenhui and Lin, Zening and Chen, Xinhong and Liu, Chongyu and Zhang, Yuyi and Jin, Lianwen}, year={2023}, month={oct}, }

@article{Liu_Li_Huang_Yang_Yu_Li_Yin_Liu_Jin_Bai_2024, title={OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models}, volume={67}, DOI={https://doi.org/10.1007/s11432-024-4235-6}, abstractNote={Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. However, their effectiveness in text-related visual tasks remains relatively unexplored. In this paper, we conducted a comprehensive evaluation of Large Multimodal Models, such as GPT4V and Gemini, in various text-related visual tasks including Text Recognition, Scene Text-Centric Visual Question Answering (VQA), Document-Oriented VQA, Key Information Extraction (KIE), and Handwritten Mathematical Expression Recognition (HMER). To facilitate the assessment of Optical Character Recognition (OCR) capabilities in Large Multimodal Models, we propose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29 datasets, making it the most comprehensive OCR evaluation benchmark available. Furthermore, our study reveals both the strengths and weaknesses of these models, particularly in handling multilingual text, handwritten text, non-semantic text, and mathematical expression recognition. Most importantly, the baseline results presented in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. The evaluation pipeline and benchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.}, note={arXiv:2305.07895 [cs]}, number={12}, journal={Science China Information Sciences}, author={Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xucheng and Liu, Cheng-lin and Jin, Lianwen and Bai, Xiang}, year={2024}, }

@book{Kaplan_1961, address={Madison}, title={A Bibliography of American Autobiographies}, callNumber={Z1224 .K3}, publisher={University of Wisconsin Press}, author={Kaplan, Louis}, year={1961}, }

@book{Wright_1939, address={California}, title={American Fiction, 1774-1850; A Contribution Toward a Bibliography}, publisher={Ward Ritchie Press}, author={Wright, Lyle}, year={1939}, }

@book{Watson_et_al_1969, address={Cambridge}, title={The New Cambridge Bibliography of English Literature}, publisher={Cambridge University Press}, author={Watson, George, and Pickles, J. D and Willison, Ian Roy and Bateson, Frederick Wilse}, year={1969}, }
