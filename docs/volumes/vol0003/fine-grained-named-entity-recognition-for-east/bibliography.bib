@inproceedings{hu-etal-2023-entity,
    title = "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
    author = "Hu, Xuming  and
      Jiang, Yong  and
      Liu, Aiwei  and
      Huang, Zhongqiang  and
      Xie, Pengjun  and
      Huang, Fei  and
      Wen, Lijie  and
      Yu, Philip S.",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.578/",
    doi = "10.18653/v1/2023.findings-acl.578",
    pages = "9072--9087",
    abstract = "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques."
}

@inproceedings{wang-etal-2024-order,
    title = "Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition",
    author = "Wang, Huiming  and
      Cheng, Liying  and
      Zhang, Wenxuan  and
      Soh, De Wen  and
      Bing, Lidong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.421/",
    doi = "10.18653/v1/2024.acl-long.421",
    pages = "7792--7807",
    abstract = "Data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in low-resource settings, including few-shot named entity recognition (NER). However, existing NER DA techniques either perform rule-based manipulations on words that break the semantic coherence of the sentence, or exploit generative models for entity or context substitution, which requires a substantial amount of labeled data and contradicts the objective of operating in low-resource settings. In this work, we propose order-agnostic data augmentation (OaDA), an alternative solution that exploits the often overlooked order-agnostic property in the training data construction phase of sequence-to-sequence NER methods for data augmentation. To effectively utilize the augmented data without suffering from the one-to-many issue, where multiple augmented target sequences exist for one single sentence, we further propose the use of ordering instructions and an innovative OaDA-XE loss. Specifically, by treating each permutation of entity types as an ordering instruction, we rearrange the entity set accordingly, ensuring a distinct input-output pair, while OaDA-XE assigns loss based on the best match between the target sequence and model predictions. We conduct comprehensive experiments and analyses across three major NER benchmarks and significantly enhance the few-shot capabilities of PLMs with OaDA."
}

@inproceedings{wu_beto_2019,
	address = {Hong Kong, China},
	title = {Beto, {Bentz}, {Becas}: {The} {Surprising} {Cross}-{Lingual} {Effectiveness} of {BERT}},
	url = {https://www.aclweb.org/anthology/D19-1077},
	doi = {10.18653/v1/D19-1077},
	abstract = {Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	month = nov,
	year = {2019},
	pages = {833--844},
	annote = {
We use Adam (Kingma and Ba,2014) for fine-tuning withβ1of 0.9,β2of 0.999and L2 weight decay of 0.01. We warm up thelearning rate over the first 10\% of batches and lin-early decay the learning rate
}
}

@article{Gu_domain_2020,
   title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
   volume={3},
   ISSN={2637-8051},
   url={http://dx.doi.org/10.1145/3458754},
   DOI={10.1145/3458754},
   number={1},
   journal={ACM Transactions on Computing for Healthcare},
   publisher={Association for Computing Machinery (ACM)},
   author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
   year={2020},
   month=oct, pages={1–23} }

@article{manjavacas-fonteyn-2022-adapting,
    title      = {Adapting vs. Pre-training Language Models for Historical Languages},
    author     = {Enrique Manjavacas and Lauren Fonteyn},
    url        = {https://jdmdh.episciences.org/9152},
    doi        = {10.46298/jdmdh.9152},
    journal    = {Journal of Data Mining \& Digital Humanities},
    issn       = {2416-5999},
    volume     = {NLP4DH},
    issuetitle = {Digital humanities in languages},
    eid        = 3,
    year       = {2022},
    month      = {Jun},
    keywords   = {[INFO.INFO-CL]Computer Science [cs]/Computation and Language [cs.CL]},
    language   = {English},
}

@inproceedings{manjavacas-arevalo-fonteyn-2022-non,
    title = "Non-Parametric Word Sense Disambiguation for Historical Languages",
    author = "Manjavacas Arevalo, Enrique  and
      Fonteyn, Lauren",
    editor = {H{\"a}m{\"a}l{\"a}inen, Mika  and
      Alnajjar, Khalid  and
      Partanen, Niko  and
      Rueter, Jack},
    booktitle = "Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities",
    month = nov,
    year = "2022",
    address = "Taipei, Taiwan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlp4dh-1.16/",
    doi = "10.18653/v1/2022.nlp4dh-1.16",
    pages = "123--134",
    abstract = "Recent approaches to Word Sense Disambiguation (WSD) have profited from the enhanced contextualized word representations coming from contemporary Large Language Models (LLMs). This advancement is accompanied by a renewed interest in WSD applications in Humanities research, where the lack of suitable, specific WSD-annotated resources is a hurdle in developing ad-hoc WSD systems. Because they can exploit sentential context, LLMs are particularly suited for disambiguation tasks. Still, the application of LLMs is often limited to linear classifiers trained on top of the LLM architecture. In this paper, we follow recent developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to achieve strong few-shot performance on WSD for historical languages (English and Dutch, date range: 1450-1950). We test our hypothesis using (i) a large, general evaluation set taken from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch."
}

@misc{vaswani_attention_2017,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@techreport{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
	language = {en},
	institution = {OpenAI},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/HHVPUF22/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/JRM9C5L9/1810.html:text/html},
}

@inproceedings{koolen_accessing_2024,
	title = {Accessing the {Republic}: {Digital} {Humanities} in the {Benelux} 2024 {Conference}},
	shorttitle = {Accessing the {Republic}},
	doi = {10.5281/zenodo.11485227},
	author = {Koolen, Marijn and Renkema, Esger and Groskamp, Nienke and Smit, Frank and Reinders, Jirsi and Sluijter, R.G.H. and Hoekstra, Rik and Oddens, Joris},
	year = {2024},
	keywords = {digital history, data curation, named entity recognition},
}

@inproceedings{provatorova_too_2024,
	address = {Torino, Italia},
	title = {Too {Young} to {NER}: {Improving} {Entity} {Recognition} on {Dutch} {Historical} {Documents}},
	shorttitle = {Too {Young} to {NER}},
	url = {https://aclanthology.org/2024.lt4hala-1.4/},
	abstract = {Named entity recognition (NER) on historical texts is beneficial for the field of digital humanities, as it allows to easily search for the names of people, places and other entities in digitised archives. While the task of historical NER in different languages has been gaining popularity in recent years, Dutch historical NER remains an underexplored topic. Using a recently released historical dataset from the Dutch Language Institute, we train three BERT-based models and analyse the errors to identify main challenges. All three models outperform a contemporary multilingual baseline by a large margin on historical test data.},
	booktitle = {Proceedings of the {Third} {Workshop} on {Language} {Technologies} for {Historical} and {Ancient} {Languages} ({LT4HALA}) @ {LREC}-{COLING}-2024},
	publisher = {ELRA and ICCL},
	author = {Provatorova, Vera and van Erp, Marieke and Kanoulas, Evangelos},
	editor = {Sprugnoli, Rachele and Passarotti, Marco},
	month = may,
	year = {2024},
	pages = {30--35},
	file = {Full Text PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/EPJY6TH8/Provatorova et al. - 2024 - Too Young to NER Improving Entity Recognition on Dutch Historical Documents.pdf:application/pdf},
}

@misc{zaratiana_gliner_2023,
	title = {{GLiNER}: {Generalist} {Model} for {Named} {Entity} {Recognition} using {Bidirectional} {Transformer}},
	shorttitle = {{GLiNER}},
	url = {http://arxiv.org/abs/2311.08526},
	doi = {10.48550/arXiv.2311.08526},
	abstract = {Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.},
	publisher = {arXiv},
	author = {Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08526 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/3Y3REUTL/Zaratiana et al. - 2023 - GLiNER Generalist Model for Named Entity Recognition using Bidirectional Transformer.pdf:application/pdf;Snapshot:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/5XJK8HP9/2311.html:text/html},
}

@misc{project_voc_2024,
	title = {{VOC} transcriptions v2 - {GLOBALISE}},
	url = {https://hdl.handle.net/10622/LVXSBW},
	abstract = {This is a collection of machine-generated transcriptions of the ‘Overgekomen brieven en papieren’ of the Dutch East India Company (VOC). They were ...},
	publisher = {IISH Data Collection},
	author = {{GLOBALISE Project}},
	year = {2024}
}

@inproceedings{van_koert_loghi_2024,
	address = {Cham},
	title = {Loghi: {An} {End}-to-{End} {Framework} for {Making} {Historical} {Documents} {Machine}-{Readable}},
	isbn = {978-3-031-70645-5},
	shorttitle = {Loghi},
	doi = {10.1007/978-3-031-70645-5_6},
	abstract = {Loghi is a novel framework and suite of tools for the layout analysis and text recognition of historical documents. Scans are processed in a modular pipeline, with the option to use alternative tools in most stages. Layout analysis and text recognition can be trained on example images with PageXML ground truth. The framework is intended to convert scanned documents to machine-readable PageXML. Additional tooling is provided for the creation of synthetic ground truth. A visualiser for troubleshooting the text recognition training is also made available. The result is a framework for end-to-end text recognition, which works from initial layout analysis on the scanned documents, and includes text line detection, text recognition, reading order detection and language detection.},
	language = {en},
	booktitle = {Document {Analysis} and {Recognition} – {ICDAR} 2024 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {van Koert, Rutger and Klut, Stefan and Koornstra, Tim and Maas, Martijn and Peters, Luke},
	editor = {Mouchère, Harold and Zhu, Anna},
	year = {2024},
	keywords = {Handwritten Text Recognition, Layout Analysis, PageXML},
	pages = {73--88},
	file = {Full Text PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/CDBY36JQ/van Koert et al. - 2024 - Loghi An End-to-End Framework for Making Historical Documents Machine-Readable.pdf:application/pdf},
}

@inproceedings{gururangan_dont_2020,
	address = {Online},
	title = {Don't {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}},
	shorttitle = {Don't {Stop} {Pretraining}},
	url = {https://aclanthology.org/2020.acl-main.740/},
	doi = {10.18653/v1/2020.acl-main.740},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {8342--8360},
	file = {Full Text PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/QFAX2WQZ/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to Domains and Tasks.pdf:application/pdf},
}

@inproceedings{ghosh_aclm_2023,
	address = {Toronto, Canada},
	title = {{ACLM}: {A} {Selective}-{Denoising} based {Generative} {Data} {Augmentation} {Approach} for {Low}-{Resource} {Complex} {NER}},
	shorttitle = {{ACLM}},
	url = {https://aclanthology.org/2023.acl-long.8},
	doi = {10.18653/v1/2023.acl-long.8},
	language = {en},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ghosh, Sreyan and Tyagi, Utkarsh and Suri, Manan and Kumar, Sonal and S, Ramaneswaran and Manocha, Dinesh},
	year = {2023},
	file = {PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/EXURP2NB/Ghosh et al. - 2023 - ACLM A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER.pdf:application/pdf},
}

@inproceedings{zhou_melm_2022,
	address = {Dublin, Ireland},
	title = {{MELM}: {Data} {Augmentation} with {Masked} {Entity} {Language} {Modeling} for {Low}-{Resource} {NER}},
	shorttitle = {{MELM}},
	url = {https://aclanthology.org/2022.acl-long.160},
	doi = {10.18653/v1/2022.acl-long.160},
	language = {en},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Ran and Li, Xin and He, Ruidan and Bing, Lidong and Cambria, Erik and Si, Luo and Miao, Chunyan},
	year = {2022},
	file = {PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/CDSSXQRK/Zhou et al. - 2022 - MELM Data Augmentation with Masked Entity Language Modeling for Low-Resource NER.pdf:application/pdf},
}

@inproceedings{arnoult_batavia_2021,
	address = {Punta Cana, Dominican Republic (online)},
	title = {Batavia asked for advice. {Pretrained} language models for {Named} {Entity} {Recognition} in historical texts.},
	url = {https://aclanthology.org/2021.latechclfl-1.3/},
	doi = {10.18653/v1/2021.latechclfl-1.3},
	abstract = {Pretrained language models like BERT have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These models are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for texts enriched with editorial notes: how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for Dutch based on 17th and 18th century United East India Company (VOC) reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all language models can leverage mixed-variant data. In particular, language models successfully incorporate notes for the prediction of entities in historical texts. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand: multilingual models lose their advantage when confronted with more semantical tasks.},
	booktitle = {Proceedings of the 5th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Arnoult, Sophie I. and Petram, Lodewijk and Vossen, Piek},
	editor = {Degaetano-Ortlieb, Stefania and Kazantseva, Anna and Reiter, Nils and Szpakowicz, Stan},
	month = nov,
	year = {2021},
	pages = {21--30},
	file = {Full Text PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/A22X9XVB/Arnoult et al. - 2021 - Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical text.pdf:application/pdf},
}

@misc{verkijk_globertise_2025,
	title = {{gloBERTise}},
	url = {https://huggingface.co/globalise/GloBERTise},
	author = {Verkijk, Stella},
	year = {2025},
}

@inproceedings{conneau_unsupervised_2020,
	address = {Online},
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {https://aclanthology.org/2020.acl-main.747/},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {8440--8451},
	file = {Full Text PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/TNB2HRDN/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning at Scale.pdf:application/pdf},
}

@inproceedings{verkijk-etal-2025-language,
    title = "Language Models Lack Temporal Generalization and Bigger is Not Better",
    author = "Verkijk, Stella  and
      Vossen, Piek  and
      Sommerauer, Pia",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1060/",
    doi = "10.18653/v1/2025.findings-acl.1060",
    pages = "20629--20637",
    ISBN = "979-8-89176-256-5",
    abstract = "This paper presents elaborate testing of various LLMs on their generalization capacities. We finetune six encoder models that have been pretrained with very different data (varying in size, language, and period) on a challenging event detection task in Early Modern Dutch archival texts. Each model is finetuned with 5 seeds on 15 different data splits, resulting in 450 finetuned models. We also pre-train a domain-specific Language Model on the target domain and fine-tune and evaluate it in the same way to provide an upper bound. Our experimental setup allows us to look at underresearched aspects of generalizability, namely i) shifts at multiple places in a modeling pipeline, ii) temporal and crosslingual shifts and iii) generalization over different initializations. The results show that none of the models reaches domain-specific model performance, demonstrating their incapacity to generalize. mBERT reaches highest F1 performance, and is relatively stable over different seeds and datasplits, contrary to XLM-R. We find that contemporary Dutch models do not generalize well to Early Modern Dutch as they underperform compared to crosslingual as well as historical models. We conclude that encoder LLMs lack temporal generalization capacities and that bigger models are not better, since even a model pre-trained with five hundred GPUs on 2.5 terabytes of training data (XLM-R) underperforms considerably compared to our domain-specific model, pre-trained on one GPU and 6 GB of data. All our code, data, and the domain-specific model are openly available."
}

@misc{kingma-2014-adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2014},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = mar,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/FQNZ5LU7/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, faster, cheaper and lighter.pdf:application/pdf;Snapshot:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/8F26P2TL/1910.html:text/html},
}

@article{spacy,
author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
doi = {10.5281/zenodo.1212303},
title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
year = {2020}
}

@misc{arnoult_voc_2022,
	type = {Datapackage},
	title = {{VOC} {GM} {NER} corpus},
	url = {https://publication.yoda.vu.nl/full/VU01/HI67KL.html},
	abstract = {Data Publication platform of Vrije Universiteit Amsterdam},
	language = {en},
	author = {Arnoult, Sophie},
	month = dec,
	year = {2022},
	doi = {10.48338/VU01-HI67KL},
	note = {Publisher: Vrije Universiteit Amsterdam},
	file = {Snapshot:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/EQ95B4A8/HI67KL.html:text/html},
}

@misc{seqeval,
  title={{seqeval}: A Python framework for sequence labeling evaluation},
  url={https://github.com/chakki-works/seqeval},
  note={Software available from https://github.com/chakki-works/seqeval},
  author={Hiroki Nakayama},
  year={2018},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/DJ8Z9NRQ/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:application/pdf;Snapshot:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/C432LRK9/1907.html:text/html},
}

@incollection{ramshaw_text_1999,
	address = {Dordrecht},
	title = {Text {Chunking} {Using} {Transformation}-{Based} {Learning}},
	isbn = {978-90-481-5349-7 978-94-017-2390-9},
	url = {http://link.springer.com/10.1007/978-94-017-2390-9_10},
	abstract = {Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive "baseNP" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92\% for baseNP chunks and 88\% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.},
	language = {en},
	booktitle = {Text, {Speech} and {Language} {Technology}},
	publisher = {Springer Netherlands},
	collaborator = {Ramshaw, L. A. and Marcus, M. P.},
	year = {1999},
	doi = {10.1007/978-94-017-2390-9_10},
	note = {ISSN: 1386-291X},
	pages = {157--176},
	file = {PDF:/home/sophie-arnoult/snap/zotero-snap/common/Zotero/storage/I4UD8AQN/1999 - Text Chunking Using Transformation-Based Learning.pdf:application/pdf},
}

@article{luthra_unsilencing_2023,
	title = {Unsilencing colonial archives via automated entity recognition},
	issn = {0022-0418},
	url = {https://doi.org/10.1108/JD-02-2022-0038},
	doi = {10.1108/JD-02-2022-0038},
	abstract = {Purpose This paper aims to expand the scope and mitigate the biases of extant archival indexes. Design/methodology/approach The authors use automatic entity recognition on the archives of the Dutch East India Company to extract mentions of underrepresented people. Findings The authors release an annotated corpus and baselines for a shared task and show that the proposed goal is feasible. Originality/value Colonial archives are increasingly a focus of attention for historians and the public, broadening access to them is a pressing need for archives.},
	urldate = {2023-07-13},
	journal = {Journal of Documentation},
	author = {Luthra, Mrinalini and Todorov, Konstantin and Jeurgens, Charles and Colavizza, Giovanni},
	year = {2023},
	keywords = {Digital humanities, Dutch East India Company, Accessibility, Archives, Named entity recognition and classification, Natural language processing},
	file = {Snapshot:/Users/brechtnijman/Zotero/storage/FERLWQZL/html.html:text/html;Submitted Version:/Users/brechtnijman/Zotero/storage/J9MWGD3S/Luthra et al. - 2023 - Unsilencing colonial archives via automated entity.pdf:application/pdf},
}

@inproceedings{klie_inception_2018,
	address = {Santa Fe, New Mexico},
	title = {The {INCEpTION} {Platform}: {Machine}-{Assisted} and {Knowledge}-{Oriented} {Interactive} {Annotation}},
	shorttitle = {The {INCEpTION} {Platform}},
	url = {https://aclanthology.org/C18-2002/},
	abstract = {We introduce INCEpTION, a new annotation platform for tasks including interactive and semantic annotation (e.g., concept linking, fact linking, knowledge base population, semantic frame annotation). These tasks are very time consuming and demanding for annotators, especially when knowledge bases are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The platform is both generic and modular. It targets a range of research domains in need of semantic annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION is publicly available as open-source software.},
	urldate = {2025-07-19},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Klie, Jan-Christoph and Bugert, Michael and Boullosa, Beto and Eckart de Castilho, Richard and Gurevych, Iryna},
	editor = {Zhao, Dongyan},
	month = aug,
	year = {2018},
	pages = {5--9},
	file = {Full Text PDF:/Users/brechtnijman/Zotero/storage/BWYXJSD7/Klie e.a. - 2018 - The INCEpTION Platform Machine-Assisted and Knowledge-Oriented Interactive Annotation.pdf:application/pdf},
}
