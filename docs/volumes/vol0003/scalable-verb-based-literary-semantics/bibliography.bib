@misc{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
}

@book{frenzelMotiveWeltliteraturLexikon1980custom,
  title = {Motive Der {{Weltliteratur}}: Ein {{Lexikon}} Dichtungsgeschichtlicher {{L{\"a}ngs\-schnitte}}},
  author = {Frenzel, Elisabeth},
  year = {1980},
  publisher = {Kr{\"o}ner},
  address = {Stuttgart}
}

@misc{gemmateam2025gemma3technicalreport,
      title={Gemma 3 Technical Report},
      author={Gemma Team},
      year={2025},
      eprint={2503.19786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19786},
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models},
      author={Llama-3 Team},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783},
}

@book{dprosecorpus,
  title = { d-Prose 1870-1920 (2.0) [Data set]},
  author = {Gius, Evelyn and Svenja Guhr and Benedikt Adelmann},
  year = {2021},
  publisher = {Zenodo},
  doi = {10.5281/zenodo.5015008},
  url = {https://doi.org/10.5281/zenodo.5015008},
}

@inproceedings{uglanova_order_2020,
	address = {Amsterdam, the Netherlands},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {The {Order} of {Things}. {A} {Study} on {Topic} {Modelling} of {Literary} {Texts}},
	url = {http://ceur-ws.org/Vol-2723/long7.pdf},
	abstract = {Topic modelling is considered a statistical tool for the thematic decomposition of texts. In reality, it captures only statistical patterns in the structure of the object. This sensitivity of the method for structure makes it less effective when applied to literary texts in which structure itself is a relevant feature with an artistic function. In this paper, we calculate a series of topic models for three corpora of literary narratives with various stages of data cleaning. We apply coherence values, qualitative interpretation and measurement of topic distances in order to shed some light on the regularities between text features and the quality of the topic modelling performed for literary prose.},
	language = {en},
	booktitle = {Proceedings of the {Workshop} on {Computational} {Humanities} {Research} ({CHR} 2020)},
	author = {Uglanova, Inna and Gius, Evelyn},
	editor = {Karsdorp, Folgert and McGillivray, Barbara and Nerghes, Adina and Wevers, Melvin},
	month = nov,
	year = {2020},
	pages = {57--76},
	file = {Uglanova und Gius - The Order of Things. A Study on Topic Modelling of.pdf:/Users/hansole/Zotero/storage/M28EHZTR/Uglanova und Gius - The Order of Things. A Study on Topic Modelling of.pdf:application/pdf},
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{maienbornEventSemantics2011,
  title     = {Event semantics},
  author    = {Maienborn, Claudia},
  editor    = {Maienborn, Claudia and Heusinger, Klaus von and Portner, Paul},
  booktitle = {Semantics: An International Handbook of Natural Language Meaning},
  volume    = {1},
  pages     = {802--829},
  publisher = {Mouton de Gruyter},
  address   = {Berlin / New York},
  year      = {2011},
  doi       = {10.1515/9783110589245-008},
}

@misc{klavansRoleOfVerbs1998,
  title     = {The Role of Verbs in Document Analysis},
  author    = {Klavans, Judith L.},
  booktitle = {Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL '98)},
  address   = {Montreal, Canada},
  month     = aug,
  year      = {1998},
  pages     = {680--686},
  doi       = {10.48550/arXiv.cmp-lg/9807002},
  note      = {arXiv:cmp\-lg/9807002},
}

@inproceedings{anonymous2025,
    author = "Hatzel, Hans Ole and Stiemer, Haimo and Biemann, Chris and Gius, Evelyn",
    title = "Towards a Verb Class-based Semantic Analysis of German Literary Texts",
    booktitle = "DH2025 Book of Abstracts",
    location = {Lisbon, Portugal},
    year = 2025,
    note = {Accepted},
}

@book{parraMembrivesFacettenKriminalromans2015,
  title     = {Facetten des Kriminalromans: Ein Genre zwischen Tradition und Innovation},
  author    = {Parra Membrives, Eva and Brylla, Wolfgang Damian},
  publisher = {Narr Francke Attempto},
  address   = {Tübingen},
  year      = {2015},
  edition   = {1},
  isbn      = {978-3-8233-6946-2},
}

@incollection{braungartHeimatliteraturReallexikonDeutschen2010,
  author    = {Braungart, Georg and Fricke, Harald and Grubm{\"u}ller, Klaus and M{\"u}ller, Jan-Dirk and Vollhardt, Friedrich and Weimar, Klaus},
  title     = {Heimatliteratur},
  booktitle = {Reallexikon der deutschen Literaturwissenschaft},
  editor    = {Braungart, Georg and Fricke, Harald and Grubm{\"u}ller, Klaus and M{\"u}ller, Jan-Dirk and Vollhardt, Friedrich and Weimar, Klaus},
  volume    = {1},
  pages     = {19--20},
  publisher = {Walter de Gruyter},
  address   = {Berlin / New York},
  year      = {2010},
  isbn      = {978-3-11-010896-5},
  doi       = {10.1515/9783110914672},
  note      = {Artikel im Band 1 (A – G)},
}

@book{koppenfelsAbenteuerErzaehlmusterFormprinzip2019,
  editor    = {von Koppenfels, Martin and Mühlbacher, Manuel},
  title     = {Abenteuer: Erzählmuster, Formprinzip, Genre},
  series    = {Philologie des Abenteuers},
  volume    = {1},
  publisher = {Wilhelm Fink Verlag},
  address   = {Paderborn},
  year      = {2019},
  isbn      = {978-3-7705-6472-9},
  doi       = {10.30965/9783846764725},
}

@inproceedings{bakerBerkeleyFrameNetProject1998,
  title = {The {{Berkeley FrameNet Project}}},
  booktitle = {{{COLING}} 1998 {{Volume}} 1: {{The}} 17th {{International Conference}} on {{Computational Linguistics}}},
  author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
  year = {1998},
  pages = {86--90},
  url = {https://aclanthology.org/C98-1013},
  urldate = {2024-07-02}
}

@inproceedings{bevilacquaRecentTrendsWord2021,
  title = {Recent {{Trends}} in {{Word Sense Disambiguation}}: {{A Survey}}},
  shorttitle = {Recent {{Trends}} in {{Word Sense Disambiguation}}},
  booktitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  editor = {Zhou, Zhi-Hua},
  year = {2021},
  month = aug,
  volume = {5},
  pages = {4330--4338},
  address = {Montral, Canada},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2021/593},
  urldate = {2024-07-01},
  abstract = {Electronic proceedings of IJCAI 2021}
}

@misc{chiangChatbotArenaOpen2024,
  title = {Chatbot {{Arena}}: {{An Open Platform}} for {{Evaluating LLMs}} by {{Human Preference}}},
  shorttitle = {Chatbot {{Arena}}},
  author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04132},
  eprint = {2403.04132},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04132},
  urldate = {2024-06-20},
  abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{ciaramitaBroadCoverageSenseDisambiguation2006,
  title = {Broad-{{Coverage Sense Disambiguation}} and {{Information Extraction}} with a {{Supersense Sequence Tagger}}},
  booktitle = {Proceedings of the 2006 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ciaramita, Massimiliano and Altun, Yasemin},
  editor = {Jurafsky, Dan and Gaussier, Eric},
  year = {2006},
  month = jul,
  pages = {594--602},
  publisher = {Association for Computational Linguistics},
  address = {Sydney, Australia},
  url = {https://aclanthology.org/W06-1670},
  urldate = {2024-07-03}
}

@book{daemmrichThemenUndMotive1995,
  title = {Themen Und {{Motive}} in Der {{Literatur}}. {{Ein Handbuch}}},
  author = {Daemmrich, Horst S. and Daemmrich, Ingrid G.},
  year = {1995},
  publisher = {Francke},
  address = {T{\"u}bingen, Basel}
}

@inproceedings{donickeMONAPipeModesNarration2022,
  title = {{{MONAPipe}}: {{Modes}} of {{Narration}} and {{Attribution Pipeline}} for {{German Computational Literary Studies}} and {{Language Analysis}} in {{spaCy}}},
  shorttitle = {{{MONAPipe}}},
  booktitle = {Proceedings of the 18th {{Conference}} on {{Natural Language Processing}} ({{KONVENS}} 2022)},
  author = {D{\"o}nicke, Tillmann and Barth, Florian and Varachkina, Hanna and Sporleder, Caroline},
  editor = {Schaefer, Robin and Bai, Xiaoyu and Stede, Manfred and Zesch, Torsten},
  year = {2022},
  pages = {8--15},
  publisher = {KONVENS 2022 Organizers},
  address = {Potsdam, Germany},
  url = {https://aclanthology.org/2022.konvens-1.2},
  urldate = {2023-05-10}
}

@book{fellbaumWordNetElectronicLexical1998,
  title = {{{WordNet}}: {{An}} Electronic Lexical Database},
  editor = {Fellbaum, Christiane},
  year = {1998},
  series = {Language, Speech, and Communication},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
  isbn = {978-0-262-06197-1},
  keywords = {wordnet}
}

@book{frenzelMotiveWeltliteraturLexikon1980,
  title = {Motive Der {{Weltliteratur}}: Ein {{Lexikon}} Dichtungsgeschichtlicher {{L{\"a}ngsschnitte}}},
  author = {Frenzel, Elisabeth},
  year = {1980},
  publisher = {Kr{\"o}ner},
  address = {Stuttgart}
}

@inproceedings{hampGermaNetLexicalSemanticNet1997,
  title = {{{GermaNet}} - a {{Lexical-Semantic Net}} for {{German}}},
  booktitle = {Automatic {{Information Extraction}} and {{Building}} of {{Lexical Semantic Resources}} for {{NLP Applications}}},
  author = {Hamp, Birgit and Feldweg, Helmut},
  editor = {Vossen, Piek and Adriaens, Geert and Calzolari, Nicoletta and Sanfilippo, Antonio and Wilks, Yorik},
  year = {1997},
  pages = {9--15},
  address = {Madrid, Spain},
  url = {https://aclanthology.org/W97-0802},
  urldate = {2024-06-20}
}

@article{hatzelMachineLearningComputational2023,
  title = {Machine Learning in Computational Literary Studies},
  author = {Hatzel, Hans Ole and Stiemer, Haimo and Biemann, Chris and Gius, Evelyn},
  editor = {Mara, Hubert and Usbeck, Ricardo},
  year = {2023},
  month = aug,
  journal = {it - Information Technology},
  volume = {65},
  number = {4-5},
  publisher = {De Gruyter Oldenbourg},
  issn = {2196-7032},
  doi = {10.1515/itit-2023-0041},
  urldate = {2023-08-25},
  abstract = {In this article, we provide an overview of machine learning as it is applied in computational literary studies, the field of computational analysis of literary texts and literature related phenomena. We survey a number of scientific publications for the machine learning methodology the scholars used and explain concepts of machine learning and natural language processing while discussing our findings. We establish that besides transformer-based language models, researchers still make frequent use of more traditional, feature-based machine learning approaches; possible reasons for this are to be found in the challenging application of modern methods to the literature domain and in the more transparent nature of traditional approaches. We shed light on how machine learning-based approaches are integrated into a research process, which often proceeds primarily from the non-quantitative, interpretative approaches of non-digital literary studies. Finally, we conclude that the application of large language models in the computational literary studies domain may simplify the application of machine learning methodology going forward, if adequate approaches for the analysis of literary texts are found.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  keywords = {computational literary studies,language models,machine learning,natural language processing,transformers}
}

@inproceedings{kumarZeroshotWordSense2019,
  title = {Zero-Shot {{Word Sense Disambiguation}} Using {{Sense Definition Embeddings}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kumar, Sawan and Jat, Sharmistha and Saxena, Karan and Talukdar, Partha},
  editor = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s},
  year = {2019},
  month = jul,
  pages = {5670--5681},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1568},
  urldate = {2024-07-02},
  abstract = {Word Sense Disambiguation (WSD) is a long-standing but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zero-shot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new state-of-the-art WSD performance.}
}

@inproceedings{labaContextualEmbeddingsUkrainian2023,
  title = {Contextual {{Embeddings}} for {{Ukrainian}}: {{A Large Language Model Approach}} to {{Word Sense Disambiguation}}},
  shorttitle = {Contextual {{Embeddings}} for {{Ukrainian}}},
  booktitle = {Proceedings of the {{Second Ukrainian Natural Language Processing Workshop}} ({{UNLP}})},
  author = {Laba, Yurii and Mudryi, Volodymyr and Chaplynskyi, Dmytro and Romanyshyn, Mariana and Dobosevych, Oles},
  editor = {Romanyshyn, Mariana},
  year = {2023},
  month = may,
  pages = {11--19},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.unlp-1.2},
  urldate = {2024-07-02},
  abstract = {This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Ukrainian language based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on the dataset generated in an unsupervised way to obtain better contextual embeddings for words with multiple senses. The paper presents a method for generating a new dataset for WSD evaluation in the Ukrainian language based on the SUM dictionary. We developed a comprehensive framework that facilitates the generation of WSD evaluation datasets, enables the use of different prediction strategies, LLMs, and pooling strategies, and generates multiple performance reports. Our approach shows 77,9\% accuracy for lexical meaning prediction for homonyms.}
}

@book{levinEnglishVerbClasses1993,
  title = {English {{Verb Classes}} and {{Alternations}}: {{A Preliminary Investigation}}},
  shorttitle = {English {{Verb Classes}} and {{Alternations}}},
  author = {Levin, Beth},
  year = {1993},
  month = sep,
  publisher = {University of Chicago Press},
  address = {Chicago, IL},
  url = {https://press.uchicago.edu/ucp/books/book/chicago/E/bo3684144.html},
  urldate = {2024-06-20},
  abstract = {In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, English Verb Classes and Alternations sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language.},
  isbn = {978-0-226-47533-2}
}

@misc{liSurveyRetrievalAugmentedText2022,
  title = {A {{Survey}} on {{Retrieval-Augmented Text Generation}}},
  author = {Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
  year = {2022},
  month = feb,
  number = {arXiv:2202.01110},
  eprint = {2202.01110},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.01110},
  urldate = {2024-07-01},
  abstract = {Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{tornbergChatGPT4OutperformsExperts2023,
  title = {{{ChatGPT-4 Outperforms Experts}} and {{Crowd Workers}} in {{Annotating Political Twitter Messages}} with {{Zero-Shot Learning}}},
  author = {T{\"o}rnberg, Petter},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06588},
  eprint = {2304.06588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06588},
  urldate = {2024-07-01},
  abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Social and Information Networks}
}

@misc{vauthRichtlinienFurAnnotation2021,
  title = {Richtlinien F{\"u}r Die {{Annotation}} Narratologischer {{Ereigniskonzepte}}},
  author = {Vauth, Michael and Gius, Evelyn},
  year = {2021},
  month = jul,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.5078174},
  urldate = {2021-07-08},
  abstract = {Mit diesen Annotationsrichtlinien werden g{\"a}ngige narratologische Ereigniskonzepte f{\"u}r die Annotation operationalisiert. Die Richtlinien sind im Rahmen des EvENT-Projekts entstanden (https://www.linglit.tu-darmstadt.de/forschungsuebersicht/index.de.jsp). Das EvENT-Projekt ist Teil des DFG-gef{\"o}rderten Schwerpunktprogramms Computational Literary Studies (https://dfg-spp-cls.github.io/).},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  keywords = {Annotation,Computational Literary Studies,Digital Humanities,Guideline,Narratology}
}
